{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "983984f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede4a3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "import requests\n",
    "import json\n",
    "import os \n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, max, col, countDistinct, when, rank, lit\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# from backend_spark.doris_common.doris_client import DorisClient\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac18e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# os.environ['PYSPARK_PYTHON'] = '/usr/local/python-3.9.13/bin/python3'\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"pandas_udf\") \\\n",
    "#     .config('spark.sql.session.timeZone', 'Asia/Shanghai') \\\n",
    "#     .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "#     .config('spark.driver.memory', '1024m') \\\n",
    "#     .config('spark.driver.cores', '3') \\\n",
    "#     .config('spark.executor.memory', '1024m') \\\n",
    "#     .config('spark.executor.cores', '1') \\\n",
    "#     .config('spark.cores.max', '2') \\\n",
    "#     .config('spark.driver.host', '192.168.22.28') \\\n",
    "#     .master(\"spark://192.168.12.47:7077,192.168.12.48:7077\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "findspark.init() \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ywj\") \\\n",
    "    .config('spark.sql.session.timeZone', 'Asia/Shanghai') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "301034e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#######################################解析SQL########################################\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef77d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 这里使用doris数据库连接\n",
    "# user =\"root\"\n",
    "# host = \"10.52.199.81\"\n",
    "# password = \"Nexchip%40123\"\n",
    "# db = \"etl\"\n",
    "# port = 9030\n",
    "\n",
    "# engine = create_engine(\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\".format(user = user,\n",
    "#                                                                                     password = password,\n",
    "#                                                                                     host = host,\n",
    "#                                                                                     port = port,\n",
    "#                                                                                     db = db))\n",
    "\n",
    "# df1_pandas = pd.read_sql_query(\"SELECT * FROM etl.DWD_POC_CASE_FD_UVA_DATA_CASE1_PROCESSED1\", engine)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8edcefe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAFER_ID</th>\n",
       "      <th>TOOL_ID</th>\n",
       "      <th>RUN_ID</th>\n",
       "      <th>EQP_NAME</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>PRODG1</th>\n",
       "      <th>TOOL_NAME</th>\n",
       "      <th>LOT_ID</th>\n",
       "      <th>RECIPE_NAME</th>\n",
       "      <th>OPER_NO</th>\n",
       "      <th>START_TIME</th>\n",
       "      <th>parametric_name</th>\n",
       "      <th>CASE_INFO</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>STATISTIC_RESULT</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NGE186-07</td>\n",
       "      <td>11341</td>\n",
       "      <td>149770</td>\n",
       "      <td>EKT72</td>\n",
       "      <td>AFKN2J01N.0U01</td>\n",
       "      <td>L11CD02A</td>\n",
       "      <td>EKT72_PM1</td>\n",
       "      <td>NGE186.000</td>\n",
       "      <td>NEW-DRM/P1/110NM/PFKN0S0D1F1A</td>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>2023-06-16 02:15:06</td>\n",
       "      <td>PROCESS_GAS_10_CO#WINDOW_1#SUM</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NGE186-07</td>\n",
       "      <td>11341</td>\n",
       "      <td>149770</td>\n",
       "      <td>EKT72</td>\n",
       "      <td>AFKN2J01N.0U01</td>\n",
       "      <td>L11CD02A</td>\n",
       "      <td>EKT72_PM1</td>\n",
       "      <td>NGE186.000</td>\n",
       "      <td>NEW-DRM/P1/110NM/PFKN0S0D1F1A</td>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>2023-06-16 02:15:06</td>\n",
       "      <td>PROCESS_GAS_3_C4F6#WINDOW_1#SUM</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NGE186-07</td>\n",
       "      <td>11341</td>\n",
       "      <td>149770</td>\n",
       "      <td>EKT72</td>\n",
       "      <td>AFKN2J01N.0U01</td>\n",
       "      <td>L11CD02A</td>\n",
       "      <td>EKT72_PM1</td>\n",
       "      <td>NGE186.000</td>\n",
       "      <td>NEW-DRM/P1/110NM/PFKN0S0D1F1A</td>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>2023-06-16 02:15:06</td>\n",
       "      <td>PROCESS_GAS_14_O2#AOTU_STEP_2#MEAN</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>7.5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NGE186-07</td>\n",
       "      <td>11341</td>\n",
       "      <td>149770</td>\n",
       "      <td>EKT72</td>\n",
       "      <td>AFKN2J01N.0U01</td>\n",
       "      <td>L11CD02A</td>\n",
       "      <td>EKT72_PM1</td>\n",
       "      <td>NGE186.000</td>\n",
       "      <td>NEW-DRM/P1/110NM/PFKN0S0D1F1A</td>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>2023-06-16 02:15:06</td>\n",
       "      <td>BOTTOMFLOWRATE#AOTU_STEP_2#MEAN</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>29.9930</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NGE186-07</td>\n",
       "      <td>11341</td>\n",
       "      <td>149770</td>\n",
       "      <td>EKT72</td>\n",
       "      <td>AFKN2J01N.0U01</td>\n",
       "      <td>L11CD02A</td>\n",
       "      <td>EKT72_PM1</td>\n",
       "      <td>NGE186.000</td>\n",
       "      <td>NEW-DRM/P1/110NM/PFKN0S0D1F1A</td>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>2023-06-16 02:15:06</td>\n",
       "      <td>PROCESS_GAS_12_CH3F#WINDOW_1#SUM</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550906</th>\n",
       "      <td>NGE714-18</td>\n",
       "      <td>11342</td>\n",
       "      <td>157510</td>\n",
       "      <td>EKT72</td>\n",
       "      <td>AFGNRE01N.0C01</td>\n",
       "      <td>L15RB20A</td>\n",
       "      <td>EKT72_PM2</td>\n",
       "      <td>NGE714.000</td>\n",
       "      <td>NEW-DRM/P2/150NM/PFGN160D1F1B</td>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>2023-06-16 20:33:54</td>\n",
       "      <td>PROCESS_GAS_4_CH2F2#AOTU_STEP_7#MEAN</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550907</th>\n",
       "      <td>NGE714-18</td>\n",
       "      <td>11342</td>\n",
       "      <td>157510</td>\n",
       "      <td>EKT72</td>\n",
       "      <td>AFGNRE01N.0C01</td>\n",
       "      <td>L15RB20A</td>\n",
       "      <td>EKT72_PM2</td>\n",
       "      <td>NGE714.000</td>\n",
       "      <td>NEW-DRM/P2/150NM/PFGN160D1F1B</td>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>2023-06-16 20:33:54</td>\n",
       "      <td>BOTTOMFLOWRATE#AOTU_STEP_6#MEAN</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>30.0013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550908</th>\n",
       "      <td>NGE714-18</td>\n",
       "      <td>11342</td>\n",
       "      <td>157510</td>\n",
       "      <td>EKT72</td>\n",
       "      <td>AFGNRE01N.0C01</td>\n",
       "      <td>L15RB20A</td>\n",
       "      <td>EKT72_PM2</td>\n",
       "      <td>NGE714.000</td>\n",
       "      <td>NEW-DRM/P2/150NM/PFGN160D1F1B</td>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>2023-06-16 20:33:54</td>\n",
       "      <td>CHAMBER_PRESSURE#AOTU_STEP_6#RANGE</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>1.9000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550909</th>\n",
       "      <td>NGE714-18</td>\n",
       "      <td>11342</td>\n",
       "      <td>157510</td>\n",
       "      <td>EKT72</td>\n",
       "      <td>AFGNRE01N.0C01</td>\n",
       "      <td>L15RB20A</td>\n",
       "      <td>EKT72_PM2</td>\n",
       "      <td>NGE714.000</td>\n",
       "      <td>NEW-DRM/P2/150NM/PFGN160D1F1B</td>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>2023-06-16 20:33:54</td>\n",
       "      <td>EDGE_HE_FLOW#AOTU_STEP_6#MEAN</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>2.2905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550910</th>\n",
       "      <td>NGE714-18</td>\n",
       "      <td>11342</td>\n",
       "      <td>157510</td>\n",
       "      <td>EKT72</td>\n",
       "      <td>AFGNRE01N.0C01</td>\n",
       "      <td>L15RB20A</td>\n",
       "      <td>EKT72_PM2</td>\n",
       "      <td>NGE714.000</td>\n",
       "      <td>NEW-DRM/P2/150NM/PFGN160D1F1B</td>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>2023-06-16 20:33:54</td>\n",
       "      <td>LO_RF_REF_POWER#AOTU_STEP_6#MEAN</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>550911 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         WAFER_ID  TOOL_ID  RUN_ID EQP_NAME      PRODUCT_ID    PRODG1  \\\n",
       "0       NGE186-07    11341  149770    EKT72  AFKN2J01N.0U01  L11CD02A   \n",
       "1       NGE186-07    11341  149770    EKT72  AFKN2J01N.0U01  L11CD02A   \n",
       "2       NGE186-07    11341  149770    EKT72  AFKN2J01N.0U01  L11CD02A   \n",
       "3       NGE186-07    11341  149770    EKT72  AFKN2J01N.0U01  L11CD02A   \n",
       "4       NGE186-07    11341  149770    EKT72  AFKN2J01N.0U01  L11CD02A   \n",
       "...           ...      ...     ...      ...             ...       ...   \n",
       "550906  NGE714-18    11342  157510    EKT72  AFGNRE01N.0C01  L15RB20A   \n",
       "550907  NGE714-18    11342  157510    EKT72  AFGNRE01N.0C01  L15RB20A   \n",
       "550908  NGE714-18    11342  157510    EKT72  AFGNRE01N.0C01  L15RB20A   \n",
       "550909  NGE714-18    11342  157510    EKT72  AFGNRE01N.0C01  L15RB20A   \n",
       "550910  NGE714-18    11342  157510    EKT72  AFGNRE01N.0C01  L15RB20A   \n",
       "\n",
       "        TOOL_NAME      LOT_ID                    RECIPE_NAME   OPER_NO  \\\n",
       "0       EKT72_PM1  NGE186.000  NEW-DRM/P1/110NM/PFKN0S0D1F1A  1F.EEK10   \n",
       "1       EKT72_PM1  NGE186.000  NEW-DRM/P1/110NM/PFKN0S0D1F1A  1F.EEK10   \n",
       "2       EKT72_PM1  NGE186.000  NEW-DRM/P1/110NM/PFKN0S0D1F1A  1F.EEK10   \n",
       "3       EKT72_PM1  NGE186.000  NEW-DRM/P1/110NM/PFKN0S0D1F1A  1F.EEK10   \n",
       "4       EKT72_PM1  NGE186.000  NEW-DRM/P1/110NM/PFKN0S0D1F1A  1F.EEK10   \n",
       "...           ...         ...                            ...       ...   \n",
       "550906  EKT72_PM2  NGE714.000  NEW-DRM/P2/150NM/PFGN160D1F1B  1F.EEK10   \n",
       "550907  EKT72_PM2  NGE714.000  NEW-DRM/P2/150NM/PFGN160D1F1B  1F.EEK10   \n",
       "550908  EKT72_PM2  NGE714.000  NEW-DRM/P2/150NM/PFGN160D1F1B  1F.EEK10   \n",
       "550909  EKT72_PM2  NGE714.000  NEW-DRM/P2/150NM/PFGN160D1F1B  1F.EEK10   \n",
       "550910  EKT72_PM2  NGE714.000  NEW-DRM/P2/150NM/PFGN160D1F1B  1F.EEK10   \n",
       "\n",
       "                 START_TIME                       parametric_name   CASE_INFO  \\\n",
       "0       2023-06-16 02:15:06        PROCESS_GAS_10_CO#WINDOW_1#SUM  2023-06-16   \n",
       "1       2023-06-16 02:15:06       PROCESS_GAS_3_C4F6#WINDOW_1#SUM  2023-06-16   \n",
       "2       2023-06-16 02:15:06    PROCESS_GAS_14_O2#AOTU_STEP_2#MEAN  2023-06-16   \n",
       "3       2023-06-16 02:15:06       BOTTOMFLOWRATE#AOTU_STEP_2#MEAN  2023-06-16   \n",
       "4       2023-06-16 02:15:06      PROCESS_GAS_12_CH3F#WINDOW_1#SUM  2023-06-16   \n",
       "...                     ...                                   ...         ...   \n",
       "550906  2023-06-16 20:33:54  PROCESS_GAS_4_CH2F2#AOTU_STEP_7#MEAN  2023-06-16   \n",
       "550907  2023-06-16 20:33:54       BOTTOMFLOWRATE#AOTU_STEP_6#MEAN  2023-06-16   \n",
       "550908  2023-06-16 20:33:54    CHAMBER_PRESSURE#AOTU_STEP_6#RANGE  2023-06-16   \n",
       "550909  2023-06-16 20:33:54         EDGE_HE_FLOW#AOTU_STEP_6#MEAN  2023-06-16   \n",
       "550910  2023-06-16 20:33:54      LO_RF_REF_POWER#AOTU_STEP_6#MEAN  2023-06-16   \n",
       "\n",
       "        STATUS  STATISTIC_RESULT  label  \n",
       "0       NORMAL            0.0000      0  \n",
       "1       NORMAL            0.0000      0  \n",
       "2       NORMAL            7.5000      0  \n",
       "3       NORMAL           29.9930      0  \n",
       "4       NORMAL            0.0000      0  \n",
       "...        ...               ...    ...  \n",
       "550906  NORMAL            0.0000      0  \n",
       "550907  NORMAL           30.0013      0  \n",
       "550908  NORMAL            1.9000      0  \n",
       "550909  NORMAL            2.2905      0  \n",
       "550910  NORMAL            0.9905      0  \n",
       "\n",
       "[550911 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_pandas = pd.read_csv(\"DWD_POC_CASE_FD_UVA_DATA_CASE1_PROCESSED1.csv\")\n",
    "df1_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532d39bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550911"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = ps.from_pandas(df1_pandas).to_spark()\n",
    "\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc892ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685\n",
      "3409\n"
     ]
    }
   ],
   "source": [
    "bad_wafer_num = df1.filter(\"label == 1\").select('WAFER_ID').distinct().count()\n",
    "good_wafer_num = df1.filter(\"label == 0\").select('WAFER_ID').distinct().count()\n",
    "print(bad_wafer_num)\n",
    "print(good_wafer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beff31c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df1.filter(\"label == 1\").select('WAFER_ID').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7ec1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "##################################FDC数据预处理###############################\n",
    "############################################################################\n",
    "def _pre_process(df):\n",
    "    \"\"\"\n",
    "    param df: 从数据库中读取出来的某个CASE数据\n",
    "    return: 数据预处理，后面要根据实际情况统一添加\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 只选出会用到的列\n",
    "        df = df.select('WAFER_ID', 'TOOL_ID', 'RUN_ID', 'EQP_NAME', 'PRODUCT_ID', 'PRODG1', \n",
    "                       'TOOL_NAME', 'OPER_NO', 'parametric_name', 'STATISTIC_RESULT', 'label')\n",
    "        # 剔除NA值\n",
    "        df = df.filter(col('STATISTIC_RESULT').isNotNull())\n",
    "        # 按照所有的行进行去重\n",
    "        df1 = df.dropDuplicates()\n",
    "        # 选最新的RUN\n",
    "        df2 = df1.groupBy('WAFER_ID', 'OPER_NO', 'TOOL_ID').agg(max('RUN_ID').alias('RUN_ID'))\n",
    "        df_run = df1.join(df2.dropDuplicates(subset=['WAFER_ID', 'OPER_NO', 'TOOL_ID', 'RUN_ID']),\n",
    "                                    on=['WAFER_ID', 'OPER_NO', 'TOOL_ID', 'RUN_ID'], how='inner')\n",
    "        return df_run\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def commonality_analysis(df_run, grpby_list):\n",
    "    \"\"\"\n",
    "    param df_run: 数据预处理后的数据\n",
    "    return: 共性分析后的结果， 返回bad wafer前五的组合\n",
    "    \"\"\"\n",
    "    try:\n",
    "        grps = (df_run.groupBy(grpby_list)\n",
    "                            .agg(countDistinct('WAFER_ID').alias('wafer_count'),\n",
    "                  countDistinct('WAFER_ID', when(df_run['label'] == 0, 1)).alias('good_num'),\n",
    "                  countDistinct('WAFER_ID', when(df_run['label'] == 1, 1)).alias('bad_num'))\n",
    "                  .orderBy('bad_num', ascending=False))\n",
    "\n",
    "        # 单站点+单腔室的情况\n",
    "        if grps.count() == 1:\n",
    "            return grps\n",
    "        else: \n",
    "            grps = grps.filter(grps['bad_num'] > 0)\n",
    "            window_sep = Window().orderBy(col(\"bad_num\").desc())\n",
    "            ranked_df = grps.withColumn(\"rank\", rank().over(window_sep))\n",
    "            grpss = ranked_df.filter(col(\"rank\") <= 5).drop(\"rank\")\n",
    "        return grpss\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75015d76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550896\n"
     ]
    }
   ],
   "source": [
    "df_run = _pre_process(df1)\n",
    "print(df_run.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20290123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+-----------+--------+-------+\n",
      "|  PRODG1| OPER_NO|TOOL_NAME|wafer_count|good_num|bad_num|\n",
      "+--------+--------+---------+-----------+--------+-------+\n",
      "|L11TG07A|1F.EEK10|EKT72_PM1|       1000|       0|   1000|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|        959|     234|    725|\n",
      "|L11TG05A|1F.EEK10|EKT72_PM1|        299|       0|    299|\n",
      "|L11EG07A|1F.EEK10|EKT72_PM1|        224|       0|    224|\n",
      "|L15KD03A|1F.EEK10|EKT72_PM1|        101|       0|    101|\n",
      "+--------+--------+---------+-----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grpby_list = ['PRODG1', 'OPER_NO', 'TOOL_NAME']\n",
    "common_res = commonality_analysis(df_run, grpby_list)\n",
    "common_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88bc19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ba88cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#################################获取有good和bad的数据#########################\n",
    "############################################################################\n",
    "def get_data_list_big_sample(common_res, grpby_list):\n",
    "    \"\"\"\n",
    "    param common_res: 共性分析后的结果, 按照good_num >= 3 AND bad_num >= 3筛选出组合\n",
    "    return: 对应组合的字典形式，包在一个大列表中\n",
    "    \"\"\"\n",
    "    try:\n",
    "        good_bad_grps = common_res.filter(\"good_num >= 3 AND bad_num >= 3\")\n",
    "\n",
    "        if 'PRODG1' in grpby_list:\n",
    "            data_list = good_bad_grps['PRODG1', 'OPER_NO', 'TOOL_NAME'].collect()\n",
    "        else:\n",
    "            data_list = good_bad_grps['OPER_NO', 'TOOL_NAME'].collect()\n",
    "\n",
    "        data_dict_list = [row.asDict() for row in data_list]\n",
    "        return data_dict_list\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_train_data_big_sample(df_run, data_dict_list):\n",
    "    \"\"\"\n",
    "    param df_run: 数据预处理后的数据\n",
    "    param data_dict: 筛选后的字典结果(get_data_list_big_sample)\n",
    "    return: 从原始数据中过滤出大样本组合的数据\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(data_dict_list[0]) == 3:\n",
    "            prod, oper, tool = data_dict_list[0]['PRODG1'], data_dict_list[0]['OPER_NO'], data_dict_list[0]['TOOL_NAME']\n",
    "            df_s = df_run.filter(\"PRODG1 == '{}' AND OPER_NO == '{}' AND TOOL_NAME == '{}'\".format(prod, oper, tool))\n",
    "            for i in range(1, len(data_dict_list)):\n",
    "                prod, oper, tool = data_dict_list[i]['PRODG1'], data_dict_list[i]['OPER_NO'], data_dict_list[i]['TOOL_NAME']\n",
    "                df_m = df_run.filter(\"PRODG1 == '{}' AND OPER_NO == '{}' and TOOL_NAME == '{}'\".format(prod, oper, tool))\n",
    "                df_s = df_s.union(df_m)\n",
    "\n",
    "        else: \n",
    "            oper, tool = data_dict_list[0]['OPER_NO'], data_dict_list[0]['TOOL_NAME']\n",
    "            df_s = df_run.filter(\"OPER_NO == '{}' AND TOOL_NAME == '{}'\".format(oper, tool))\n",
    "            for i in range(1, len(data_dict_list)):\n",
    "                oper, tool = data_dict_list[i]['OPER_NO'], data_dict_list[i]['TOOL_NAME']\n",
    "                df_m = df_run.filter(\"OPER_NO == '{}' and TOOL_NAME == '{}'\".format(oper, tool))\n",
    "                df_s = df_s.union(df_m)\n",
    "        return df_s\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bed8418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'PRODG1': 'L11CD02A', 'OPER_NO': '1F.EEK10', 'TOOL_NAME': 'EKT72_PM1'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict_list_bs = get_data_list_big_sample(common_res=common_res, grpby_list=grpby_list)\n",
    "data_dict_list_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "127c35c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57540\n"
     ]
    }
   ],
   "source": [
    "df_run_bs = get_train_data_big_sample(df_run=df_run, data_dict_list=data_dict_list_bs)\n",
    "print(df_run_bs.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280b1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a569a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#########################获取传入的整个数据中的所有bad_wafer个数####################\n",
    "############################################################################\n",
    "def get_all_bad_wafer_num(df):\n",
    "    \"\"\"\n",
    "    param df: 筛选后的数据\n",
    "    return: 数据中所有bad_wafer的数量\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return df.filter(\"label == 1\").select('WAFER_ID').distinct().count()\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25b9cc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "725"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_wafer_num_big_sample = get_all_bad_wafer_num(df_run_bs)\n",
    "bad_wafer_num_big_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515e601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe3b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "###########################对有good和bad的数据，建模############################\n",
    "############################################################################\n",
    "def fit_rf_big_sample(df, by):\n",
    "    \"\"\"\n",
    "    param df: 大样本组合的数据\n",
    "    param by: 分组字段\n",
    "    return: RandomForest建模后的结果\n",
    "    \"\"\"\n",
    "    try:\n",
    "        schema_all = StructType([StructField(\"PRODG1\", StringType(), True),\n",
    "                                 StructField(\"OPER_NO\", StringType(), True),\n",
    "                                 StructField(\"TOOL_NAME\", StringType(), True),\n",
    "                                 StructField(\"bad_wafer\", IntegerType(), True),\n",
    "                                 StructField(\"roc_auc_score\", FloatType(), True),\n",
    "                                 StructField(\"features\", StringType(), True),\n",
    "                                 StructField(\"importance\", FloatType(), True)]) \n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_model_result(df_run):\n",
    "            if len(by) == 3: \n",
    "                df_pivot = df_run.dropna(axis=0).pivot_table(index=['WAFER_ID', 'label'], \n",
    "                                                             columns=['PRODG1', 'OPER_NO', 'TOOL_NAME', 'parametric_name'],\n",
    "                                                             values=['STATISTIC_RESULT'])\n",
    "            else:\n",
    "                df_pivot = df_run.dropna(axis=0).pivot_table(index=['WAFER_ID', 'label'], \n",
    "                                                             columns=['OPER_NO', 'TOOL_NAME', 'parametric_name'],\n",
    "                                                             values=['STATISTIC_RESULT'])\n",
    "\n",
    "            df_pivot.columns = df_pivot.columns.map('#'.join)\n",
    "            df_pivot = df_pivot.fillna(df_pivot.mean()).reset_index(drop=False)\n",
    "\n",
    "            # 定义自变量和因变量\n",
    "            X_train = df_pivot[df_pivot.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "            y_train = df_pivot[['label']]\n",
    "\n",
    "            z_ratio = y_train.value_counts(normalize = True)\n",
    "            good_ratio = z_ratio[0]\n",
    "            bad_ratio = z_ratio[1]\n",
    "            if abs(good_ratio - bad_ratio) > 0.7:\n",
    "                undersampler = ClusterCentroids(random_state=101)\n",
    "                X_train, y_train = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "            # 网格搜索\n",
    "            pipe = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value=-999)),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('model', RandomForestClassifier())])\n",
    "            param_grid = {'model__n_estimators': [*range(50, 100, 10)],\n",
    "                                    'model__max_depth': [*range(10, 50, 10)]}\n",
    "            grid = GridSearchCV(estimator=pipe, scoring='roc_auc', param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "            grid.fit(X_train.values, y_train.values.ravel())\n",
    "            roc_auc_score_ = grid.best_score_\n",
    "\n",
    "            # 特征重要度、结果汇总\n",
    "            small_importance_res = pd.DataFrame({\n",
    "                'features': X_train.columns,\n",
    "                'importance': grid.best_estimator_.steps[2][1].feature_importances_}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "            if len(by) == 3: \n",
    "                small_sample_res = pd.DataFrame({'PRODG1': df_run['PRODG1'].unique(),\n",
    "                                                'OPER_NO': df_run['OPER_NO'].unique(),\n",
    "                                                'TOOL_NAME': df_run['TOOL_NAME'].unique(),\n",
    "                                                'bad_wafer': sum(df_pivot['label']),\n",
    "                                                'roc_auc_score': roc_auc_score_})\n",
    "            else:\n",
    "                PRODG1 = 'grplen2'\n",
    "                small_sample_res = pd.DataFrame({'PRODG1': PRODG1,\n",
    "                                                'OPER_NO': df_run['OPER_NO'].unique(),\n",
    "                                                'TOOL_NAME': df_run['TOOL_NAME'].unique(),\n",
    "                                                'bad_wafer': sum(df_pivot['label']),\n",
    "                                                'roc_auc_score': roc_auc_score_})\n",
    "\n",
    "            return pd.concat([small_importance_res, small_sample_res])\n",
    "        return df.groupby(by).apply(get_model_result)\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99fa511f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+---------+-------------+--------------------+-----------+\n",
      "|PRODG1|OPER_NO|TOOL_NAME|bad_wafer|roc_auc_score|            features| importance|\n",
      "+------+-------+---------+---------+-------------+--------------------+-----------+\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...| 0.15355894|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...| 0.12863249|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...| 0.10832934|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.063552104|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.056137845|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.044394266|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...| 0.04281729|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...| 0.03856614|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.031577896|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...| 0.03099188|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.028279396|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.027067771|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.026425932|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.025893325|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.025835106|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...| 0.02226231|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.019962393|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.018707264|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.018130375|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.015819833|\n",
      "+------+-------+---------+---------+-------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = fit_rf_big_sample(df=df_run_bs, by=grpby_list)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bf9431b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['STATISTIC_RESULT#L11CD02A#1F.EEK10#EKT72_PM1#LO_RF_VPP#AOTU_STEP_2#MEAN'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.toPandas()[['features']].iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "64f25fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.filter(col('PRODG1').isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d25c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dddfc281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#########################对有good和bad的建模后的结果进行整合###############################\n",
    "#####################################################################################\n",
    "def split_score_big_sample(df, by):\n",
    "    \"\"\"\n",
    "    param df: RandomForest建模后的结果\n",
    "    param by: 分组字段\n",
    "    return: roc_auc分数结果\n",
    "    \"\"\"\n",
    "    try:\n",
    "        schema_all = StructType([ StructField(\"PRODG1\", StringType(), True),\n",
    "                                  StructField(\"OPER_NO\", StringType(), True),\n",
    "                                  StructField(\"TOOL_NAME\", StringType(), True),\n",
    "                                  StructField(\"bad_wafer\", IntegerType(), True),\n",
    "                                  StructField(\"roc_auc_score\", FloatType(), True) ]) \n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_result(model_results):\n",
    "            sample_res = model_results[['PRODG1', 'OPER_NO', 'TOOL_NAME', 'bad_wafer', 'roc_auc_score']].dropna(axis=0)\n",
    "            sample_res = sample_res[sample_res['roc_auc_score'] > 0.6]\n",
    "            return sample_res\n",
    "        return df.groupby(by).apply(get_result)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "        \n",
    "    \n",
    "\n",
    "def split_calculate_features_big_sample(df, by):\n",
    "    \"\"\"\n",
    "    param df: RandomForest建模后的结果\n",
    "    param by: 分组字段\n",
    "    return: features和importance结果\n",
    "    \"\"\"\n",
    "    try:\n",
    "        schema_all = StructType([ StructField(\"PRODG1\", StringType(), True),\n",
    "                                  StructField(\"OPER_NO\", StringType(), True),\n",
    "                                  StructField(\"TOOL_NAME\", StringType(), True),\n",
    "                                  StructField(\"parametric_name\", StringType(), True),\n",
    "                                  StructField(\"importance\", FloatType(), True),\n",
    "                                  StructField(\"stats\", StringType(), True)]) \n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_result(model_results):\n",
    "            # 先从随机森林的模型结果中取出包含features和importance的dataframe\n",
    "            feature_importance_table = model_results[['features', 'importance']].dropna(axis=0)\n",
    "\n",
    "            if len(by) == 3:\n",
    "                # 对其进行split\n",
    "                feature_importance_table['STATISTIC_RESULT'] = feature_importance_table['features'].apply(lambda x: x.split('#')[0])\n",
    "                feature_importance_table['PRODG1'] = feature_importance_table['features'].apply(lambda x: x.split('#')[1])\n",
    "                feature_importance_table['OPER_NO'] = feature_importance_table['features'].apply(lambda x: x.split('#')[2])\n",
    "                feature_importance_table['TOOL_NAME'] = feature_importance_table['features'].apply(lambda x: x.split('#')[3])\n",
    "                feature_importance_table['parametric_name'] = feature_importance_table['features'].apply(lambda x: x.split('#')[4])\n",
    "                feature_importance_table['step'] = feature_importance_table['features'].apply(lambda x: x.split('#')[5])\n",
    "                feature_importance_table['stats'] = feature_importance_table['features'].apply(lambda x: x.split('#')[6])\n",
    "                feature_importance_res_split = feature_importance_table.drop(['features', 'STATISTIC_RESULT'], axis=1).reset_index(drop=True)\n",
    "            else:\n",
    "                feature_importance_table['STATISTIC_RESULT'] = feature_importance_table['features'].apply(lambda x: x.split('#')[0])\n",
    "                feature_importance_table['OPER_NO'] = feature_importance_table['features'].apply(lambda x: x.split('#')[1])\n",
    "                feature_importance_table['TOOL_NAME'] = feature_importance_table['features'].apply(lambda x: x.split('#')[2])\n",
    "                feature_importance_table['parametric_name'] = feature_importance_table['features'].apply(lambda x: x.split('#')[3])\n",
    "                feature_importance_table['step'] = feature_importance_table['features'].apply(lambda x: x.split('#')[4])\n",
    "                feature_importance_table['stats'] = feature_importance_table['features'].apply(lambda x: x.split('#')[5])\n",
    "                feature_importance_res_split = feature_importance_table.drop(['features', 'STATISTIC_RESULT'], axis=1).reset_index(drop=True)\n",
    "                feature_importance_res_split = feature_importance_res_split.assign(PRODG1 = 'grplen2')\n",
    "\n",
    "            # 去除importance为0的组合\n",
    "            feature_importance_res_split_drop = feature_importance_res_split.query(\"importance > 0\").reset_index(drop=True)\n",
    "\n",
    "            #取每一种组合结果的前60%\n",
    "            feature_importance_res_split_nlargest = (feature_importance_res_split_drop.groupby(by = ['PRODG1', 'OPER_NO', 'TOOL_NAME'])\n",
    "                                                    .apply(lambda x:x.nlargest(int(x.shape[0]*0.6), 'importance'))\n",
    "                                                    .reset_index(drop=True))\n",
    "\n",
    "            # 新增一列，含有参数的所有统计特征:feature_stats\n",
    "            feature_stats = feature_importance_res_split_nlargest.groupby(['PRODG1', 'OPER_NO', 'TOOL_NAME', 'parametric_name','step'])['stats'].unique().reset_index()\n",
    "            feature_stats['stats'] = [feature_stats['stats'].iloc[i].tolist() for i in range(len(feature_stats))]\n",
    "            feature_stats['stats'] = feature_stats['stats'].apply(lambda x: \"#\".join(x))\n",
    "            feature_stats = feature_stats.assign(parametric_name=lambda x: x['parametric_name']+str('#')+x['step']).drop('step', axis=1)\n",
    "\n",
    "            #对同一种组合里的同一个参数进行求和:feature_importance_groupby\n",
    "            feature_importance_groupby = (feature_importance_res_split_nlargest.groupby(['PRODG1', 'OPER_NO', 'TOOL_NAME', \n",
    "                                                                                         'parametric_name','step'])['importance'].sum().reset_index())\n",
    "            feature_importance_groupby = feature_importance_groupby.assign(parametric_name=lambda x: x['parametric_name']+str('#')+x['step']).drop('step', axis=1)\n",
    "\n",
    "            # feature_stats和feature_importance_groupby连接\n",
    "            grpby_stats = pd.merge(feature_stats, feature_importance_groupby, on=['PRODG1', 'OPER_NO', 'TOOL_NAME', 'parametric_name']).dropna().reset_index(drop=True)\n",
    "            return grpby_stats\n",
    "        return df.groupby(by).apply(get_result)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def get_finall_results_big_sample(s_res, f_res, bad_wafer_num):\n",
    "    \"\"\"\n",
    "    param s_res: roc_auc分数结果\n",
    "    param f_res: features和importance结果\n",
    "    param bad_wafer_num: 数据中所有bad_wafer的数量\n",
    "    return: 最后的建模结果\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # feature_importance_groupby和sample_res连接\n",
    "        roc_auc_score_all = s_res.agg({\"roc_auc_score\": \"sum\"}).collect()[0][0]\n",
    "        s_res = s_res.withColumn(\"roc_auc_score_ratio\", col('roc_auc_score')/roc_auc_score_all)\n",
    "        s_res = s_res.withColumn('bad_ratio', col('bad_wafer')/bad_wafer_num)\n",
    "        \n",
    "        df_merge = s_res.join(f_res, on=['PRODG1', 'OPER_NO', 'TOOL_NAME'], how='left')\n",
    "        df_merge = df_merge.withColumn('weight_original', col('roc_auc_score_ratio')*col('bad_ratio')*col('importance'))\n",
    "                                       \n",
    "        # 最后再次进行归一化\n",
    "        weight_all = df_merge.agg({\"weight_original\": \"sum\"}).collect()[0][0]\n",
    "        df_merge = df_merge.withColumn('weight', col('weight_original')/weight_all)\n",
    "        df_merge = df_merge.select(['PRODG1', 'OPER_NO', 'TOOL_NAME', 'parametric_name', 'weight', 'stats']).orderBy('weight', ascending=False)\n",
    "        return df_merge\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42947aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+---------+-------------+\n",
      "|  PRODG1| OPER_NO|TOOL_NAME|bad_wafer|roc_auc_score|\n",
      "+--------+--------+---------+---------+-------------+\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      725|   0.86313295|\n",
      "+--------+--------+---------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_res = split_score_big_sample(df=res, by=['PRODG1', 'OPER_NO', 'TOOL_NAME'])\n",
    "s_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a54fb92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+--------------------+-----------+----------+\n",
      "|  PRODG1| OPER_NO|TOOL_NAME|     parametric_name| importance|     stats|\n",
      "+--------+--------+---------+--------------------+-----------+----------+\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|CENTER_GAS_PRESSU...| 0.02997711|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|CHAMBER_PRESSURE#...|0.032337744|     RANGE|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|EDGE_GAS_PRESSURE...|0.028183555|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|EDGE_HE_FLOW#AOTU...| 0.03766934|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|FLOWSPLITCENTER#A...| 0.02766729|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|FLOWSPLITEDGE#AOT...|0.025220623|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LOWERBRINETEMP#AO...|0.020730473|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LOWER_TEMPERATURE...| 0.04409152|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_C1_VAR_CAPACIT...| 0.04655628|     SLOPE|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_C2_VAR_CAPACIT...|0.033808645|     RANGE|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_RF_REF_POWER#A...|0.053021178|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_RF_VPP#AOTU_ST...| 0.16177113|MEAN#SLOPE|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_RF_VPP#STEP2_M...| 0.10810305|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_RF_VPP#STEP2_MINI| 0.10843501|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|PROCESS_GAS_8_O2#...| 0.06857979|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|UPPER_TEMPERATURE...|0.025222626|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|WALL_TEMPERATURE#...|0.019340562|      MEAN|\n",
      "+--------+--------+---------+--------------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_res = split_calculate_features_big_sample(df=res, by=grpby_list)\n",
    "f_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "047840ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+--------------------+--------------------+----------+\n",
      "|  PRODG1| OPER_NO|TOOL_NAME|     parametric_name|              weight|     stats|\n",
      "+--------+--------+---------+--------------------+--------------------+----------+\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_RF_VPP#STEP2_M...| 0.17365270525150364|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_RF_VPP#STEP2_MINI| 0.16633419419766116|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_RF_VPP#AOTU_ST...|  0.1503439847670655|MEAN#SLOPE|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LOWER_TEMPERATURE...| 0.06521256721255377|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_RF_REF_POWER#A...| 0.05011106527960036|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_C1_VAR_CAPACIT...|0.047430968067066814|     SLOPE|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|PROCESS_GAS_8_O2#...| 0.04612305319105883|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|FLOWSPLITCENTER#A...| 0.03536474700960457|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_C2_VAR_CAPACIT...| 0.03409484354197643|     RANGE|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|CHAMBER_PRESSURE#...| 0.03402422160369392|     RANGE|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|FLOWSPLITEDGE#AOT...| 0.03302091328043849|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|CENTER_GAS_PRESSU...|0.032732464462702615|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|EDGE_GAS_PRESSURE...|0.030656801668660517|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|EDGE_HE_FLOW#AOTU...|0.028174839296477044|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_RF_POWER#AOTU_...|0.026474026547115906|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|UPPER_TEMPERATURE...| 0.02430050009164091|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|BOTTOMFLOWRATE#AO...| 0.02412132605506586|      MEAN|\n",
      "+--------+--------+---------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_res_bs = get_finall_results_big_sample(s_res=s_res, f_res=f_res, bad_wafer_num=bad_wafer_num_big_sample)\n",
    "model_res_bs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7421d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0d4b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_score_all = s_res.agg({\"roc_auc_score\": \"sum\"}).collect()[0][0]\n",
    "# s_res = s_res.withColumn(\"roc_auc_score_ratio\", col('roc_auc_score')/roc_auc_score_all)\n",
    "# s_res = s_res.withColumn('bad_ratio', col('bad_wafer')/bad_wafer_num_big_sample)\n",
    "# s_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9c2f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merge = s_res.join(f_res, on=['PRODG1', 'OPER_NO', 'TOOL_NAME'], how='left')\n",
    "# df_merge.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b87fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb7f1232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#############################将建模后的结果增加特定的列####################################\n",
    "#####################################################################################\n",
    "def add_certain_column(df, by, request_id):\n",
    "    \"\"\"\n",
    "    param df: 最后的建模结果\n",
    "    param by: 分组字段, 手动增加一列add\n",
    "    param request_id: 传入的request_id\n",
    "    return: 最后的建模结果增加特定的列\n",
    "    \"\"\"\n",
    "    try:\n",
    "        schema_all = StructType([StructField(\"PRODG1\", StringType(), True),\n",
    "                                  StructField(\"OPER_NO\", StringType(), True),\n",
    "                                  StructField(\"TOOL_NAME\", StringType(), True),\n",
    "                                  StructField(\"stats\", StringType(), True),\n",
    "                                  StructField(\"parametric_name\", StringType(), True),\n",
    "                                  StructField(\"weight\", FloatType(), True),\n",
    "                                  StructField(\"request_id\", StringType(), True),\n",
    "                                  StructField(\"weight_percent\", FloatType(), True),\n",
    "                                  StructField(\"index_no\", IntegerType(), True)]) \n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_result(final_res):\n",
    "            final_res['weight'] = final_res['weight'].astype(float)\n",
    "            final_res = final_res.query(\"weight > 0\")\n",
    "            final_res['request_id'] = request_id\n",
    "            final_res['weight_percent'] = final_res['weight']*100\n",
    "            final_res = final_res.sort_values('weight', ascending=False)\n",
    "            final_res['index_no'] = [i+1 for i in range(len(final_res))]\n",
    "            final_res = final_res.drop('add', axis=1)\n",
    "            # final_res['parametric_name'] = final_res['parametric_name'].str.replace(\"_\", \"+\")\n",
    "            return final_res\n",
    "        return df.groupby(by).apply(get_result)\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c14dac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "|  PRODG1| OPER_NO|TOOL_NAME|     stats|     parametric_name|     weight|request_id|weight_percent|index_no|\n",
      "+--------+--------+---------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|MEAN#SLOPE|LO_RF_VPP#AOTU_ST...| 0.16034496|     addcd|     16.034496|       1|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|LO_RF_VPP#STEP2_M...| 0.15882933|     addcd|     15.882933|       2|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|LO_RF_VPP#STEP2_MINI| 0.14967404|     addcd|     14.967404|       3|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|PROCESS_GAS_8_O2#...|0.076276235|     addcd|     7.6276236|       4|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|LO_RF_REF_POWER#A...|0.057714064|     addcd|      5.771406|       5|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|LOWER_TEMPERATURE...|0.056541007|     addcd|      5.654101|       6|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|     RANGE|LO_C2_VAR_CAPACIT...|0.054289375|     addcd|     5.4289374|       7|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|     SLOPE|LO_C1_VAR_CAPACIT...|0.051312104|     addcd|     5.1312103|       8|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|EDGE_HE_FLOW#AOTU...|0.034939118|     addcd|     3.4939117|       9|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|CENTER_GAS_PRESSU...|0.032012407|     addcd|     3.2012408|      10|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|FLOWSPLITEDGE#AOT...|0.031212686|     addcd|     3.1212687|      11|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|FLOWSPLITCENTER#A...|0.030581318|     addcd|     3.0581317|      12|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|     RANGE|CHAMBER_PRESSURE#...|0.030457245|     addcd|     3.0457244|      13|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|EDGE_GAS_PRESSURE...|0.025227342|     addcd|     2.5227342|      14|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|UPPER_TEMPERATURE...| 0.02199403|     addcd|      2.199403|      15|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|     RANGE|LO_C1_VAR_CAPACIT...|0.019328464|     addcd|     1.9328463|      16|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|PROCESS_GAS_7_AR#...|0.018696336|     addcd|     1.8696336|      17|\n",
      "+--------+--------+---------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "request_id = 'addcd'\n",
    "final_res_bs = model_res_bs.withColumn('add', lit(0))\n",
    "\n",
    "final_res_bs_add_columns = add_certain_column(df=final_res_bs, by='add', request_id=request_id)\n",
    "final_res_bs_add_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7883291f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requestId</th>\n",
       "      <th>requestParam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ssssss</td>\n",
       "      <td>{'date': {}, 'prod': [], 'mergeProdg1': '1'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  requestId                                  requestParam\n",
       "0    ssssss  {'date': {}, 'prod': [], 'mergeProdg1': '1'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_loads_dict = {\n",
    "    \"requestId\" : \"ssssss\",\n",
    "    \"requestParam\": [\n",
    "        {\n",
    "            \"date\": {},\n",
    "            \"prod\": [],\n",
    "            \"mergeProdg1\": \"1\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "df2 = pd.DataFrame(json_loads_dict)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d034c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认只取第一行\n",
    "if len(df2) > 0:\n",
    "    df2 = df2.head(1)\n",
    "request_id = df2[\"requestId\"].values[0]\n",
    "request_params = df2[\"requestParam\"].values[0]\n",
    "# request_params = request_params.replace('\\'', \"\\\"\")    # 避免存在单引号，因为json 引号只有双引号\n",
    "# parse_dict = json.loads(request_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa42114c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97dc14d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merge_prodg(df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    0：未选中PRODG1(默认)\n",
    "    1：选中PRODG1,按照OPER+TOOL进行分组\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(df) > 0:\n",
    "            df = df.head(1)\n",
    "        request_params = df[\"requestParam\"].values[0]\n",
    "        merge_prodg1 = request_params['mergeProdg1']\n",
    "        \n",
    "        if merge_prodg1 == '1':\n",
    "            grpby_list = ['OPER_NO', 'TOOL_NAME']\n",
    "        elif merge_prodg1 == '0':\n",
    "            grpby_list = ['PRODG1', 'OPER_NO', 'TOOL_NAME']\n",
    "        else:\n",
    "            raise ValueError\n",
    "        return grpby_list\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38dd137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(df, msg, request_id):\n",
    "    if df is None or df.count() == 0:\n",
    "        df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": f'{msg}',  'requestId': request_id}, index=[0])\n",
    "        df11 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26c4c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpby_list = ['OPER_NO', 'TOOL_NAME', 'PRODG1']\n",
    "request_id = 'addcd'\n",
    "df11 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a9a4e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffff\n"
     ]
    }
   ],
   "source": [
    "# 主程序\n",
    "try:\n",
    "    # 1. 数据预处理\n",
    "    df_run = _pre_process(df1)\n",
    "    msg = '该条件下数据库中暂无数据，请检查！'\n",
    "    check(df_run, msg, request_id)\n",
    "    \n",
    "    # 2. 进行共性分析\n",
    "    common_res = commonality_analysis(df_run, grpby_list)\n",
    "    msg = '共性分析结果异常！'\n",
    "    check(df_run, msg, request_id)\n",
    "\n",
    "    # 3. 挑选出数据：bad和good要同时大于3\n",
    "    data_dict_list_bs = get_data_list_big_sample(common_res=common_res, grpby_list=grpby_list)\n",
    "    if data_dict_list_bs is None or len(data_dict_list_bs) == 0:\n",
    "        print('data_dict_list_bs')\n",
    "        msg = '以PRODG1+OPER_NO+TOOL_NAME分组或以OPER_NO+TOOL_NAME分组的情况下, BAD_WAFER和GOOD_WAFER数量都要大于3片, 请检查数据！'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": f'{msg}', 'requestId': request_id}, index=[0])\n",
    "        df11 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "        \n",
    "    df_run_bs = get_train_data_big_sample(df_run=df_run, data_dict_list=data_dict_list_bs)\n",
    "    if df_run_bs is None or df_run_bs.count() == 0:\n",
    "        print('df_run_bs')\n",
    "        df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": '数据库中暂无此类数据！',  'requestId': request_id}, index=[0])\n",
    "        df11 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "        \n",
    "    # 4. 获取所有bad wafer数量\n",
    "    bad_wafer_num_big_sample = get_all_bad_wafer_num(df_run_bs)\n",
    "    if bad_wafer_num_big_sample is None or bad_wafer_num_big_sample < 3:\n",
    "        df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": '数据库中实际BAD_WAFER数量小于3片, 请提供更多的BAD_WAFER数量! ', 'requestId': request_id}, index=[0])\n",
    "        df11 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "        \n",
    "    # 5. 对挑选出的大样本数据进行建模\n",
    "    res = fit_rf_big_sample(df=df_run_bs, by=grpby_list)\n",
    "    if res is None or res.count() == 0 :\n",
    "        df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": '算法内部暂时异常! ',  'requestId': request_id}, index=[0])\n",
    "        df11 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "        \n",
    "    # 6. 将建模结果进行整合\n",
    "    s_res = split_score_big_sample(df=res, by=['PRODG1', 'OPER_NO', 'TOOL_NAME'])\n",
    "    if s_res is None or s_res.count() == 0:\n",
    "        df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": '算法运行评分结果较低,暂无输出,建议增加BAD_WAFER数量', 'requestId': request_id}, index=[0])\n",
    "        df11 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "        \n",
    "    f_res = split_calculate_features_big_sample(df=res, by=grpby_list)\n",
    "    if f_res is None or f_res.count() == 0:\n",
    "        df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": '算法结果求和暂时异常！', 'requestId': request_id}, index=[0])\n",
    "        df11 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "    \n",
    "    model_res_bs = get_finall_results_big_sample(s_res=s_res, f_res=f_res, bad_wafer_num=bad_wafer_num_big_sample)\n",
    "    if model_res_bs is None or model_res_bs.count() == 0:\n",
    "        df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": '算法结果拼接暂时异常！', 'requestId': request_id}, index=[0])\n",
    "        df11 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "        \n",
    "    # 7. 增加特定的列\n",
    "    final_res_bs = model_res_bs.withColumn('add', lit(0))\n",
    "    final_res_bs_add_columns = add_certain_column(df=final_res_bs, by='add', request_id=request_id)\n",
    "    if final_res_bs_add_columns is None or final_res_bs_add_columns.count() == 0:\n",
    "        df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": '算法结果增加列暂时异常！', 'requestId': request_id}, index=[0])\n",
    "        df11 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "\n",
    "    df_kafka = pd.DataFrame({\"code\": 0, \"msg\": '运行成功', 'requestId': request_id}, index=[0])\n",
    "    df11 = spark.createDataFrame(df_kafka)\n",
    "    print('D')\n",
    "\n",
    "except ValueError as ve:\n",
    "    # 如果任何函数发生异常，会在这里捕获 ValueError 异常，不执行下面的代码\n",
    "    print('qqq')\n",
    "    pass\n",
    "\n",
    "except Exception as e:\n",
    "    # 如果发生其他未知异常，会在这里捕获 Exception 异常\n",
    "    print('ffff')\n",
    "    df_kafka = pd.DataFrame({\"code\": 0, \"msg\": f\"主程序发生异常：{str(e)}\", 'requestId': request_id}, index=[0])\n",
    "    df11 = spark.createDataFrame(df_kafka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ebd69ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'主程序发生异常：An error occurred while calling o980.count.\\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 974.0 failed 1 times, most recent failure: Lost task 7.0 in stage 974.0 (TID 1992) (IKAS-NB-203.oa.ikasinfo.com executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\\r\\n\\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\\r\\n\\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\\r\\n\\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\\r\\n\\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\\r\\n\\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\\r\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\\r\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\\r\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\r\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\\r\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\\r\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\r\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\\r\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\\r\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\r\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\\r\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\\r\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\r\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\\r\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\\r\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\r\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\\r\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\\r\\n\\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\\r\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\\r\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\\r\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\\r\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\\r\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\\r\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\\r\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\r\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\r\\n\\tat java.lang.Thread.run(Thread.java:750)\\r\\nCaused by: java.net.SocketTimeoutException: Accept timed out\\r\\n\\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\\r\\n\\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\\r\\n\\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\\r\\n\\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\\r\\n\\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\\r\\n\\tat java.net.ServerSocket.accept(ServerSocket.java:513)\\r\\n\\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\\r\\n\\t... 31 more\\r\\n\\nDriver stacktrace:\\r\\n\\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\\r\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\\r\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\\r\\n\\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\\r\\n\\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\\r\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\\r\\n\\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\\r\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\\r\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\\r\\n\\tat scala.Option.foreach(Option.scala:407)\\r\\n\\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\\r\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\\r\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\\r\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\\r\\n\\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\\r\\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\\r\\n\\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\\r\\n\\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\\r\\n\\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\\r\\n\\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\\r\\n\\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\\r\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\\r\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\\r\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\r\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\\r\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\\r\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\r\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\\r\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\\r\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\r\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\\r\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\\r\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\r\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\\r\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\\r\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\r\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\\r\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\\r\\n\\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\\r\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\\r\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\\r\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\\r\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\\r\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\\r\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\\r\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\r\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\r\\n\\tat java.lang.Thread.run(Thread.java:750)\\r\\nCaused by: java.net.SocketTimeoutException: Accept timed out\\r\\n\\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\\r\\n\\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\\r\\n\\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\\r\\n\\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\\r\\n\\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\\r\\n\\tat java.net.ServerSocket.accept(ServerSocket.java:513)\\r\\n\\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\\r\\n\\t... 31 more\\r\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df11.select('msg').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "318b9473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict_list_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a604de49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'OPER_NO': '1F.EEK10', 'TOOL_NAME': 'EKT72_PM1'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict_list_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b4b4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1148d060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9952017358737066\n",
      "99.81897125020623\n"
     ]
    }
   ],
   "source": [
    "print(final_res_bs_add_columns.agg({\"weight\": \"sum\"}).collect()[0][0])\n",
    "print(final_res_bs_add_columns.agg({\"weight_percent\": \"sum\"}).collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c6124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efccc45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3ef8a5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\sql\\pandas\\group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "grpby_list = ['OPER_NO', 'TOOL_NAME']\n",
    "request_id = 'addcd'\n",
    "\n",
    "\n",
    "# 1. 数据预处理\n",
    "df_run = _pre_process(df1)\n",
    "\n",
    "\n",
    "# 2. 进行共性分析\n",
    "common_res = commonality_analysis(df_run, grpby_list)\n",
    "\n",
    "\n",
    "# 3. 挑选出数据：bad和good要同时大于3\n",
    "data_dict_list_bs = get_data_list_big_sample(common_res=common_res, grpby_list=grpby_list)\n",
    "df_run_bs = get_train_data_big_sample(df_run=df_run, data_dict_list=data_dict_list_bs)\n",
    "\n",
    "\n",
    "# 4. 获取所有bad wafer数量\n",
    "bad_wafer_num_big_sample = get_all_bad_wafer_num(df_run_bs)\n",
    "\n",
    "\n",
    "# 5. 对挑选出的大样本数据进行建模\n",
    "res = fit_rf_big_sample(df=df_run_bs, by=grpby_list)\n",
    "\n",
    "\n",
    "# 6. 将建模结果进行整合\n",
    "s_res = split_score_big_sample(df=res, by=['PRODG1', 'OPER_NO', 'TOOL_NAME'])\n",
    "f_res = split_calculate_features_big_sample(df=res, by=grpby_list)\n",
    "model_res_bs = get_finall_results_big_sample(s_res=s_res, f_res=f_res,bad_wafer_num=bad_wafer_num_big_sample)\n",
    "\n",
    "\n",
    "# 7. 增加特定的列\n",
    "final_res_bs = model_res_bs.withColumn('add', lit(0))\n",
    "final_res_bs_add_columns = add_certain_column(df=final_res_bs, by='add', request_id=request_id)\n",
    "\n",
    "\n",
    "# 8. final_res_bs_add_columns是最后的结果，要写回数据库\n",
    "# user =\"root\"\n",
    "# host = \"10.52.199.81\"\n",
    "# password = \"Nexchip%40123\"\n",
    "# db = \"etl\"\n",
    "# port = 9030\n",
    "# engine = create_engine(\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\".format(user = user,\n",
    "#                                                                                     password = password,\n",
    "#                                                                                     host = host,\n",
    "#                                                                                     port = port,\n",
    "#                                                                                     db = db))\n",
    "# doris_stream_load_from_df(final_res_bs_add_columns, engine, \"results\")\n",
    "\n",
    "\n",
    "# 9. 最终成功的话，就会输出下面这条\n",
    "df_kafka = pd.DataFrame({\"code\": 0, \"msg\": '运行成功', 'requestId': request_id}, index=[0])\n",
    "df1 = spark.createDataFrame(df_kafka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed664ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f854840",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'requestId'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3803\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3804\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'requestId'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16260\\3758562889.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mrequest_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"requestId\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mrequest_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"requestParam\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mrequest_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\\"\"\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# 避免存在单引号，因为json 引号只有双引号\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3802\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3803\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3804\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3805\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3806\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3804\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3805\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3806\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3807\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'requestId'"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "#######################################正式调用以上函数#######################################\n",
    "##########################################################################################\n",
    "\n",
    "# # 1. 解析json 为字典， df1为kafka 输入的结果数据\n",
    "# df2 = df1.toPandas() \n",
    "# # 默认只取第一行\n",
    "# if len(df2) > 0:\n",
    "#     df2 = df2.head(1)\n",
    "# request_id = df2[\"requestId\"].values[0]\n",
    "# request_params = df2[\"requestParam\"].values[0]\n",
    "# request_params = request_params.replace('\\'', \"\\\"\")    # 避免存在单引号，因为json 引号只有双引号\n",
    "# parse_dict = json.loads(request_params)\n",
    "\n",
    "# # 2. 从kafka 关键字映射都具体数据源中的字段,没有的可以删除\n",
    "# keyword_map_from_json_to_table: dict = {\n",
    "#     \"prodg1\": \"PRODG1\",\n",
    "#     \"waferId\": \"WAFER_ID\",\n",
    "#     \"dateRange\": \"START_TIME\",\n",
    "#     \"productId\": \"PRODUCT_ID\",\n",
    "#     \"operNo\": \"OPER_NO\",\n",
    "#     \"eqp\": \"EQP_NAME\",\n",
    "#     \"tool\": \"TOOL_NAME\",\n",
    "#     \"lot\": \"LOT_ID\",\n",
    "#     \"recipeName\": \"RECIPE_NAME\"\n",
    "# }\n",
    "# # 获取查询条件list\n",
    "# select_condition_list = parse_dict\n",
    "\n",
    "# #  3. 查询表名, 需要修改\n",
    "# table_name = \"etl.DWD_POC_CASE_FD_UVA_DATA_TEST\"\n",
    "\n",
    "# 4. 查询条件转sql,并读取数据\n",
    "\n",
    "# try:\n",
    "#     select_df_list = [  read_sql(trans_select_condition_to_sql_with_label(select_condition_dict, table_name)) for select_condition_dict in select_condition_list  ]\n",
    "#     # 多个进行union\n",
    "#     df1 = reduce(DataFrame.unionAll, select_df_list)\n",
    "# except Exception as e:\n",
    "#     flag = False\n",
    "#     df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": '系统内部异常! ',  'requestId': request_id}, index=[0])\n",
    "#     df_kafka_ = spark.createDataFrame(df_kafka)\n",
    "#     df1 = df_kafka_\n",
    "\n",
    "\n",
    "flag = True\n",
    "# 1. 数据预处理\n",
    "df_run = _pre_process(df1)\n",
    "if df_run.count() == 0:\n",
    "    df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": '该条件下无数据，请检查！',  'requestId': request_id}, index=[0])\n",
    "    df1 = spark.createDataFrame(df_kafka)\n",
    "else:\n",
    "    # 2. 进行共性分析\n",
    "    common_res = commonality_analysis(df_run)\n",
    "    common_res.show()\n",
    "\n",
    "    # 3. 挑选出数据：bad和good要同时大于3\n",
    "    data_dict_bs = get_data_dict_big_sample(common_res)\n",
    "    # print(data_dict_bs)\n",
    "    if len(data_dict_bs) == 0:\n",
    "        df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": '以PRODG1+OPER_NO+TOOL_NAME分组, BAD_WAFER和GOOD_WAFER数量都要大于3片, 请检查数据! ',  'requestId': request_id}, index=[0])\n",
    "        df_kafka_ = spark.createDataFrame(df_kafka)\n",
    "        df1 = df_kafka_\n",
    "    else:\n",
    "        df_run_bs = get_train_data_big_sample(df_run, data_dict_bs)\n",
    "        flag_al = True\n",
    "        try:\n",
    "            # 4. 对挑选出的数据进行建模\n",
    "            res = fit_rf_big_sample(df=df_run_bs, by=['PRODG1', 'OPER_NO', 'TOOL_NAME'])\n",
    "        except Exception as e:\n",
    "            flag_al = False\n",
    "            df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": '算法内部异常! ',  'requestId': request_id}, index=[0])\n",
    "            df_kafka_ = spark.createDataFrame(df_kafka)\n",
    "            df1 = df_kafka_\n",
    "\n",
    "        if flag_al == False:\n",
    "            df1 = df1\n",
    "        else:\n",
    "            # 5. 获取所有bad wafer数量\n",
    "            bad_wafer_num_big_sample = get_all_bad_wafer_num(df_run_bs)\n",
    "            if bad_wafer_num_big_sample < 3:\n",
    "                df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": 'BAD_WAFER数量需要大于3片, 请检查数据! ',  'requestId': request_id}, index=[0])\n",
    "                df_kafka_ = spark.createDataFrame(df_kafka)\n",
    "                df1 = df_kafka_\n",
    "            else:\n",
    "                flag_score = True\n",
    "                try:\n",
    "                    # 6. 将建模结果进行整合\n",
    "                    s_res = split_score_big_sample(df=res, by=['PRODG1', 'OPER_NO', 'TOOL_NAME'])\n",
    "                except Exception as e:\n",
    "                    flag_score = False\n",
    "                    df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": '算法运行评分结果较低，暂无输出! ',  'requestId': request_id}, index=[0])\n",
    "                    df_kafka_ = spark.createDataFrame(df_kafka)\n",
    "                    df1 = df_kafka_\n",
    "\n",
    "                if flag_score == False:\n",
    "                    df1 = df1\n",
    "                else:\n",
    "                    f_res = split_calculate_features_big_sample(df=res, by=['PRODG1', 'OPER_NO', 'TOOL_NAME'])\n",
    "                    # s_res.show()\n",
    "                    model_res_bs = get_finall_results_big_sample(s_res=s_res, f_res=f_res, bad_wafer_num=bad_wafer_num_big_sample)\n",
    "\n",
    "                    # 7. 增加特定的列\n",
    "                    final_res_bs = model_res_bs.withColumn('add', lit(0))\n",
    "                    ddd = add_certain_column(df=final_res_bs, by='add', request_id=request_id)\n",
    "                    ddd = ddd.drop('add')\n",
    "                    ddd = ddd.toPandas()\n",
    "\n",
    "                    # 8. ddd 是最后的结果，要写回数据库\n",
    "                    user =\"root\"\n",
    "                    host = \"10.52.199.81\"\n",
    "                    password = \"Nexchip%40123\"\n",
    "                    db = \"etl\"\n",
    "                    port = 9030\n",
    "                    engine = create_engine(\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\".format(user = user,\n",
    "                                                                                                        password = password,\n",
    "                                                                                                        host = host,\n",
    "                                                                                                        port = port,\n",
    "                                                                                                        db = db))\n",
    "                    doris_stream_load_from_df(ddd, engine, \"results\")\n",
    "\n",
    "\n",
    "\n",
    "                    # 9. 最终成功的话，就会输出下面这条\n",
    "                    df_kafka = pd.DataFrame({\"code\": 0, \"msg\": '运行成功', 'requestId': request_id}, index=[0])\n",
    "                    df_kafka_ = spark.createDataFrame(df_kafka)\n",
    "                    df1 = df_kafka_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd28e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbbcca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8886e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf221c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac5349b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fae7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "################################将最后的结果写回数据库####################################\n",
    "#####################################################################################\n",
    "def doris_stream_load_from_df(df, engine, table, is_json=True, chunksize=100000, partitions=None):\n",
    "    engine_url = engine.url\n",
    "    url = 'http://%s:18030/api/%s/%s/_stream_load' % (engine_url.host, engine_url.database, table)\n",
    "\n",
    "    format_str = 'csv' if not is_json else 'json'\n",
    "    headers = {\n",
    "        'Content-Type': 'text/plain; charset=UTF-8',\n",
    "        'format': format_str,\n",
    "        'Expect': '100-continue'\n",
    "    }\n",
    "    if is_json:\n",
    "        headers['strip_outer_array'] = 'true'\n",
    "        headers['read_json_by_line'] = 'true'\n",
    "    else:\n",
    "        headers['column_separator'] = '@'\n",
    "    \n",
    "    if partitions:\n",
    "        headers['partitions'] = partitions\n",
    "    \n",
    "    auth = requests.auth.HTTPBasicAuth(engine_url.username, engine_url.password)\n",
    "    session = requests.sessions.Session()\n",
    "    session.should_strip_auth = lambda old_url, new_url: False\n",
    "    \n",
    "    l = len(df)\n",
    "    if l > 0:\n",
    "        if chunksize and chunksize < l:\n",
    "            batches = l // chunksize\n",
    "            if l % chunksize > 0:\n",
    "                batches += 1\n",
    "            for i in range(batches):\n",
    "                si = i * chunksize\n",
    "                ei = min(si + chunksize, l)\n",
    "                sub = df[si:ei]\n",
    "                do_doris_stream_load_from_df(sub, session, url, headers, auth, is_json)\n",
    "        else:\n",
    "            do_doris_stream_load_from_df(df, session, url, headers, auth, is_json)\n",
    "\n",
    "\n",
    "def do_doris_stream_load_from_df(df, session, url, headers, auth, is_json=False):\n",
    "    data = df.to_csv(header=False, index=False, sep='@') if not is_json else df.to_json(orient='records', date_format='iso')\n",
    "    #print(data)\n",
    "    \n",
    "    resp = session.request(\n",
    "        'PUT',\n",
    "        url = url,\n",
    "        data=data.encode('utf-8'),\n",
    "        headers=headers,\n",
    "        auth=auth\n",
    "    )\n",
    "    print(resp.reason, resp.text)\n",
    "    check_stream_load_response(resp.text)\n",
    "\n",
    "\n",
    "def check_stream_load_response(resp_text):\n",
    "    resp = json.loads(resp_text)\n",
    "    if resp['Status'] not in [\"Success\", \"Publish Timeout\"]:\n",
    "        raise Exception(resp['Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7c88ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe2cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f76af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a889319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430676a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf8b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
