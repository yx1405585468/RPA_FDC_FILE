{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca05c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pyspark.sql.dataframe\n",
    "from pca import pca\n",
    "from pyspark.sql.functions import max, countDistinct, when, rank, lit, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.window import Window\n",
    "from typing import List, Dict, Union, Tuple, Optional, Any\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import ClusterCentroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a870bb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[scatterd] >WARNING> 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/python-3.9.13/bin/python3'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"pandas_udf\") \\\n",
    "    .config('spark.sql.session.timeZone', 'Asia/Shanghai') \\\n",
    "    .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.driver.cores', '10') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .config('spark.executor.cores', '10') \\\n",
    "    .config('spark.cores.max', '10') \\\n",
    "    .config('spark.driver.host', '192.168.22.28') \\\n",
    "    .master(\"spark://192.168.12.47:7077,192.168.12.48:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "41c13ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine\n",
    "# engine = create_engine(\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\".format(user=\"root\", password=\"123456\",\n",
    "#                                                                                          host=\"192.168.13.17\",\n",
    "#                                                                                          port=9030,\n",
    "#                                                                                          db=\"rca\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a9e07329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"SELECT * FROM rca.DWD_FD_UVA_DATA WHERE START_TIME >= '2021-12-05 14:41:26' AND START_TIME <= '2024-03-05 14:41:26' AND PRODUCT_ID in ('AMKNS301N.0A01','AMKNS301N.0B01','AFPNR901N.0B0J','AFPNR901N.0B0L')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c15055e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operno = [\"2V.PQW10\",\"2U.PQA10\",\"1V.PQA10\",\"1U.CDG20\",\"1U.CDG10\",\"3U.PQA10\",\"6V.CDG10\",\"7U.PQA10\",\"7U.PQX10\",\"TM.PQX10\",\"XX.PQW01\",\"XX.PQX02\",\"1U.PQX10\",\"1V.PQX10\",\"1V.PQX20\",\"2U.PQX10\",\"6V.PQA10\",\"3U.PQX10\",\"7U.PQW10\",\"PV.PQX10\",\"TV.PQA10\",\"1C.PQA10\",\"1C.PQX10\",\"7U.CDG10\",\"PV.PQW10\",\"1C.CDG10\",\"1U.PQA10\",\"2V.PQA10\",\"3U.CDG10\",\"3U.CDG20\",\"3U.PQW10\",\"6V.CDG20\",\"6V.EQA10\",\"6V.PQX10\",\"7U.EQA10\",\"7U.EQA20\",\"7U.PQA20\",\"PV.CDG10\",\"PV.EQA10\",\"PV.PQA10\",\"TV.CDG10\",\"TV.EQA20\",\"TV.PQX10\",\"XX.EQW01\",\"2V.PQX10\",\"2V.PQX20\",\"6V.PQW10\",\"7U.ECU10\",\"7U.EQW10\",\"TM.EQA10\",\"1F.FQE10\",\"TV.PQW10\",\"TM.PQW10\",\"TM.PQA10\",\"XX.PQX01\",\"1F.EEK10\",\"2F.CDS10\"]\n",
    "# operno = tuple([string.replace('\"', \"'\") for string in operno])\n",
    "\n",
    "# prodg1 = [\"TESTPRODG1\",\"L11PN03B\",\"L11HH01A\",\"L11HD11A\",\"L11PN06B\",\"L11MW20A\",\"L15EG04A\",\"L11CB14A\",\"L15DV07A\",\"C90WA20A\",\"L11CG02A\",\"L11HH03A\",\"C90WA15A\",\"L15KD03A\",\"C90WA02P\",\"L11PN29B\",\"L15DP03A\",\"L11CR02A\",\"L2800Z3N\",\"L2800Z1N\",\"L2800Z2N\",\"C90WA12A\",\"L11EG07A\",\"L11TG07A\",\"L15RB03A\",\"L11CS04A\",\"C90WA30A\",\"L11DM10A\",\"L11CD02A\",\"L11PP03A\",\"L11TG05A\"]\n",
    "# prodg1 = tuple([string.replace('\"', \"'\") for string in prodg1])\n",
    "\n",
    "# productid = [\"AFPNM301N.0A01\",\"AFPNR901N.0B01\",\"AFKNNW01N.0C01\",\"AMKNS301N.0A01\",\"AFGN2T01N.0G01\",\"AEMNE801N.0B01\",\"AEMNV901N.0A01\",\"AMKNWX01N.0B01\",\"AMKNZC01N.0A01\",\"AMKNSE01N.0B01\",\"AMKNS301N.0B01\",\"AFGN4201N.0B01\",\"AMKNTJ01N.0A01\",\"AMKNZD01N.0A01\",\"AFKN8401N.0D01\"]\n",
    "# productid = tuple([string.replace('\"', \"'\") for string in productid])\n",
    "\n",
    "# eqp = [\"DSA02\",\"EKT72\",\"TEST01\"]\n",
    "# eqp = tuple([string.replace('\"', \"'\") for string in eqp])\n",
    "\n",
    "# tool = [\"TEST01_A\",\"EKT72_PM1\",\"DSA02_B\"]\n",
    "# tool = tuple([string.replace('\"', \"'\") for string in tool])\n",
    "\n",
    "# recipe = [\"\",\"NEW-DRM/P1/110NM/PFKN0G0D1F1A\",\"S180DE5C680-2X\",\"NEW-DRM/P1/110NM/PFKN9F0D1F1A\",\"NEW-DRM/P1/150NM/PFGN420D1F1A\",\"NEW-DRM/P1/110NM/PFKN560D1F1A\",\"NEW-DRM/P1/110NM/PFKN690D1F1A\",\"NEW-DRM/P1/110NM/PFKN6N0D1F1A\",\"NEW-DRM/P1/110NM/PFKN050D1F1A\",\"S8900DX0580\",\"NEW-DRM/P1/150NM/PFGN2T0D1F1A\",\"NEW-DRM/P1/110NM/PFKN750D1F1A\"]\n",
    "# recipe = tuple([string.replace('\"', \"'\") for string in recipe])\n",
    "\n",
    "# query = f\"SELECT * FROM rca.DWD_FD_UVA_DATA WHERE START_TIME >= '2021-12-05 16:14:52'\\\n",
    "# AND START_TIME <= '2024-03-05 16:14:52'\\\n",
    "# AND OPER_NO in {operno}\\\n",
    "# AND PRODG1 in {prodg1}\\\n",
    "# AND PRODUCT_ID in {productid}\\\n",
    "# AND EQP_NAME in {eqp}\\\n",
    "# AND TOOL_NAME in {tool}\\\n",
    "# AND RECIPE_NAME in {recipe}\"\n",
    "\n",
    "# data_pandas = pd.read_sql(sql=query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0fbd840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_wafer = \"SELECT * FROM rca.conf_wafer WHERE upload_id = '767d668b11664c47a04903b246994437'\"\n",
    "# data_wafer = pd.read_sql(sql=query_wafer, con=engine)\n",
    "# data_wafer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "64cfa5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_wafer['label'] = data_wafer['label'].map({'BAD':1, 'GOOD':0})\n",
    "# data_wafer.rename({'name':'WAFER_ID'}, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1f58525d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_wafer[data_wafer['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3e9e6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据和label连接\n",
    "# data_pandas_label = pd.merge(data_pandas, data_wafer[['WAFER_ID', 'label']])\n",
    "# data_pandas_label[data_pandas_label['label']==1]['WAFER_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1970567",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.read_csv(\"C:/Users/yang.wenjun/Desktop/dd1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a75b97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAFER_ID</th>\n",
       "      <th>TOOL_ID</th>\n",
       "      <th>RUN_ID</th>\n",
       "      <th>EQP_NAME</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>PRODG1</th>\n",
       "      <th>TOOL_NAME</th>\n",
       "      <th>LOT_ID</th>\n",
       "      <th>RECIPE_NAME</th>\n",
       "      <th>OPER_NO</th>\n",
       "      <th>parametric_name</th>\n",
       "      <th>CASE_INFO</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>STATISTIC_RESULT</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>14847</td>\n",
       "      <td>125676</td>\n",
       "      <td>WKL5A</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>WKL5A_L1</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>KFPM10A.PrD</td>\n",
       "      <td>1V.WWK20</td>\n",
       "      <td>CHUCK_ROTATION_SPEED#WINDOW_1#MEAN</td>\n",
       "      <td>2023/12/25</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>1483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>14847</td>\n",
       "      <td>125676</td>\n",
       "      <td>WKL5A</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>WKL5A_L1</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>KFPM10A.PrD</td>\n",
       "      <td>1V.WWK20</td>\n",
       "      <td>DIW_FLOW#WINDOW_1#MEAN</td>\n",
       "      <td>2023/12/25</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>1511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>14847</td>\n",
       "      <td>125676</td>\n",
       "      <td>WKL5A</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>WKL5A_L1</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>KFPM10A.PrD</td>\n",
       "      <td>1V.WWK20</td>\n",
       "      <td>FPM_CIRCULATION_FLOW#WINDOW_1#MEAN</td>\n",
       "      <td>2023/12/25</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>14847</td>\n",
       "      <td>125676</td>\n",
       "      <td>WKL5A</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>WKL5A_L1</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>KFPM10A.PrD</td>\n",
       "      <td>1V.WWK20</td>\n",
       "      <td>PSL_PM1_CHUCKPINWAFERCOUNT#WINDOW_1#MEAN</td>\n",
       "      <td>2023/12/25</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>5906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>14847</td>\n",
       "      <td>125676</td>\n",
       "      <td>WKL5A</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>WKL5A_L1</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>KFPM10A.PrD</td>\n",
       "      <td>1V.WWK20</td>\n",
       "      <td>FPM_TEMP_BEFORE_HEATER#NEW#MEAN</td>\n",
       "      <td>2023/12/25</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25271</th>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>7046</td>\n",
       "      <td>170461</td>\n",
       "      <td>EML01</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>EML01_PM3</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>RJPL28M2U1T01R02</td>\n",
       "      <td>2U.EEM10</td>\n",
       "      <td>BASI_RF_VOLTAGE#AUTO_CURRENT_STEP_9#MEAN</td>\n",
       "      <td>2023/12/8</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25272</th>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>7046</td>\n",
       "      <td>170461</td>\n",
       "      <td>EML01</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>EML01_PM3</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>RJPL28M2U1T01R02</td>\n",
       "      <td>2U.EEM10</td>\n",
       "      <td>BIAS_MATCH_SHUNT_POSITION#AUTO_CURRENT_STEP_9#...</td>\n",
       "      <td>2023/12/8</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25273</th>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>7046</td>\n",
       "      <td>170461</td>\n",
       "      <td>EML01</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>EML01_PM3</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>RJPL28M2U1T01R02</td>\n",
       "      <td>2U.EEM10</td>\n",
       "      <td>ESC_TEMPERATURE_INNTER#AUTO_CURRENT_STEP_9#MEAN</td>\n",
       "      <td>2023/12/8</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25274</th>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>7046</td>\n",
       "      <td>170462</td>\n",
       "      <td>EML01</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>EML01_PM3</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>Tier2 WAC_Fluorine</td>\n",
       "      <td>2U.EEM10</td>\n",
       "      <td>GAS8_CL2_500#WINDOW_1#AREA</td>\n",
       "      <td>2023/12/8</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>3334</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25275</th>\n",
       "      <td>NBX293-07</td>\n",
       "      <td>13545</td>\n",
       "      <td>38195</td>\n",
       "      <td>MTN52</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>MTN52_1</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>/Ch1/RJPVD-DEGAS350C120S;1</td>\n",
       "      <td>2U.CMT20</td>\n",
       "      <td>IG_PRESSURE#BASE_PRESSURE#MIN</td>\n",
       "      <td>2023/12/8</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25276 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        WAFER_ID  TOOL_ID  RUN_ID EQP_NAME      PRODUCT_ID    PRODG1  \\\n",
       "0      NBX293-06    14847  125676    WKL5A  AFPNR901N.0B0L  L2800Z2N   \n",
       "1      NBX293-06    14847  125676    WKL5A  AFPNR901N.0B0L  L2800Z2N   \n",
       "2      NBX293-06    14847  125676    WKL5A  AFPNR901N.0B0L  L2800Z2N   \n",
       "3      NBX293-06    14847  125676    WKL5A  AFPNR901N.0B0L  L2800Z2N   \n",
       "4      NBX293-06    14847  125676    WKL5A  AFPNR901N.0B0L  L2800Z2N   \n",
       "...          ...      ...     ...      ...             ...       ...   \n",
       "25271  NBX293-06     7046  170461    EML01  AFPNR901N.0B0L  L2800Z2N   \n",
       "25272  NBX293-06     7046  170461    EML01  AFPNR901N.0B0L  L2800Z2N   \n",
       "25273  NBX293-06     7046  170461    EML01  AFPNR901N.0B0L  L2800Z2N   \n",
       "25274  NBX293-06     7046  170462    EML01  AFPNR901N.0B0L  L2800Z2N   \n",
       "25275  NBX293-07    13545   38195    MTN52  AFPNR901N.0B0L  L2800Z2N   \n",
       "\n",
       "       TOOL_NAME      LOT_ID                 RECIPE_NAME   OPER_NO  \\\n",
       "0       WKL5A_L1  NBX293.200                 KFPM10A.PrD  1V.WWK20   \n",
       "1       WKL5A_L1  NBX293.200                 KFPM10A.PrD  1V.WWK20   \n",
       "2       WKL5A_L1  NBX293.200                 KFPM10A.PrD  1V.WWK20   \n",
       "3       WKL5A_L1  NBX293.200                 KFPM10A.PrD  1V.WWK20   \n",
       "4       WKL5A_L1  NBX293.200                 KFPM10A.PrD  1V.WWK20   \n",
       "...          ...         ...                         ...       ...   \n",
       "25271  EML01_PM3  NBX293.200            RJPL28M2U1T01R02  2U.EEM10   \n",
       "25272  EML01_PM3  NBX293.200            RJPL28M2U1T01R02  2U.EEM10   \n",
       "25273  EML01_PM3  NBX293.200            RJPL28M2U1T01R02  2U.EEM10   \n",
       "25274  EML01_PM3  NBX293.200          Tier2 WAC_Fluorine  2U.EEM10   \n",
       "25275    MTN52_1  NBX293.200  /Ch1/RJPVD-DEGAS350C120S;1  2U.CMT20   \n",
       "\n",
       "                                         parametric_name   CASE_INFO  STATUS  \\\n",
       "0                     CHUCK_ROTATION_SPEED#WINDOW_1#MEAN  2023/12/25  NORMAL   \n",
       "1                                 DIW_FLOW#WINDOW_1#MEAN  2023/12/25  NORMAL   \n",
       "2                     FPM_CIRCULATION_FLOW#WINDOW_1#MEAN  2023/12/25  NORMAL   \n",
       "3               PSL_PM1_CHUCKPINWAFERCOUNT#WINDOW_1#MEAN  2023/12/25  NORMAL   \n",
       "4                        FPM_TEMP_BEFORE_HEATER#NEW#MEAN  2023/12/25  NORMAL   \n",
       "...                                                  ...         ...     ...   \n",
       "25271           BASI_RF_VOLTAGE#AUTO_CURRENT_STEP_9#MEAN   2023/12/8  NORMAL   \n",
       "25272  BIAS_MATCH_SHUNT_POSITION#AUTO_CURRENT_STEP_9#...   2023/12/8  NORMAL   \n",
       "25273    ESC_TEMPERATURE_INNTER#AUTO_CURRENT_STEP_9#MEAN   2023/12/8  NORMAL   \n",
       "25274                         GAS8_CL2_500#WINDOW_1#AREA   2023/12/8  NORMAL   \n",
       "25275                      IG_PRESSURE#BASE_PRESSURE#MIN   2023/12/8  NORMAL   \n",
       "\n",
       "       STATISTIC_RESULT  label  \n",
       "0                  1483      0  \n",
       "1                  1511      0  \n",
       "2                    21      0  \n",
       "3                  5906      0  \n",
       "4                    23      0  \n",
       "...                 ...    ...  \n",
       "25271                 2      0  \n",
       "25272               686      0  \n",
       "25273                48      0  \n",
       "25274              3334      0  \n",
       "25275                 0      0  \n",
       "\n",
       "[25276 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa5203a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56173a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a95d68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3598b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e60e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "700876b8",
   "metadata": {},
   "source": [
    "#### uva算法代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46808dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25276\n"
     ]
    }
   ],
   "source": [
    "df1_ = ps.from_pandas(pandas_df).to_spark()\n",
    "print(df1_.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81202f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b63350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessForUvaData:\n",
    "    @staticmethod\n",
    "    def integrate_columns(df: pyspark.sql.dataframe,\n",
    "                          merge_operno_list: List[Dict[str, List[str]]],\n",
    "                          merge_prodg1_list: List[Dict[str, List[str]]],\n",
    "                          merge_product_list: List[Dict[str, List[str]]],\n",
    "                          merge_eqp_list: List[Dict[str, List[str]]],\n",
    "                          merge_chamber_list: List[Dict[str, List[str]]], ) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Integrate columns in the DataFrame based on the provided list.\n",
    "\n",
    "        :param df: The input DataFrame.\n",
    "        :param merge_operno_list: A list of dictionaries where each dictionary contains values to be merged.\n",
    "               Example: [{'2F.CDS10_XX.TDS01': ['2F.CDS10', 'XX.TDS01']},\n",
    "                         {'2F.CDS20_XX.CDS20': ['2F.CDS20', 'XX.CDS20']}]\n",
    "        :param merge_prodg1_list: A list of dictionaries for merging 'PRODG1' column in a similar fashion.\n",
    "        :param merge_product_list: A list of dictionaries for merging 'PRODUCT_ID' column in a similar fashion.\n",
    "        :param merge_eqp_list: A list of dictionaries for merging 'EQP_NAME' column in a similar fashion.\n",
    "        :param merge_chamber_list: A list of dictionaries for merging 'TOOL_NAME' column in a similar fashion.\n",
    "\n",
    "        :return: DataFrame with 'OPER_NO' and other specified columns integrated according to the merge rules.\n",
    "        \"\"\"\n",
    "        if merge_operno_list is not None and len(merge_operno_list) > 0:\n",
    "            # Extract values from each dictionary in merge_operno_list and create a list\n",
    "            values_to_replace = [list(rule.values())[0] for rule in merge_operno_list]\n",
    "            # Concatenate values from each dictionary\n",
    "            merged_values = [\"_\".join(list(rule.values())[0]) for rule in merge_operno_list]\n",
    "\n",
    "            # Replace values in 'OPER_NO' column based on the rules defined in merge_operno_list\n",
    "            for values, replacement_value in zip(values_to_replace, merged_values):\n",
    "                df = df.withColumn(\"OPER_NO\",\n",
    "                                   when(col(\"OPER_NO\").isin(values), replacement_value).otherwise(col(\"OPER_NO\")))\n",
    "\n",
    "        if merge_prodg1_list is not None and len(merge_prodg1_list) > 0:\n",
    "            values_to_replace = [list(rule.values())[0] for rule in merge_prodg1_list]\n",
    "            merged_values = [\"_\".join(list(rule.values())[0]) for rule in merge_prodg1_list]\n",
    "\n",
    "            for values, replacement_value in zip(values_to_replace, merged_values):\n",
    "                df = df.withColumn(\"PRODG1\",\n",
    "                                   when(col(\"PRODG1\").isin(values), replacement_value).otherwise(col(\"PRODG1\")))\n",
    "\n",
    "        if merge_product_list is not None and len(merge_product_list) > 0:\n",
    "            values_to_replace = [list(rule.values())[0] for rule in merge_product_list]\n",
    "            merged_values = [\"_\".join(list(rule.values())[0]) for rule in merge_product_list]\n",
    "\n",
    "            for values, replacement_value in zip(values_to_replace, merged_values):\n",
    "                df = df.withColumn(\"PRODUCT_ID\",\n",
    "                                   when(col(\"PRODUCT_ID\").isin(values), replacement_value).otherwise(col(\"PRODUCT_ID\")))\n",
    "\n",
    "        if merge_eqp_list is not None and len(merge_eqp_list) > 0:\n",
    "            values_to_replace = [list(rule.values())[0] for rule in merge_eqp_list]\n",
    "            merged_values = [\"_\".join(list(rule.values())[0]) for rule in merge_eqp_list]\n",
    "\n",
    "            for values, replacement_value in zip(values_to_replace, merged_values):\n",
    "                df = df.withColumn(\"EQP_NAME\",\n",
    "                                   when(col(\"EQP_NAME\").isin(values), replacement_value).otherwise(col(\"EQP_NAME\")))\n",
    "\n",
    "        if merge_chamber_list is not None and len(merge_chamber_list) > 0:\n",
    "            values_to_replace = [list(rule.values())[0] for rule in merge_chamber_list]\n",
    "            merged_values = [\"_\".join(list(rule.values())[0]) for rule in merge_chamber_list]\n",
    "\n",
    "            for values, replacement_value in zip(values_to_replace, merged_values):\n",
    "                df = df.withColumn(\"TOOL_NAME\",\n",
    "                                   when(col(\"TOOL_NAME\").isin(values), replacement_value).otherwise(col(\"TOOL_NAME\")))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def pre_process(df: pyspark.sql.dataframe) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Preprocess the data extracted from the database for a specific CASE.\n",
    "        :param df: Data for a specific CASE retrieved from the database.\n",
    "        :return: Preprocessed data with relevant columns and filters applied.\n",
    "        \"\"\"\n",
    "        # Select only the columns that will be used\n",
    "        df = df.select('WAFER_ID', 'TOOL_ID', 'RUN_ID', 'EQP_NAME', 'PRODUCT_ID', 'PRODG1', 'TOOL_NAME',\n",
    "                       'OPER_NO', 'parametric_name', 'STATISTIC_RESULT', 'label')\n",
    "        # Remove rows with missing values in 'STATISTIC_RESULT' column\n",
    "        df = df.filter(col('STATISTIC_RESULT').isNotNull())\n",
    "        # Drop duplicates based on all columns\n",
    "        df1 = df.dropDuplicates()\n",
    "        # Select the rows with the latest 'RUN_ID' for each combination of 'WAFER_ID', 'OPER_NO', 'TOOL_ID'\n",
    "        df2 = df1.groupBy('WAFER_ID', 'OPER_NO', 'TOOL_ID').agg(max('RUN_ID').alias('RUN_ID'))\n",
    "        df_run = df1.join(df2.dropDuplicates(subset=['WAFER_ID', 'OPER_NO', 'TOOL_ID', 'RUN_ID']),\n",
    "                          on=['WAFER_ID', 'OPER_NO', 'TOOL_ID', 'RUN_ID'], how='inner')\n",
    "        return df_run\n",
    "\n",
    "    @staticmethod\n",
    "    def commonality_analysis(df_run: pyspark.sql.dataframe, grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Perform commonality analysis on preprocessed data.\n",
    "        :param df_run: Preprocessed data after data preprocessing.\n",
    "        :param grpby_list: List of columns ['PRODG1', 'EQP_NAME', 'OPER_NO', 'PRODUCT_ID', 'TOOL_NAME'] for grouping.\n",
    "                Example: grpby_list = ['PRODG1', 'TOOL_NAME', 'OPER_NO'], grpby_list = ['PRODUCT_ID', 'OPER_NO']\n",
    "        :return: Results of commonality analysis, showing the top ten combinations with the highest number of bad wafers.\n",
    "        \"\"\"\n",
    "        grps = (df_run.groupBy(grpby_list)\n",
    "                .agg(countDistinct('WAFER_ID').alias('wafer_count'),\n",
    "                     countDistinct('WAFER_ID', when(df_run['label'] == 0, 1)).alias('good_num'),\n",
    "                     countDistinct('WAFER_ID', when(df_run['label'] == 1, 1)).alias('bad_num'))\n",
    "                .orderBy('bad_num', ascending=False))\n",
    "\n",
    "        # Handle the case of a single OPER_NO or single TOOL_NAME\n",
    "        if grps.count() == 1:\n",
    "            return grps\n",
    "        else:\n",
    "            # Filter out groups with no bad wafers\n",
    "            grps = grps.filter(grps['bad_num'] > 0)\n",
    "            # Rank the groups based on the number of bad wafers\n",
    "            window_sep = Window().orderBy(col(\"bad_num\").desc())\n",
    "            ranked_df = grps.withColumn(\"rank\", rank().over(window_sep))\n",
    "            # Select the top ten groups and remove the 'rank' column\n",
    "            grpss = ranked_df.filter(col(\"rank\") <= 10).drop(\"rank\")\n",
    "            return grpss\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_list(common_res: pyspark.sql.dataframe,\n",
    "                      grpby_list: List[str],\n",
    "                      big_or_small: str = 'big') -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Get a list of dictionaries for corresponding groups based on commonality analysis.\n",
    "\n",
    "        :param common_res: Result of commonality analysis.\n",
    "        :param grpby_list:  List of columns ['PRODG1', 'EQP_NAME', 'OPER_NO', 'PRODUCT_ID', 'TOOL_NAME'] for grouping.\n",
    "        :param big_or_small: 'big' or 'small'.\n",
    "        :return: List of dictionaries for corresponding groups.\n",
    "                Example: [{'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFKN2J01N.0U01'},\n",
    "                          {'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFKN4X01N.0B01'},\n",
    "                          {'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFGN1501N.0C02'}]\n",
    "        \"\"\"\n",
    "        assert big_or_small in ['big', 'small'], \"Choose only 'big' or 'small'. Please check the spelling.\"\n",
    "\n",
    "        # Filter groups based on big or small sample conditions\n",
    "        if big_or_small == 'big':\n",
    "            good_bad_grps = common_res.filter(\"good_num >= 3 AND bad_num >= 3\")\n",
    "        else:\n",
    "            good_bad_grps = common_res.filter(\"bad_num >= 1 AND wafer_count >= 2\")\n",
    "\n",
    "        # Order the results and limit to the top 10 groups\n",
    "        good_bad_grps = good_bad_grps.orderBy(col(\"bad_num\").desc(), col(\"wafer_count\").desc(),\n",
    "                                              col(\"good_num\").desc()).limit(10)\n",
    "\n",
    "        # Collect the data and convert it into a list of dictionaries\n",
    "        data_list = good_bad_grps[grpby_list].collect()\n",
    "        data_dict_list = [row.asDict() for row in data_list]\n",
    "        return data_dict_list\n",
    "\n",
    "    @staticmethod\n",
    "    def get_train_data(df_run: pyspark.sql.dataframe, data_dict_list: List[Dict[str, str]]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Get the actual combination data for modeling from the original data.\n",
    "\n",
    "        :param df_run: Preprocessed data after data preprocessing.\n",
    "        :param data_dict_list: List of dictionaries with filtering conditions.\n",
    "               Example: [{'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFKN2J01N.0U01'},\n",
    "                          {'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFKN4X01N.0B01'},\n",
    "                          {'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFGN1501N.0C02'}]\n",
    "        :return: Filtered data for modeling.\n",
    "        \"\"\"\n",
    "        # Get the filtering conditions for the first data dictionary\n",
    "        first_data_dict = data_dict_list[0]\n",
    "        conditions = \" AND \".join([\"{} == '{}'\".format(col_, first_data_dict[col_]) for col_ in first_data_dict])\n",
    "        # Filter the data for the first condition\n",
    "        df_s = df_run.filter(conditions)\n",
    "\n",
    "        # Loop through the remaining data dictionaries and filter the data accordingly\n",
    "        for i in range(1, len(data_dict_list)):\n",
    "            data_dict = data_dict_list[i]\n",
    "            conditions = \" AND \".join([\"{} == '{}'\".format(col_, data_dict[col_]) for col_ in data_dict])\n",
    "            df_m = df_run.filter(conditions)\n",
    "            df_s = df_s.union(df_m)\n",
    "        return df_s\n",
    "\n",
    "    @staticmethod\n",
    "    def get_all_bad_wafer_num(df: pyspark.sql.dataframe) -> int:\n",
    "        \"\"\"\n",
    "        Get the number of distinct bad WAFER in the DataFrame.\n",
    "        \"\"\"\n",
    "        return df.filter(\"label == 1\").select('WAFER_ID').distinct().count()\n",
    "\n",
    "\n",
    "class FitModelForUvaData:\n",
    "    @staticmethod\n",
    "    def get_pivot_table(df: pd.DataFrame, grpby_list: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Pivot the DataFrame based on specified grouping columns.\n",
    "\n",
    "        Parameters:\n",
    "        - df: Data for modeling.\n",
    "        - grpby_list: List of grouping columns.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Result of pivoting the table.\n",
    "        \"\"\"\n",
    "        index_cols = ['WAFER_ID', 'label']\n",
    "        columns_cols = grpby_list + ['parametric_name']\n",
    "        df_pivot = df.dropna(axis=0).pivot_table(index=index_cols,\n",
    "                                                 columns=columns_cols,\n",
    "                                                 values=['STATISTIC_RESULT'])\n",
    "        df_pivot.columns = df_pivot.columns.map('#'.join)\n",
    "        df_pivot = df_pivot.fillna(df_pivot.mean()).reset_index(drop=False)\n",
    "        # Remove completely identical columns\n",
    "        for column in df_pivot.columns.difference(index_cols):\n",
    "            if df_pivot[column].nunique() == 1:\n",
    "                df_pivot = df_pivot.drop(column, axis=1)\n",
    "        return df_pivot\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_rf_big_sample(df: pyspark.sql.dataframe, grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Fit a RandomForest model on the train data. It is for large sample method(good_wafer_num >= 3 AND bad_wafer_num >= 3)\n",
    "\n",
    "        Parameters:\n",
    "        - df: Data for modeling.\n",
    "        - grpby_list: List of grouping columns.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Combined dataframe of roc_auc_score result and feature importance after RandomForest modeling.\n",
    "        \"\"\"\n",
    "        # Dynamically build schema according to the grpby_list\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"bad_wafer\", IntegerType(), True),\n",
    "                              StructField(\"roc_auc_score\", FloatType(), True),\n",
    "                              StructField(\"features\", StringType(), True),\n",
    "                              StructField(\"importance\", FloatType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_model_result(df_run: pd.DataFrame) -> pd.DataFrame:\n",
    "            # Pivot the table\n",
    "            df_pivot = FitModelForUvaData.get_pivot_table(df=df_run, grpby_list=grpby_list)\n",
    "\n",
    "            # Define independent and dependent variables\n",
    "            x_train = df_pivot[df_pivot.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "            y_train = df_pivot[['label']]\n",
    "            if min(x_train.shape) <= 0:\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            z_ratio = y_train.value_counts(normalize=True)\n",
    "            good_ratio = z_ratio[0]\n",
    "            bad_ratio = z_ratio[1]\n",
    "            if abs(good_ratio - bad_ratio) > 0.7:\n",
    "                undersampler = ClusterCentroids(random_state=101)\n",
    "                x_train, y_train = undersampler.fit_resample(x_train, y_train)\n",
    "\n",
    "            # Grid search\n",
    "            pipe = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value=-999)),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('model', RandomForestClassifier(random_state=2024))])\n",
    "            param_grid = {'model__n_estimators': [*range(50, 100, 10)],\n",
    "                          'model__max_depth': [*range(10, 50, 10)]}\n",
    "            grid = GridSearchCV(estimator=pipe, scoring='roc_auc', param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "            grid.fit(x_train.values, y_train.values.ravel())\n",
    "            roc_auc_score_ = grid.best_score_\n",
    "\n",
    "            # Feature importance and result summary\n",
    "            small_importance_res = pd.DataFrame({'features': x_train.columns,\n",
    "                                                 'importance': grid.best_estimator_.steps[2][1].feature_importances_})\n",
    "\n",
    "            sample_res_dict = {'bad_wafer': sum(df_pivot['label']),\n",
    "                               'roc_auc_score': roc_auc_score_}\n",
    "            sample_res_dict.update({col_: df_run[col_].unique() for col_ in grpby_list})\n",
    "            small_sample_res = pd.DataFrame(sample_res_dict)\n",
    "            return pd.concat([small_importance_res, small_sample_res])\n",
    "\n",
    "        return df.groupby(grpby_list).apply(get_model_result)\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_pca_small_sample(df: pyspark.sql.dataframe, grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Fit a PCA model on the train data. It is for small sample method (bad_wafer_num >= 1 AND wafer_count >= 2).\n",
    "\n",
    "        Parameters:\n",
    "        - df: Data for modeling.\n",
    "        - grpby_list: List of grouping columns.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Combined dataframe of every feature and its importance in each combination of grpby_list after PCA modeling.\n",
    "        \"\"\"\n",
    "        # Dynamically build schema according to the grpby_list\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"features\", StringType(), True),\n",
    "                              StructField(\"importance\", FloatType(), True),\n",
    "                              StructField(\"bad_wafer\", IntegerType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_model_result(df_run: pd.DataFrame) -> pd.DataFrame:\n",
    "            \"\"\"\n",
    "            Perform PCA modeling on the small sample data.\n",
    "\n",
    "            Parameters:\n",
    "            - df_run: Subset of data for modeling (pandas.DataFrame).\n",
    "\n",
    "            Returns:\n",
    "            - pd.DataFrame: Combined dataframe of every feature and its importance in each combination of grpby_list after PCA modeling.\n",
    "            \"\"\"\n",
    "            df_pivot = FitModelForUvaData.get_pivot_table(df=df_run, grpby_list=grpby_list)\n",
    "            # Since it is a small sample, make a copy to generate more data for the PCA model\n",
    "            df_pivot_copy = df_pivot.copy()\n",
    "            df_pivot_all = pd.concat([df_pivot, df_pivot_copy], axis=0)\n",
    "\n",
    "            # Define independent variables\n",
    "            x_train = df_pivot_all[df_pivot_all.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "            if min(x_train.shape) <= 0:\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            n_components = min(min(x_train.shape) - 2, 20)\n",
    "            model = pca(n_components=n_components, verbose=None)\n",
    "            results = model.fit_transform(x_train)\n",
    "            res_top = results['topfeat']\n",
    "            res_top_select = res_top[res_top['type'] == 'best'][['feature', 'loading']]\n",
    "            res_top_select['importance'] = abs(res_top_select['loading'])\n",
    "            res_top_select = res_top_select.rename(columns={'feature': 'features'}).drop(\"loading\",\n",
    "                                                                                         axis=1).drop_duplicates()\n",
    "\n",
    "            # Add some field information\n",
    "            res_top_select['bad_wafer'] = sum(df_pivot['label'])\n",
    "            for col_ in grpby_list:\n",
    "                res_top_select[col_] = df_run[col_].values[0]\n",
    "            return res_top_select\n",
    "\n",
    "        return df.groupby(grpby_list).apply(get_model_result)\n",
    "\n",
    "\n",
    "class GetFinalResultsForUvaData:\n",
    "    @staticmethod\n",
    "    def split_score_big_sample(df: pyspark.sql.dataframe, grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Split the ROC AUC scores based on the specified grouping columns.\n",
    "\n",
    "        Parameters:\n",
    "        - df: Results after RandomForest modeling.\n",
    "        - grpby_list: List of grouping columns.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: ROC AUC scores result with each element in grpby_list as columns.\n",
    "        \"\"\"\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"bad_wafer\", IntegerType(), True),\n",
    "                              StructField(\"roc_auc_score\", FloatType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_result(model_results: pd.DataFrame) -> pd.DataFrame:\n",
    "            select_expr = grpby_list + ['bad_wafer', 'roc_auc_score']\n",
    "            sample_res = model_results[select_expr].dropna(axis=0)\n",
    "            sample_res = sample_res[sample_res['roc_auc_score'] > 0.6]\n",
    "            return sample_res\n",
    "\n",
    "        return df.groupby(grpby_list).apply(get_result)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_features(df: pd.DataFrame, index: int) -> str:\n",
    "        \"\"\"\n",
    "        Split the 'features' column based on the specified index.\n",
    "\n",
    "        Parameters:\n",
    "        - df: RandomForest modeling results with 'features' column.\n",
    "        - index: Order value.\n",
    "\n",
    "        Returns:\n",
    "        - str: Field attribute value.\n",
    "        \"\"\"\n",
    "        return df['features'].apply(lambda x: x.split('#')[index])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_split_feature_importance_table(df: pd.DataFrame, grpby_list: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the table after splitting the 'features' column based on the specified grouping columns.\n",
    "\n",
    "        Parameters:\n",
    "        - df: RandomForest modeling results with 'features' column.\n",
    "        - grpby_list: List of grouping columns.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Table after splitting features.\n",
    "        \"\"\"\n",
    "        n_feats = len(grpby_list)\n",
    "        for i in range(n_feats):\n",
    "            df[grpby_list[i]] = GetFinalResultsForUvaData.split_features(df, i + 1)\n",
    "\n",
    "        df['parametric_name'] = GetFinalResultsForUvaData.split_features(df, n_feats + 1)\n",
    "        df['step'] = GetFinalResultsForUvaData.split_features(df, n_feats + 2)\n",
    "        df['stats'] = GetFinalResultsForUvaData.split_features(df, n_feats + 3)\n",
    "        df = df.drop(['features'], axis=1).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def add_feature_stats(df: pd.DataFrame, grpby_list: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Add a column with all statistical features of parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - df: Feature importance table after processing.\n",
    "        - grpby_list: List of grouping columns.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: New column containing all statistical features: 'feature_stats'.\n",
    "        \"\"\"\n",
    "        grpby_list_extend = grpby_list + ['parametric_name', 'step']\n",
    "        feature_stats = df.groupby(grpby_list_extend)['stats'].unique().reset_index()\n",
    "        feature_stats['stats'] = [feature_stats['stats'].iloc[i].tolist() for i in range(len(feature_stats))]\n",
    "        feature_stats['stats'] = feature_stats['stats'].apply(lambda x: \"#\".join(x))\n",
    "        feature_stats = feature_stats.assign(\n",
    "            parametric_name=lambda x: x['parametric_name'] + str('#') + x['step']).drop(\n",
    "            'step', axis=1)\n",
    "        return feature_stats\n",
    "\n",
    "    @staticmethod\n",
    "    def split_calculate_features_big_sample(df: pyspark.sql.dataframe, grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Split and calculate features based on the specified grouping columns.\n",
    "\n",
    "        Parameters:\n",
    "        - df: Results after RandomForest modeling.\n",
    "        - grpby_list: List of grouping columns.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Features importance results.\n",
    "        \"\"\"\n",
    "        # Dynamically build schema\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"parametric_name\", StringType(), True),\n",
    "                              StructField(\"importance\", FloatType(), True),\n",
    "                              StructField(\"stats\", StringType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_result(model_results: pd.DataFrame) -> pd.DataFrame:\n",
    "            # Extract 'features' and 'importance' from the RandomForest model results\n",
    "            feature_importance_table = model_results[['features', 'importance']].dropna(axis=0)\n",
    "\n",
    "            # Split features\n",
    "            feature_importance_res_split = GetFinalResultsForUvaData.get_split_feature_importance_table(\n",
    "                df=feature_importance_table,\n",
    "                grpby_list=grpby_list)\n",
    "\n",
    "            # Remove combinations with importance equal to 0\n",
    "            feature_importance_res_split_drop = feature_importance_res_split.query(\"importance > 0\").reset_index(\n",
    "                drop=True)\n",
    "\n",
    "            # Take the top 60% or 100% of each combination result\n",
    "            feature_importance_res_split_nlargest = (feature_importance_res_split_drop.groupby(by=grpby_list)\n",
    "                                                     .apply(\n",
    "                lambda x: x.nlargest(int(x.shape[0] * 0.6), 'importance') if x.shape[0] > 1 else x.nlargest(\n",
    "                    int(x.shape[0] * 1), 'importance'))\n",
    "                                                     .reset_index(drop=True))\n",
    "\n",
    "            # Add a column with all statistical features: 'feature_stats'\n",
    "            feature_stats = GetFinalResultsForUvaData.add_feature_stats(df=feature_importance_res_split_drop,\n",
    "                                                                        grpby_list=grpby_list)\n",
    "\n",
    "            # Sum the importance for the same combination and parameter: 'feature_importance_groupby'\n",
    "            feature_importance_groupby = (\n",
    "                feature_importance_res_split_nlargest.groupby(grpby_list + ['parametric_name', 'step'])['importance']\n",
    "                .sum().reset_index())\n",
    "            feature_importance_groupby = (\n",
    "                feature_importance_groupby.assign(parametric_name=lambda x: x['parametric_name'] + str('#') + x['step'])\n",
    "                .drop('step', axis=1))\n",
    "\n",
    "            # Connect 'feature_stats' and 'feature_importance_groupby'\n",
    "            grpby_stats = pd.merge(feature_stats, feature_importance_groupby,\n",
    "                                   on=grpby_list + ['parametric_name']).dropna().reset_index(drop=True)\n",
    "            return grpby_stats\n",
    "\n",
    "        return df.groupby(grpby_list).apply(get_result)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_final_results_big_sample(s_res: pyspark.sql.dataframe, f_res: pyspark.sql.dataframe, grpby_list: List[str],\n",
    "                                     bad_wafer_num: int) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Get the final modeling results.\n",
    "\n",
    "        Parameters:\n",
    "        - s_res: ROC AUC scores result.\n",
    "        - f_res: Features importance result.\n",
    "        - grpby_list: List of grouping columns.\n",
    "        - bad_wafer_num: Number of bad wafers in the data.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Final modeling result.\n",
    "        \"\"\"\n",
    "        roc_auc_score_all = s_res.agg({\"roc_auc_score\": \"sum\"}).collect()[0][0]\n",
    "        s_res = s_res.withColumn(\"roc_auc_score_ratio\", col(\"roc_auc_score\") / roc_auc_score_all)\n",
    "        s_res = s_res.withColumn(\"bad_ratio\", col(\"bad_wafer\") / bad_wafer_num)\n",
    "\n",
    "        df_merge = s_res.join(f_res, on=grpby_list, how='left')\n",
    "        df_merge = df_merge.withColumn('weight_original',\n",
    "                                       col('roc_auc_score_ratio') * col('bad_ratio') * col('importance'))\n",
    "\n",
    "        # Normalize again\n",
    "        weight_all = df_merge.agg({\"weight_original\": \"sum\"}).collect()[0][0]\n",
    "        df_merge = df_merge.withColumn(\"weight\", col(\"weight_original\") / weight_all)\n",
    "        df_merge = df_merge.select(grpby_list + ['parametric_name', 'weight', 'stats']).orderBy('weight',\n",
    "                                                                                                ascending=False)\n",
    "        return df_merge\n",
    "\n",
    "    @staticmethod\n",
    "    def split_calculate_features_small_sample(df: pyspark.sql.dataframe,\n",
    "                                              grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Calculate features and importance after PCA modeling on a small sample.\n",
    "\n",
    "        Parameters:\n",
    "        - df: PCA modeling results (pyspark.sql.dataframe).\n",
    "        - grpby_list: List of grouping columns (List[str]).\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Dataframe containing features, importance and other information after PCA modeling.\n",
    "        \"\"\"\n",
    "        # Dynamically build schema\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"parametric_name\", StringType(), True),\n",
    "                              StructField(\"importance\", FloatType(), True),\n",
    "                              StructField(\"bad_wafer\", IntegerType(), True),\n",
    "                              StructField(\"stats\", StringType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_result(model_results: pd.DataFrame) -> pd.DataFrame:\n",
    "            feature_importance_table = model_results[['features', 'importance', 'bad_wafer']].dropna(axis=0)\n",
    "            # Split features\n",
    "            feature_importance_res_split = GetFinalResultsForUvaData.get_split_feature_importance_table(\n",
    "                df=feature_importance_table,\n",
    "                grpby_list=grpby_list)\n",
    "\n",
    "            # Add a column with all statistical features containing parameters: feature_stats\n",
    "            feature_stats = GetFinalResultsForUvaData.add_feature_stats(df=feature_importance_res_split,\n",
    "                                                                        grpby_list=grpby_list)\n",
    "\n",
    "            # Sum the same parameter in the same combination: feature_importance_groupby\n",
    "            feature_importance_groupby = (\n",
    "                feature_importance_res_split.groupby(grpby_list + ['bad_wafer', 'parametric_name', 'step'])[\n",
    "                    'importance'].sum().reset_index())\n",
    "            feature_importance_groupby = feature_importance_groupby.assign(\n",
    "                parametric_name=lambda x: x['parametric_name'] + str('#') + x['step']).drop('step', axis=1)\n",
    "\n",
    "            # Connect feature_stats and feature_importance_groupby\n",
    "            grpby_stats = pd.merge(feature_stats, feature_importance_groupby,\n",
    "                                   on=grpby_list + ['parametric_name']).dropna().reset_index(drop=True)\n",
    "            return grpby_stats\n",
    "\n",
    "        return df.groupby(grpby_list).apply(get_result)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_final_results_small_sample(f_res: pyspark.sql.dataframe,\n",
    "                                       bad_wafer_num: int,\n",
    "                                       grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Get the final modeling results for a small sample.\n",
    "\n",
    "        Parameters:\n",
    "        - f_res: Features and importance results (pyspark.sql.dataframe).\n",
    "        - bad_wafer_num: Total number of bad wafers in the data (int).\n",
    "        - grpby_list: List of grouping columns (List[str]).\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Final modeling results with weights and statistics.\n",
    "        \"\"\"\n",
    "        f_res = f_res.withColumn(\"bad_ratio\", col(\"bad_wafer\") / bad_wafer_num)\n",
    "        df_merge = f_res.withColumn('weight_original', col('importance') * col('bad_ratio'))\n",
    "\n",
    "        # Normalize weights again\n",
    "        weight_all = df_merge.agg({\"weight_original\": \"sum\"}).collect()[0][0]\n",
    "        df_merge = df_merge.withColumn(\"weight\", col(\"weight_original\") / weight_all)\n",
    "\n",
    "        # Select columns and order by weight in descending order\n",
    "        df_merge = df_merge.select(grpby_list + ['parametric_name', 'weight', 'stats']).orderBy('weight',\n",
    "                                                                                                ascending=False)\n",
    "        return df_merge\n",
    "\n",
    "    @staticmethod\n",
    "    def add_certain_column(df: pyspark.sql.dataframe, by: str, request_id: str,\n",
    "                           grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Add specific columns to the final modeling results.\n",
    "\n",
    "        Parameters:\n",
    "        - df: Final modeling result.\n",
    "        - by: Grouping column, manually add a column 'add'.\n",
    "        - request_id: Request ID passed in.\n",
    "        - grpby_list: List of grouping columns.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Final modeling result with specific columns added.\n",
    "        \"\"\"\n",
    "        # Dynamically build schema_all\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"stats\", StringType(), True),\n",
    "                              StructField(\"parametric_name\", StringType(), True),\n",
    "                              StructField(\"weight\", FloatType(), True),\n",
    "                              StructField(\"request_id\", StringType(), True),\n",
    "                              StructField(\"weight_percent\", FloatType(), True),\n",
    "                              StructField(\"index_no\", IntegerType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_result(final_res: pd.DataFrame) -> pd.DataFrame:\n",
    "            final_res['weight'] = final_res['weight'].astype(float)\n",
    "            final_res = final_res.query(\"weight > 0\")\n",
    "            final_res['request_id'] = request_id\n",
    "            final_res['weight_percent'] = final_res['weight'] * 100\n",
    "            final_res = final_res.sort_values('weight', ascending=False)\n",
    "            final_res['index_no'] = [i + 1 for i in range(len(final_res))]\n",
    "            final_res = final_res.drop('add', axis=1)\n",
    "            return final_res\n",
    "\n",
    "        return df.groupby(by).apply(get_result)\n",
    "\n",
    "\n",
    "class ExertUvaAlgorithm:\n",
    "    @staticmethod\n",
    "    def fit_big_data_model(df_run: pyspark.sql.dataframe,\n",
    "                           data_dict_list_bs: List[Dict[str, str]],\n",
    "                           grpby_list: List[str],\n",
    "                           request_id: str) -> Union[str, pyspark.sql.dataframe.DataFrame]:\n",
    "        \"\"\"\n",
    "        Perform random forest model on a large sample of data(good_wafer_num >= 3 AND bad_wafer_num >= 3).\n",
    "\n",
    "        Parameters:\n",
    "        - df_run: Preprocessed data after data preprocessing.\n",
    "        - data_dict_list_bs: List of dictionaries with filtering conditions for big sample.\n",
    "        - grpby_list: List of grouping columns.\n",
    "        - request_id: Unique identifier for the request.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Dataframe containing the error message or the final modeling results.\n",
    "        \"\"\"\n",
    "        # 1. Get data for modeling with a large sample\n",
    "        df_run_bs = PreprocessForUvaData.get_train_data(df_run=df_run, data_dict_list=data_dict_list_bs)\n",
    "        if df_run_bs.count() == 0:\n",
    "            msg = 'No data of this type in the database!'\n",
    "            return msg\n",
    "\n",
    "        # 2. Get the total number of bad wafers\n",
    "        bad_wafer_num_big_sample = PreprocessForUvaData.get_all_bad_wafer_num(df=df_run_bs)\n",
    "        if bad_wafer_num_big_sample < 3:\n",
    "            msg = 'The actual number of BAD_WAFER in the database is less than 3, please provide more BAD_WAFER!'\n",
    "            return msg\n",
    "\n",
    "        # 3. Model the selected big sample data\n",
    "        res = FitModelForUvaData.fit_rf_big_sample(df=df_run_bs, grpby_list=grpby_list)\n",
    "        if res.count() == 0:\n",
    "            msg = 'Temporary algorithm exception!'\n",
    "            return msg\n",
    "\n",
    "        # 4. Integrate the modeling results\n",
    "        s_res = GetFinalResultsForUvaData.split_score_big_sample(df=res, grpby_list=grpby_list)\n",
    "        if s_res.count() == 0:\n",
    "            msg = 'The algorithm has a low running score, no output for now, it is recommended to increase the number of BAD_WAFER'\n",
    "            return msg\n",
    "\n",
    "        f_res = GetFinalResultsForUvaData.split_calculate_features_big_sample(df=res, grpby_list=grpby_list)\n",
    "        if f_res.count() == 0:\n",
    "            msg = 'Temporary exception in calculating the algorithm result'\n",
    "            return msg\n",
    "\n",
    "        model_res_bs = GetFinalResultsForUvaData.get_final_results_big_sample(s_res=s_res, f_res=f_res,\n",
    "                                                                              grpby_list=grpby_list,\n",
    "                                                                              bad_wafer_num=bad_wafer_num_big_sample)\n",
    "        if model_res_bs.count() == 0:\n",
    "            msg = 'Temporary exception in splicing algorithm results'\n",
    "            return msg\n",
    "\n",
    "        # 7. Add specific columns\n",
    "        final_res_bs = model_res_bs.withColumn('add', lit(0))\n",
    "        final_res_add_columns = GetFinalResultsForUvaData.add_certain_column(df=final_res_bs, by='add',\n",
    "                                                                             request_id=request_id,\n",
    "                                                                             grpby_list=grpby_list)\n",
    "        if final_res_add_columns.count() == 0:\n",
    "            msg = 'Temporary exception in adding columns to algorithm results'\n",
    "            return msg\n",
    "        else:\n",
    "            return final_res_add_columns\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_small_data_model(df_run: pyspark.sql.dataframe,\n",
    "                             common_res: pyspark.sql.dataframe,\n",
    "                             grpby_list: List[str],\n",
    "                             request_id: str) -> Union[str, pyspark.sql.dataframe.DataFrame]:\n",
    "        \"\"\"\n",
    "        Perform PCA model on a small sample of data (bad_wafer_num >= 1 AND wafer_count >= 2).\n",
    "\n",
    "        Parameters:\n",
    "        - df_run: Preprocessed data after data preprocessing.\n",
    "        - common_res: Common analysis results dataframe.\n",
    "        - grpby_list: List of grouping columns.\n",
    "        - request_id: Unique identifier for the request.\n",
    "\n",
    "        Returns:\n",
    "        - Union[str, DataFrame]: Error message or the final modeling results dataframe.\n",
    "        \"\"\"\n",
    "        # 1. Get data for modeling with a large sample\n",
    "        data_dict_list_ss = PreprocessForUvaData.get_data_list(common_res=common_res, grpby_list=grpby_list,\n",
    "                                                               big_or_small='small')\n",
    "        if len(data_dict_list_ss) == 0:\n",
    "            msg = 'The actual number of WAFER in the database under this query condition is 1, unable to analyze'\n",
    "            return msg\n",
    "\n",
    "        # 2. Get the data\n",
    "        df_run_ss = PreprocessForUvaData.get_train_data(df_run=df_run, data_dict_list=data_dict_list_ss)\n",
    "        if df_run_ss.count() == 0:\n",
    "            msg = 'No data of this type in the database!'\n",
    "            return msg\n",
    "\n",
    "        # 3. Get the total number of bad wafers\n",
    "        bad_wafer_num_small_sample = PreprocessForUvaData.get_all_bad_wafer_num(df_run_ss)\n",
    "        if bad_wafer_num_small_sample <= 1:\n",
    "            msg = 'The actual number of BAD_WAFER in the database under this query condition is 1, please provide more BAD_WAFER!'\n",
    "            return msg\n",
    "\n",
    "        # 4. Model the selected big sample data\n",
    "        res = FitModelForUvaData.fit_pca_small_sample(df=df_run_ss, grpby_list=grpby_list)\n",
    "        if res.count() == 0:\n",
    "            msg = 'Temporary algorithm exception!'\n",
    "            return msg\n",
    "\n",
    "        f_res = GetFinalResultsForUvaData.split_calculate_features_small_sample(df=res, grpby_list=grpby_list)\n",
    "        if f_res.count() == 0:\n",
    "            msg = 'Temporary exception in calculating the algorithm result'\n",
    "            return msg\n",
    "\n",
    "        model_res_ss = GetFinalResultsForUvaData.get_final_results_small_sample(f_res=f_res,\n",
    "                                                                                bad_wafer_num=bad_wafer_num_small_sample,\n",
    "                                                                                grpby_list=grpby_list)\n",
    "        if model_res_ss.count() == 0:\n",
    "            msg = 'Temporary exception in splicing algorithm results'\n",
    "            return msg\n",
    "\n",
    "        final_res_ss = model_res_ss.withColumn('add', lit(0))\n",
    "        final_res_add_columns = GetFinalResultsForUvaData.add_certain_column(df=final_res_ss, by='add',\n",
    "                                                                             request_id=request_id,\n",
    "                                                                             grpby_list=grpby_list)\n",
    "        if final_res_add_columns.count() == 0:\n",
    "            msg = 'Temporary exception in adding columns to algorithm results'\n",
    "            return msg\n",
    "        else:\n",
    "            return final_res_add_columns\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_grpby_list(grpby_list: List[str]) -> bool:\n",
    "        valid_fields = {'PRODUCT_ID', 'OPER_NO', 'EQP_NAME', 'PRODG1', 'TOOL_NAME'}\n",
    "        for field in grpby_list:\n",
    "            if field not in valid_fields:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def get_some_info(df: pd.DataFrame) -> tuple[\n",
    "        Any, Any, Any, Optional[list[Any]], Optional[list[Any]], Optional[list[Any]], Optional[list[Any]], Optional[\n",
    "            list[Any]]]:\n",
    "        \"\"\"\n",
    "        Extracts information from a DataFrame containing request information.\n",
    "\n",
    "        Parameters:\n",
    "        - df: DataFrame containing request information.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple containing parsed information.\n",
    "        \"\"\"\n",
    "        request_id = df[\"requestId\"].values[0]\n",
    "        request_params = df[\"requestParam\"].values[0]\n",
    "        request_params = request_params.replace('\\'', '\\\"')\n",
    "\n",
    "        parse_dict = json.loads(request_params)\n",
    "        grpby_list = parse_dict[0]['grpby_list']\n",
    "        if not ExertUvaAlgorithm.validate_grpby_list(grpby_list):\n",
    "            raise ValueError(\n",
    "                \"Invalid grpby_list. Must include at least one of 'PRODUCT_ID', 'OPER_NO', 'EQP_NAME', 'PRODG1', 'TOOL_NAME'.\")\n",
    "\n",
    "        try:\n",
    "            merge_operno = list(parse_dict[0]['mergeOperno']) if parse_dict[0].get('mergeOperno') else None\n",
    "        except KeyError:\n",
    "            merge_operno = None\n",
    "\n",
    "        try:\n",
    "            merge_prodg1 = list(parse_dict[0]['mergeProdg1']) if parse_dict[0].get('mergeProdg1') else None\n",
    "        except KeyError:\n",
    "            merge_prodg1 = None\n",
    "\n",
    "        try:\n",
    "            merge_product = list(parse_dict[0]['mergeProductId']) if parse_dict[0].get('mergeProductId') else None\n",
    "        except KeyError:\n",
    "            merge_product = None\n",
    "\n",
    "        try:\n",
    "            merge_eqp = list(parse_dict[0]['mergeEqp']) if parse_dict[0].get('mergeEqp') else None\n",
    "        except KeyError:\n",
    "            merge_eqp = None\n",
    "\n",
    "        try:\n",
    "            merge_chamber = list(parse_dict[0]['mergeChamber']) if parse_dict[0].get('mergeChamber') else None\n",
    "        except KeyError:\n",
    "            merge_chamber = None\n",
    "        return parse_dict, request_id, grpby_list, merge_operno, merge_prodg1, merge_product, merge_eqp, merge_chamber\n",
    "\n",
    "    @staticmethod\n",
    "    def run(df_info: pd.DataFrame, df1: pyspark.sql.dataframe):\n",
    "        try:\n",
    "            parse_dict, request_id, grpby_list, merge_operno, merge_prodg1, merge_product, merge_eqp, merge_chamber = ExertUvaAlgorithm.get_some_info(\n",
    "                df_info)\n",
    "            # todo1: 将parse_dict传给解析SQL的函数, 拼接SQL获取数据\n",
    "\n",
    "            # 1. Station merge and data preprocessing\n",
    "            df_integrate_columns = PreprocessForUvaData.integrate_columns(df=df1,\n",
    "                                                                          merge_operno_list=merge_operno,\n",
    "                                                                          merge_prodg1_list=merge_prodg1,\n",
    "                                                                          merge_product_list=merge_product,\n",
    "                                                                          merge_eqp_list=merge_eqp,\n",
    "                                                                          merge_chamber_list=merge_chamber)\n",
    "            m, n = df_integrate_columns.count(), len(df_integrate_columns.columns)\n",
    "            print(f\"Merged data: ({m}, {n})\")\n",
    "            if df_integrate_columns.count() == 0:\n",
    "                msg = 'Station merge exception!！'\n",
    "                return msg\n",
    "\n",
    "            df_run = PreprocessForUvaData.pre_process(df_integrate_columns)\n",
    "            m, n = df_run.count(), len(df_run.columns)\n",
    "            print(f\"Preprocessed data: ({m}, {n})\")\n",
    "            if df_run.count() == 0:\n",
    "                msg = 'No data in the database under this condition！'\n",
    "                return msg\n",
    "\n",
    "            # 2. Commonality analysis\n",
    "            common_res = PreprocessForUvaData.commonality_analysis(df_run=df_run, grpby_list=grpby_list)\n",
    "            common_res.show()\n",
    "            if common_res.count() == 0:\n",
    "                msg = 'Commonality analysis result exception!'\n",
    "                return msg\n",
    "\n",
    "            data_dict_list_bs = PreprocessForUvaData.get_data_list(common_res=common_res, grpby_list=grpby_list,\n",
    "                                                                   big_or_small='big')\n",
    "            print(\"data_dict_list_bs:\", data_dict_list_bs)\n",
    "            print(\"len(data_dict_list_bs):\", len(data_dict_list_bs))\n",
    "\n",
    "            if len(data_dict_list_bs) != 0:\n",
    "                print(\"****************Call Big Sample Algorithm****************\")\n",
    "                result = ExertUvaAlgorithm.fit_big_data_model(df_run=df_run, data_dict_list_bs=data_dict_list_bs,\n",
    "                                                              grpby_list=grpby_list, request_id=request_id)\n",
    "            else:\n",
    "                print(\"****************Call Small Sample Algorithm****************\")\n",
    "                result = ExertUvaAlgorithm.fit_small_data_model(df_run=df_run, common_res=common_res,\n",
    "                                                                grpby_list=grpby_list, request_id=request_id)\n",
    "\n",
    "            if isinstance(result, str):\n",
    "                return result\n",
    "            else:\n",
    "                # todo2: result 需要写入数据库的正确代码\n",
    "                result.show()\n",
    "                # engine = create_engine(\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\".format(user=\"root\", password=\"123456\",\n",
    "                #                                                                                          host=\"192.168.13.17\",\n",
    "                #                                                                                          port=9030,\n",
    "                #                                                                                          db=\"rca\"))\n",
    "                # result.to_sql('uva_results', con=engine, index=False, if_exists='append')\n",
    "\n",
    "        except Exception as e:\n",
    "            result = str(e)\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f675af",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_loads_dict = {\n",
    "        \"requestId\": \"uva\",\n",
    "        \"requestParam\": [\n",
    "            {'dateRange': [{'start': \"2023-12-01 00:00:00\", 'end': \"2024-01-15 00:00:00\"}],\n",
    "             'lot': [],\n",
    "             'operNo': [\"1G.EEG1R\", \"1G.PPB10\"],\n",
    "             'prodg1': [],\n",
    "             'productId': [],\n",
    "             'eqp': [],\n",
    "             'tool': [],\n",
    "             'recipeName': [],\n",
    "             'waferId': {'good': [\"NBX392-15\", \"NBX392-20\", \"NBX392-24\", \"NBX391-24\", \"NBX391-25\", \"NBX548-09\",\n",
    "                                  \"NBX391-01\", \"NBX391-02\", \"NBX391-13\", \"NBX391-17\"],\n",
    "                         'bad': [\"NBX500-10\", \"NBX500-01\", \"NBX500-09\"]},\n",
    "             'uploadId': '20240110170016023',\n",
    "             'grpby_list': [\"PRODG1\", \"OPER_NO\", \"EQP_NAME\", \"TOOL_NAME\"],\n",
    "             'mergeOperno': [],\n",
    "             'mergeProdg1': [],\n",
    "             'mergeProductId': [],\n",
    "             'mergeEqp': [],\n",
    "             'mergeChamber': [],\n",
    "             }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b9c0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_ = pd.DataFrame({\"requestId\": [json_loads_dict[\"requestId\"]],\n",
    "                         \"requestParam\": [json.dumps(json_loads_dict[\"requestParam\"])]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3e4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {'df_info': df_info_, 'df1': df1_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da3c9b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data: (25276, 15)\n",
      "Preprocessed data: (13780, 11)\n",
      "+--------+--------+--------+---------------+-----------+--------+-------+\n",
      "|  PRODG1| OPER_NO|EQP_NAME|      TOOL_NAME|wafer_count|good_num|bad_num|\n",
      "+--------+--------+--------+---------------+-----------+--------+-------+\n",
      "|L2800Z2N|1V.PPB10|   PBT01|PBT01_CGHA_4-14|          4|       1|      3|\n",
      "|L2800Z2N|1V.PPB10|   PBT01|PBT01_CLHA_4-12|          4|       1|      3|\n",
      "|L2800Z2N|1V.PPB10|   PBT01|PBT01_CLHA_4-21|          2|       0|      2|\n",
      "|L2800Z2N|1V.PPB10|   PBT01|PBT01_CGHA_4-24|          2|       0|      2|\n",
      "+--------+--------+--------+---------------+-----------+--------+-------+\n",
      "\n",
      "data_dict_list_bs: []\n",
      "len(data_dict_list_bs): 0\n",
      "****************Call Small Sample Algorithm****************\n",
      "<class 'str'>\n",
      "最后结果是：\n",
      "Temporary algorithm exception!\n"
     ]
    }
   ],
   "source": [
    "result_ = ExertUvaAlgorithm.run(**params_dict)\n",
    "print(type(result_))\n",
    "print(\"最后结果是：\")\n",
    "print(result_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52b21f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "prodg1, oper, eqp, tool = 'L2800Z2N', '1V.PPB10', 'PBT01', 'PBT01_CGHA_4-14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47d3378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prodg1, oper, eqp, tool = 'L2800Z2N', '1V.PPB10', 'PBT01', 'PBT01_CLHA_4-12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c05147fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prodg1, oper, eqp, tool = 'L2800Z2N', '1V.PPB10', 'PBT01', 'PBT01_CLHA_4-21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba635567",
   "metadata": {},
   "outputs": [],
   "source": [
    "prodg1, oper, eqp, tool = 'L2800Z2N', '1V.PPB10', 'PBT01', 'PBT01_CGHA_4-24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d49ad51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAFER_ID</th>\n",
       "      <th>TOOL_ID</th>\n",
       "      <th>RUN_ID</th>\n",
       "      <th>EQP_NAME</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>PRODG1</th>\n",
       "      <th>TOOL_NAME</th>\n",
       "      <th>LOT_ID</th>\n",
       "      <th>RECIPE_NAME</th>\n",
       "      <th>OPER_NO</th>\n",
       "      <th>parametric_name</th>\n",
       "      <th>CASE_INFO</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>STATISTIC_RESULT</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>NBX221-13</td>\n",
       "      <td>9287</td>\n",
       "      <td>329563</td>\n",
       "      <td>PBT01</td>\n",
       "      <td>AFPNR901N.0B0J</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>PBT01_CGHA_4-14</td>\n",
       "      <td>NBX221.100</td>\n",
       "      <td>WaferFlow/PRD/PROCESS/1NJ500N0</td>\n",
       "      <td>1V.PPB10</td>\n",
       "      <td>PLATE_TEMP#HDB205C60S_MEAN#MEAN</td>\n",
       "      <td>2023/10/13</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7478</th>\n",
       "      <td>NBX220-06</td>\n",
       "      <td>9287</td>\n",
       "      <td>323876</td>\n",
       "      <td>PBT01</td>\n",
       "      <td>AFPNR901N.0B0J</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>PBT01_CGHA_4-14</td>\n",
       "      <td>NBX220.150</td>\n",
       "      <td>WaferFlow/PRD/PROCESS/1NJ500N0</td>\n",
       "      <td>1V.PPB10</td>\n",
       "      <td>PLATE_TEMP#HDB205C60S_MEAN#MEAN</td>\n",
       "      <td>2023/9/28</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7481</th>\n",
       "      <td>NBX272-19</td>\n",
       "      <td>9287</td>\n",
       "      <td>331771</td>\n",
       "      <td>PBT01</td>\n",
       "      <td>AFPNR901N.0B0J</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>PBT01_CGHA_4-14</td>\n",
       "      <td>NBX272.000</td>\n",
       "      <td>WaferFlow/PRD/PROCESS/1NJ500N0</td>\n",
       "      <td>1V.PPB10</td>\n",
       "      <td>PLATE_TEMP#HDB205C60S_MEAN#MEAN</td>\n",
       "      <td>2023/10/18</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24786</th>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>9287</td>\n",
       "      <td>362271</td>\n",
       "      <td>PBT01</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>PBT01_CGHA_4-14</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>WaferFlow/PRD/PROCESS/1NJ500N0</td>\n",
       "      <td>1V.PPB10</td>\n",
       "      <td>PLATE_TEMP#HDB205C60S_MEAN#MEAN</td>\n",
       "      <td>2023/12/20</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24787</th>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>9287</td>\n",
       "      <td>362271</td>\n",
       "      <td>PBT01</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>PBT01_CGHA_4-14</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>WaferFlow/PRD/PROCESS/1NJ500N0</td>\n",
       "      <td>1V.PPB10</td>\n",
       "      <td>PLATE_TEMP#HDB205C60S_MEAN#MEAN</td>\n",
       "      <td>2023/12/20</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        WAFER_ID  TOOL_ID  RUN_ID EQP_NAME      PRODUCT_ID    PRODG1  \\\n",
       "488    NBX221-13     9287  329563    PBT01  AFPNR901N.0B0J  L2800Z2N   \n",
       "7478   NBX220-06     9287  323876    PBT01  AFPNR901N.0B0J  L2800Z2N   \n",
       "7481   NBX272-19     9287  331771    PBT01  AFPNR901N.0B0J  L2800Z2N   \n",
       "24786  NBX293-06     9287  362271    PBT01  AFPNR901N.0B0L  L2800Z2N   \n",
       "24787  NBX293-06     9287  362271    PBT01  AFPNR901N.0B0L  L2800Z2N   \n",
       "\n",
       "             TOOL_NAME      LOT_ID                     RECIPE_NAME   OPER_NO  \\\n",
       "488    PBT01_CGHA_4-14  NBX221.100  WaferFlow/PRD/PROCESS/1NJ500N0  1V.PPB10   \n",
       "7478   PBT01_CGHA_4-14  NBX220.150  WaferFlow/PRD/PROCESS/1NJ500N0  1V.PPB10   \n",
       "7481   PBT01_CGHA_4-14  NBX272.000  WaferFlow/PRD/PROCESS/1NJ500N0  1V.PPB10   \n",
       "24786  PBT01_CGHA_4-14  NBX293.200  WaferFlow/PRD/PROCESS/1NJ500N0  1V.PPB10   \n",
       "24787  PBT01_CGHA_4-14  NBX293.200  WaferFlow/PRD/PROCESS/1NJ500N0  1V.PPB10   \n",
       "\n",
       "                       parametric_name   CASE_INFO  STATUS  STATISTIC_RESULT  \\\n",
       "488    PLATE_TEMP#HDB205C60S_MEAN#MEAN  2023/10/13  NORMAL               250   \n",
       "7478   PLATE_TEMP#HDB205C60S_MEAN#MEAN   2023/9/28  NORMAL               250   \n",
       "7481   PLATE_TEMP#HDB205C60S_MEAN#MEAN  2023/10/18  NORMAL               250   \n",
       "24786  PLATE_TEMP#HDB205C60S_MEAN#MEAN  2023/12/20  NORMAL               250   \n",
       "24787  PLATE_TEMP#HDB205C60S_MEAN#MEAN  2023/12/20  NORMAL               250   \n",
       "\n",
       "       label  \n",
       "488        1  \n",
       "7478       1  \n",
       "7481       1  \n",
       "24786      0  \n",
       "24787      0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandass = pandas_df.query(f\"PRODG1 == '{prodg1}' & OPER_NO == '{oper}' & EQP_NAME == '{eqp}' & TOOL_NAME == '{tool}'\")\n",
    "\n",
    "df_pandass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ee37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d3b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b03c328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4bdbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
