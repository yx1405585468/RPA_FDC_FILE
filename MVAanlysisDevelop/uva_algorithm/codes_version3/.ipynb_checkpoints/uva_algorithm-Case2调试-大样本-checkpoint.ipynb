{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7541854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from pca import pca\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, max, col, countDistinct, when, rank, lit\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# from backend_spark.doris_common.doris_client import DorisClient\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#######################################解析SQL########################################\n",
    "#####################################################################################\n",
    "# doris 数据库连接\n",
    "client = DorisClient(\"10.52.199.81\", 18030, 9030, user=\"root\", password=\"Nexchip@123\", data_base=\"etl\",\n",
    "                     mem_limit=\"68719476736\")\n",
    "                     \n",
    "\"\"\"解析sql 的辅助函数\"\"\"\n",
    "def read_sql(sql_stat, read_client=client, session=spark):\n",
    "    df1 = read_client.doris_read(session, sql_stat)\n",
    "    return df1\n",
    "\n",
    "\n",
    "def process_like(key: str, value: list[str]) -> str:\n",
    "    # 处理模糊条件的匹配: (key like 'aa%' or key like \"bb%\")\n",
    "    key = keyword_map_from_json_to_table.get(key)\n",
    "    v_join = ' or '.join([f\"d1.{key} like  '{v.replace('*', '%')}' \" for v in value])\n",
    "    return \"({})\".format(v_join)\n",
    "\n",
    "\n",
    "def process_not_like(key: str, value: list[str]) -> str:\n",
    "    # 处理非模糊条件的匹配:key in ('aa', 'bb')\n",
    "    key = keyword_map_from_json_to_table.get(key)\n",
    "    v_join = \",\".join([f\"'{v}'\" for v in value])\n",
    "    return \"d1.{} in ({})\".format(key, v_join)\n",
    "\n",
    "\n",
    "def test_not_like():\n",
    "    result = (process_not_like(\"tool_name\", [\"aa\", \"bb\", \"cc\"]))\n",
    "    assert \"tool_name in ('aa','bb','cc')\" == result, \"not like 验证失败\"\n",
    "\n",
    "\n",
    "def test_like():\n",
    "    result = process_like(\"tool_name\", [\"aa*\", \"bb*\", \"cc*\"])\n",
    "    assert \"(tool_name like  'aa%'  or tool_name like  'bb%'  or tool_name like  'cc%' )\" == result, \"like 验证失败\"\n",
    "\n",
    "\n",
    "def process_one_keyword(key, value: list[str]) -> Optional[str]:\n",
    "    if len(value) == 0:\n",
    "        return None\n",
    "\n",
    "    not_like_list = [v for v in value if \"*\" not in v]\n",
    "    like_list = [v for v in value if \"*\" in v]\n",
    "\n",
    "    # 处理模糊条件\n",
    "    if len(not_like_list) != 0:\n",
    "        not_like_sql_str = process_not_like(key, not_like_list)\n",
    "    else:\n",
    "        not_like_sql_str = \"\"\n",
    "\n",
    "    # 处理非模糊条件\n",
    "\n",
    "    if len(like_list) != 0:\n",
    "        like_sql_str = process_like(key, like_list)\n",
    "    else:\n",
    "        like_sql_str = \"\"\n",
    "\n",
    "    # 去除为一个元素为空字符串的情况的情况的情况\n",
    "    concat_sql_str_list = [sql_str for sql_str in [like_sql_str, not_like_sql_str] if len(sql_str) != 0]\n",
    "    # 使用or 操作 单字段过滤 的like 和 not like 语句\n",
    "    return \"(\" + \" or \".join(concat_sql_str_list) + \")\"\n",
    "\n",
    "\n",
    "def check_time_start_end(min_time, max_time):\n",
    "    if min_time is not None and max_time is not None:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"起始时间和结束时间必须全填\")\n",
    "\n",
    "def get_time_selection_sql(time_keyword, max_time=None, min_time=None):\n",
    "    \"\"\"\n",
    "    获取时间区间的筛选的sql, 起始时间和结束时间都是可选的\n",
    "    :param time_keyword:\n",
    "    :param max_time:\n",
    "    :param min_time:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 根据取值，生成单个时间过滤条件\n",
    "    if min_time:\n",
    "        time_part_min = f\"d1.{time_keyword} >= '{min_time}'\"\n",
    "    else:\n",
    "        time_part_min = \" \"\n",
    "\n",
    "    if max_time:\n",
    "        time_part_max = f\"d1.{time_keyword} < '{max_time}'\"\n",
    "    else:\n",
    "        time_part_max = \" \"\n",
    "\n",
    "    # 如果存在，拼接多个查询条件，或者只保留一个过滤条件\n",
    "    if (max_time is not None) and (min_time is not None):\n",
    "        time_sql = f' {time_part_min} and {time_part_max}'\n",
    "    elif (max_time is None) and (min_time is None):\n",
    "        time_sql = \" \"\n",
    "    else:\n",
    "        time_sql = time_part_max if max_time else time_part_min\n",
    "\n",
    "    return time_sql\n",
    "\n",
    "def concat_time_filter_sql_with_other_keyword_sql(time_filter_sql: str, other_keyword_sql:str) -> str:\n",
    "    \"\"\"\n",
    "    拼接时间过滤条件与非时间过滤条件\n",
    "    :param time_filter_sql:\n",
    "    :param other_keyword_sql:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    time_strip = time_filter_sql.strip()\n",
    "    other_strip = other_keyword_sql.strip()\n",
    "    if len(time_strip) == 0 and len(other_strip) == 0:\n",
    "        return \"\"\n",
    "    elif len(time_strip) != 0 and len(other_strip) == 0:\n",
    "        return  time_filter_sql\n",
    "    elif len(time_strip) == 0 and len(other_strip) != 0:\n",
    "        return other_keyword_sql\n",
    "    else:\n",
    "        return f'{time_filter_sql} and {other_keyword_sql}'\n",
    "    \n",
    "    \n",
    "def trans_select_condition_to_sql_with_label(select_condition_dict: dict, table_name: str) -> str:\n",
    "    # 查询条件转sql,并打标签，label '0': good wafer, '1': bad wafer\n",
    "    filter_sql_list = []\n",
    "    for keyword, value in select_condition_dict.items():\n",
    "        if keyword not in [\"dateRange\", \"waferId\", \"uploadId\", \"mergeProdg1\"]:\n",
    "            sql_filter_one_keyword = process_one_keyword(keyword, value)\n",
    "            if sql_filter_one_keyword is not None:\n",
    "                filter_sql_list.append(sql_filter_one_keyword)\n",
    "\n",
    "    # 处理时间区间\n",
    "    time_bin = select_condition_dict.get(\"dateRange\")\n",
    "\n",
    "\n",
    "    if len(time_bin) == 1: # list[dict]\n",
    "        time_bin_dict = time_bin[0]\n",
    "        min_time = time_bin_dict.get(\"start\")\n",
    "        max_time = time_bin_dict.get(\"end\")\n",
    "    else:\n",
    "        min_time = None\n",
    "        max_time = None \n",
    "\n",
    "    # 去除时间检查，时间范围为可选输入\n",
    "    # 检查起始时间和结束时间全部非空\n",
    "    # check_time_start_end(min_time, max_time)\n",
    "\n",
    "    # 处理waferId\n",
    "    waferId = select_condition_dict.get(\"waferId\")\n",
    "    good_wafer_list = waferId.get(\"good\")\n",
    "    bad_wafer_list = waferId.get(\"bad\")\n",
    "    upload_id = select_condition_dict.get(\"uploadId\")\n",
    "    # upload_id = '20231116152808771'\n",
    "\n",
    "    # 根据time 过滤条件,生成sql\n",
    "    time_filter_sql = get_time_selection_sql(time_keyword=keyword_map_from_json_to_table.get('dateRange'), max_time=max_time, min_time=min_time)\n",
    "\n",
    "\n",
    "    #if len(good_wafer_list) > 0 and len(bad_wafer_list) > 0:\n",
    "    if upload_id is not None and len(upload_id) > 0:\n",
    "\n",
    "        # good wafer, bad wafe 均有指定，需要从层层字段的过滤的条件下选择\n",
    "        # good_wafer_filter_sql = process_one_keyword(\"waferId\", good_wafer_list)\n",
    "        # bad_wafer_filter_sql = process_one_keyword(\"waferId\", bad_wafer_list)\n",
    "        # # or 拼接\n",
    "        # wafer_filter_sql = \" or \".join([good_wafer_filter_sql, bad_wafer_filter_sql])\n",
    "        # wafer_filter_sql = f\"({wafer_filter_sql})\"\n",
    "        # # 加入wafer 过滤条件\n",
    "        # filter_sql_list.append(wafer_filter_sql)\n",
    "        other_keyword_filter = \" and \".join(filter_sql_list)\n",
    "\n",
    "        case_when_statment = f\"\"\"  (case\n",
    "        when d2.GB_FLAG = 'good' then 0 \n",
    "        else 1\n",
    "        end ) as label\n",
    "        \"\"\"\n",
    "\n",
    "        filter_sql_concat = concat_time_filter_sql_with_other_keyword_sql(time_filter_sql, other_keyword_filter)\n",
    "\n",
    "        if filter_sql_concat != '':\n",
    "            select_sql = f\"\"\"select *, {case_when_statment} from {table_name} d1  \n",
    "            join etl.UPLOADED_WAFER d2\n",
    "        on d1.WAFER_ID = d2.WAFER_ID\n",
    "\n",
    "            where {filter_sql_concat} and d2.UPLOAD_ID = '{upload_id}'\"\"\"\n",
    "        else:\n",
    "            select_sql = f\"\"\"select *, {case_when_statment} from {table_name} d1  \n",
    "            join etl.UPLOADED_WAFER d2\n",
    "        on d1.WAFER_ID = d2.WAFER_ID where d2.UPLOAD_ID = '{upload_id}'\"\"\"\n",
    "\n",
    "    else: \n",
    "        raise ValueError(\"good bad wafer 都必须选！\")\n",
    "\n",
    "    # elif len(good_wafer_list) > 0 and len(bad_wafer_list) == 0:\n",
    "    #     # 选good, 剩余为 bad\n",
    "\n",
    "    #     other_keyword_filter = \" and \".join(filter_sql_list)\n",
    "    #     good_wafer_filter_sql = process_one_keyword(\"waferId\", good_wafer_list)\n",
    "\n",
    "    #     case_when_statment = f\"(case when {good_wafer_filter_sql} then 0 else 1  end ) label\"\n",
    "    #     filter_sql_concat = concat_time_filter_sql_with_other_keyword_sql(time_filter_sql, other_keyword_filter)\n",
    "    #     select_sql = f\"select *, {case_when_statment} from {table_name} where {filter_sql_concat}\"\n",
    "\n",
    "    # elif len(good_wafer_list) == 0 and len(bad_wafer_list) > 0:\n",
    "    #     # 选bad, 剩余为good\n",
    "    #     other_keyword_filter = \" and \".join(filter_sql_list)\n",
    "    #     bad_wafer_filter_sql = process_one_keyword(\"waferId\", bad_wafer_list)\n",
    "    #     case_when_statment = f\"\"\"(case when {bad_wafer_filter_sql} then 1 else 0 end ) label\"\"\"\n",
    "    #     filter_sql_concat = concat_time_filter_sql_with_other_keyword_sql(time_filter_sql, other_keyword_filter)\n",
    "    #     select_sql = f\"select *, {case_when_statment} from {table_name} where {filter_sql_concat}\"\n",
    "    #     # case1 stat results 表没有case_info 时间列，暂时去掉\n",
    "        # select_sql = f\"select *, {case_when_statment} from {table_name} where {other_keyword_filter}\"\n",
    "\n",
    "    # print(select_sql)    \n",
    "    select_sql = select_sql.replace(\"*\",  \"d1.WAFER_ID, d1.TOOL_ID, d1.RUN_ID, d1.EQP_NAME, d1.PRODUCT_ID, d1.PRODG1, d1.TOOL_NAME, d1.LOT_ID, d1.RECIPE_NAME, d1.OPER_NO, d1.parametric_name, d1.CASE_INFO, d1.STATUS, d1.STATISTIC_RESULT\")\n",
    "    if table_name == \"etl.DWD_POC_CASE_FD_UVA_DATA_TEST\":\n",
    "        select_sql = f\"{select_sql} and d1.STATUS != 'ERROR'\"\n",
    "    print(\"select_sql\", select_sql) \n",
    "    return select_sql\n",
    " \n",
    "\n",
    "\n",
    "def get_data_from_doris(select_condition_list, table_name):\n",
    "    select_df_list = [read_sql(trans_select_condition_to_sql_with_label(select_condition_dict, table_name)) for select_condition_dict in select_condition_list]\n",
    "    # 多个进行union\n",
    "    df1 = reduce(DataFrame.unionAll, select_df_list)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2377dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "##############################从kafka消息读取需要的资料#########################\n",
    "############################################################################\n",
    "def get_some_info(df:pd.DataFrame):\n",
    "    if len(df) > 0:\n",
    "        df = df.head(1)\n",
    "\n",
    "    request_id = df[\"requestId\"].values[0]\n",
    "    request_params = df[\"requestParam\"].values[0]\n",
    "    # 避免存在单引号，因为json 引号只有双引号\n",
    "    request_params = request_params.replace('\\'', \"\\\"\")   \n",
    "    parse_dict = json.loads(request_params)\n",
    "    merge_prodg1 = parse_dict[0]['mergeProdg1']\n",
    "    \n",
    "    try:\n",
    "        merge_operno = list(parse_dict[0]['mergeOperno'])\n",
    "    except KeyError:\n",
    "        merge_operno = None\n",
    "\n",
    "    if merge_prodg1 == '1':\n",
    "        grpby_list = ['OPER_NO', 'TOOL_NAME']\n",
    "    elif merge_prodg1 == '0':\n",
    "        grpby_list = ['PRODG1', 'OPER_NO', 'TOOL_NAME']\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return parse_dict, request_id, grpby_list, merge_operno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e78a413c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/local/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "# 真正的kafka消息里全都是双引号\n",
    "json_loads_dict = {\n",
    "    \"requestId\": \"fff\",\n",
    "    \"requestParam\": [\n",
    "        {'dateRange': [{'start': \"2023-12-01 00:00:00\", 'end': \"2024-01-15 00:00:00\"}], \n",
    "         'lot': [], \n",
    "         'operNo': [\"1G.EEG1R\",\"1G.PPB10\"], \n",
    "         'prodg1': [], \n",
    "         'productId': [], \n",
    "         'eqp': [], \n",
    "         'tool': [], \n",
    "         'recipeName': [], \n",
    "         'waferId': {'good': [\"NBX392-15\",\"NBX392-20\",\"NBX392-24\",\"NBX391-24\",\"NBX391-25\",\"NBX548-09\",\n",
    "                     \"NBX391-01\",\"NBX391-02\",\"NBX391-13\",\"NBX391-17\"], \n",
    "                     'bad': [\"NBX500-10\",\"NBX500-01\",\"NBX500-09\"]}, \n",
    "         'uploadId': '20240110170016023', \n",
    "         'mergeProdg1': '0',\n",
    "         'mergeOperno': [{\"2F.CDS10_XX.TDS01\": [\"2F.CDS10\", \"XX.TDS01\"]},\n",
    "                           {\"2F.CDS20_XX.CDS20\": [\"2F.CDS20\", \"XX.CDS20\"]}]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_pa = pd.DataFrame({\n",
    "    \"requestId\": [json_loads_dict[\"requestId\"]], \n",
    "    \"requestParam\": [json.dumps(json_loads_dict[\"requestParam\"])]})\n",
    "\n",
    "df1 = ps.from_pandas(df_pa).to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "04740a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse_dict是： [{'dateRange': [{'start': '2023-12-01 00:00:00', 'end': '2024-01-15 00:00:00'}], 'lot': [], 'operNo': ['1G.EEG1R', '1G.PPB10'], 'prodg1': [], 'productId': [], 'eqp': [], 'tool': [], 'recipeName': [], 'waferId': {'good': ['NBX392-15', 'NBX392-20', 'NBX392-24', 'NBX391-24', 'NBX391-25', 'NBX548-09', 'NBX391-01', 'NBX391-02', 'NBX391-13', 'NBX391-17'], 'bad': ['NBX500-10', 'NBX500-01', 'NBX500-09']}, 'uploadId': '20240110170016023', 'mergeProdg1': '0', 'mergeOperno': [{'2F.CDS10_XX.TDS01': ['2F.CDS10', 'XX.TDS01']}, {'2F.CDS20_XX.CDS20': ['2F.CDS20', 'XX.CDS20']}]}]\n",
      "parse_dict的类型是： <class 'list'>\n",
      "request_id是： fff\n",
      "grpby_list是： ['PRODG1', 'OPER_NO', 'TOOL_NAME']\n",
      "merge_operno是： [{'2F.CDS10_XX.TDS01': ['2F.CDS10', 'XX.TDS01']}, {'2F.CDS20_XX.CDS20': ['2F.CDS20', 'XX.CDS20']}]\n"
     ]
    }
   ],
   "source": [
    "#  1. 解析json 为字典， df1为kafka输入的结果数据，获取到parse_dict, request_id, grpby_list\n",
    "df2 = df1.toPandas() \n",
    "parse_dict, request_id, grpby_list, merge_operno = get_some_info(df2)\n",
    "print(\"parse_dict是：\", parse_dict)\n",
    "print(\"parse_dict的类型是：\", type(parse_dict))\n",
    "print(\"request_id是：\", request_id)\n",
    "print(\"grpby_list是：\", grpby_list)\n",
    "print(\"merge_operno是：\", merge_operno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "58e5a7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(merge_operno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "86c7cb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170117, 16)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPER_NO IN ('2F.CDS10', 'XX.TDS01', '1C.CDS10', '1F.CDS10')\n",
    "df_pandas = pd.read_csv(\"DWD_POC_CASE_FD_UVA_DATA_CASE2_PROCESSED_FOUR_OPERNO.csv\")\n",
    "\n",
    "# 以上数据量太大，会产生Connection refused, 选择特定的站点\n",
    "df_pandas = df_pandas[df_pandas['OPER_NO'].isin(['2F.CDS10', 'XX.TDS01'])]\n",
    "df_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4424521a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2F.CDS10', 'XX.TDS01'], dtype=object)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas['OPER_NO'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9e06b1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-07-09 03:40:08'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas['START_TIME'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "48c1ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas[df_pandas['label']==1]['WAFER_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4336cb28",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/local/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "170117"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = ps.from_pandas(df_pandas).to_spark()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "98ae65eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+--------+--------------+--------+---------+----------+-----------+--------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "| WAFER_ID|TOOL_ID|RUN_ID|EQP_NAME|    PRODUCT_ID|  PRODG1|TOOL_NAME|    LOT_ID|RECIPE_NAME| OPER_NO|         START_TIME|     parametric_name| CASE_INFO|STATUS|STATISTIC_RESULT|label|\n",
      "+---------+-------+------+--------+--------------+--------+---------+----------+-----------+--------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|RFREFLECTEDPOWER_...|2023-06-11|NORMAL|            13.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|CUSTOMEQUATION#CU...|2023-06-11|NORMAL|        0.005367|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|WAFER_TEMPERATURE...|2023-06-11|NORMAL|   490.822509734|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_NF3_TOP#ZERO...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_SIH4#ZERO_CH...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|HX_WATER_TEMPERAT...|2023-06-11|NORMAL|            75.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|X_LCF_RET#LCF_OUT...|2023-06-11|NORMAL|          5.9E-5|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_SIH4_SETTING...|2023-06-11|NORMAL|          129.54|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_SIH4_TOP#DEP...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_HE_SIDE#DEPO...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FI_FFU_FAN_PRESSU...|2023-06-11|NORMAL|           0.005|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_AR_TOP#ZERO_...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_NF3#ZERO_CHE...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_O2#ZERO_CHEC...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|CHAMBER_PRESSURE#...|2023-06-11|NORMAL|          6.1E-4|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_O2#DEPO_01#MEAN|2023-06-11|NORMAL|           300.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_SIH4_TOP_SET...|2023-06-11|NORMAL|           32.13|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|RFFORWARDPOWER_TO...|2023-06-11|NORMAL|             2.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_HE_TOP#DEPO_...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|RFFORWARDPOWER_TO...|2023-06-11|NORMAL|          5000.0|    0|\n",
      "+---------+-------+------+--------+--------------+--------+---------+----------+-----------+--------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d1edd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "##########################融合OPER_NO字段##########################\n",
    "###################################################################\n",
    "def integrate_operno(df, merge_operno_list):\n",
    "    if merge_operno_list is not None:\n",
    "        # 将mergeOperno中每个字典的values提取出来，组成一个列表\n",
    "        values_to_replace = [list(rule.values())[0] for rule in merge_operno_list]\n",
    "\n",
    "        # 将每一个字典中的values拼接起来\n",
    "        merged_values = [\"_\".join( list(rule.values())[0]) for rule in merge_operno_list]\n",
    "\n",
    "        for values, replacement_value in zip(values_to_replace, merged_values):\n",
    "            df = df.withColumn(\"OPER_NO\", when(col(\"OPER_NO\").isin(values), replacement_value).otherwise(col(\"OPER_NO\")))\n",
    "        return df\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "232a8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = integrate_operno(df=df1, merge_operno_list=merge_operno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "23053032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7a2e5f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+--------+--------------+--------+---------+----------+-----------+-----------------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "| WAFER_ID|TOOL_ID|RUN_ID|EQP_NAME|    PRODUCT_ID|  PRODG1|TOOL_NAME|    LOT_ID|RECIPE_NAME|          OPER_NO|         START_TIME|     parametric_name| CASE_INFO|STATUS|STATISTIC_RESULT|label|\n",
      "+---------+-------+------+--------+--------------+--------+---------+----------+-----------+-----------------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|RFREFLECTEDPOWER_...|2023-06-11|NORMAL|            13.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|CUSTOMEQUATION#CU...|2023-06-11|NORMAL|        0.005367|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|WAFER_TEMPERATURE...|2023-06-11|NORMAL|   490.822509734|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|FLOW_NF3_TOP#ZERO...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|FLOW_SIH4#ZERO_CH...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|HX_WATER_TEMPERAT...|2023-06-11|NORMAL|            75.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|X_LCF_RET#LCF_OUT...|2023-06-11|NORMAL|          5.9E-5|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|FLOW_SIH4_SETTING...|2023-06-11|NORMAL|          129.54|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|FLOW_SIH4_TOP#DEP...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|FLOW_HE_SIDE#DEPO...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|FI_FFU_FAN_PRESSU...|2023-06-11|NORMAL|           0.005|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|FLOW_AR_TOP#ZERO_...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|FLOW_NF3#ZERO_CHE...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|FLOW_O2#ZERO_CHEC...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|CHAMBER_PRESSURE#...|2023-06-11|NORMAL|          6.1E-4|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|FLOW_O2#DEPO_01#MEAN|2023-06-11|NORMAL|           300.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|FLOW_SIH4_TOP_SET...|2023-06-11|NORMAL|           32.13|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|RFFORWARDPOWER_TO...|2023-06-11|NORMAL|             2.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|FLOW_HE_TOP#DEPO_...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10_XX.TDS01|2023-06-11 20:57:01|RFFORWARDPOWER_TO...|2023-06-11|NORMAL|          5000.0|    0|\n",
      "+---------+-------+------+--------+--------------+--------+---------+----------+-----------+-----------------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8e3e0657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2F.CDS10_XX.TDS01'], dtype=object)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.toPandas()['OPER_NO'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e1c8dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "##################################FDC数据预处理###############################\n",
    "############################################################################\n",
    "def _pre_process(df):\n",
    "    \"\"\"\n",
    "    param df: 从数据库中读取出来的某个CASE数据\n",
    "    return: 数据预处理，后面要根据实际情况统一添加\n",
    "    \"\"\"\n",
    "    # 只选出会用到的列\n",
    "    df = df.select('WAFER_ID', 'TOOL_ID', 'RUN_ID', 'EQP_NAME', 'PRODUCT_ID', 'PRODG1', 'TOOL_NAME',\n",
    "                   'OPER_NO', 'parametric_name', 'STATISTIC_RESULT', 'label')\n",
    "    # 剔除NA值\n",
    "    df = df.filter(col('STATISTIC_RESULT').isNotNull())\n",
    "    # 按照所有的行进行去重\n",
    "    df1 = df.dropDuplicates()\n",
    "    # 选最新的RUN\n",
    "    df2 = df1.groupBy('WAFER_ID', 'OPER_NO', 'TOOL_ID').agg(max('RUN_ID').alias('RUN_ID'))\n",
    "    df_run = df1.join(df2.dropDuplicates(subset=['WAFER_ID', 'OPER_NO', 'TOOL_ID', 'RUN_ID']),\n",
    "                      on=['WAFER_ID', 'OPER_NO', 'TOOL_ID', 'RUN_ID'], how='inner')\n",
    "    return df_run\n",
    "\n",
    "\n",
    "\n",
    "def commonality_analysis(df_run, grpby_list):\n",
    "    \"\"\"\n",
    "    param df_run: 数据预处理后的数据\n",
    "    return: 共性分析后的结果， 返回bad wafer前十的组合\n",
    "    \"\"\"\n",
    "    grps = (df_run.groupBy(grpby_list)\n",
    "            .agg(countDistinct('WAFER_ID').alias('wafer_count'),\n",
    "                 countDistinct('WAFER_ID', when(df_run['label'] == 0, 1)).alias('good_num'),\n",
    "                 countDistinct('WAFER_ID', when(df_run['label'] == 1, 1)).alias('bad_num'))\n",
    "            .orderBy('bad_num', ascending=False))\n",
    "\n",
    "    # 单站点+单腔室的情况\n",
    "    if grps.count() == 1:\n",
    "        return grps\n",
    "    else:\n",
    "        grps = grps.filter(grps['bad_num'] > 0)\n",
    "        window_sep = Window().orderBy(col(\"bad_num\").desc())\n",
    "        ranked_df = grps.withColumn(\"rank\", rank().over(window_sep))\n",
    "        grpss = ranked_df.filter(col(\"rank\") <= 10).drop(\"rank\")\n",
    "        return grpss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ba0ae6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170117\n"
     ]
    }
   ],
   "source": [
    "df_run = _pre_process(df1)\n",
    "print(df_run.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e1a51a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+---------+-----------+--------+-------+\n",
      "|  PRODG1|          OPER_NO|TOOL_NAME|wafer_count|good_num|bad_num|\n",
      "+--------+-----------------+---------+-----------+--------+-------+\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|        186|      47|    139|\n",
      "|C90WA01A|2F.CDS10_XX.TDS01|  DSA02_B|         50|       0|     50|\n",
      "|C90WA15A|2F.CDS10_XX.TDS01|  DSA02_B|         33|       8|     25|\n",
      "|C90WA20A|2F.CDS10_XX.TDS01|  DSA02_B|         49|      25|     24|\n",
      "+--------+-----------------+---------+-----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "common_res = commonality_analysis(df_run, grpby_list)\n",
    "common_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9344e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grpby_list1 = ['PRODG1', 'OPER_NO', 'TOOL_NAME']\n",
    "# common_res1 = commonality_analysis(df_run, grpby_list1)\n",
    "# common_res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267d955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "501fa47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#################################获取样本数据#########################\n",
    "############################################################################\n",
    "def get_data_list(common_res, grpby_list, big_or_small='big'):\n",
    "    \"\"\"\n",
    "    param common_res: 共性分析后的结果, 按照大样本或者小样本条件筛选出组合\n",
    "    param grpby_list: 按照PRODG1+OPER_NO+TOOL_NAME分组或OPER_NO+TOOL_NAME分组\n",
    "    param big_or_small: big或者small\n",
    "    return: 对应组合的字典形式, 包在一个大列表中\n",
    "    \"\"\"\n",
    "    assert big_or_small in ['big', 'small'], \"只能选择big或者small, 请检查拼写\"\n",
    "    if big_or_small == 'big':\n",
    "        good_bad_grps = common_res.filter(\"good_num >= 3 AND bad_num >= 3\")\n",
    "    else:\n",
    "        good_bad_grps = common_res.filter(\"bad_num >= 1 AND wafer_count >=2\")\n",
    "    good_bad_grps = good_bad_grps.orderBy(col(\"bad_num\").desc(), col(\"wafer_count\").desc(), col(\"good_num\").desc()).limit(5)\n",
    "\n",
    "    if 'PRODG1' in grpby_list:\n",
    "        data_list = good_bad_grps['PRODG1', 'OPER_NO', 'TOOL_NAME'].collect()  \n",
    "    else:\n",
    "        data_list = good_bad_grps['OPER_NO', 'TOOL_NAME'].collect()\n",
    "\n",
    "    data_dict_list = [row.asDict() for row in data_list]\n",
    "    return data_dict_list\n",
    "\n",
    "\n",
    "def get_train_data(df_run, data_dict_list):\n",
    "    \"\"\"\n",
    "    param df_run: 数据预处理后的数据\n",
    "    param data_dict: 筛选后的字典结果\n",
    "    return: 从原始数据中过滤出真正用来建模的组合数据\n",
    "    \"\"\"\n",
    "    if len(data_dict_list[0]) == 3:\n",
    "        prod, oper, tool = data_dict_list[0]['PRODG1'], data_dict_list[0]['OPER_NO'], data_dict_list[0]['TOOL_NAME']\n",
    "        df_s = df_run.filter(\"PRODG1 == '{}' AND OPER_NO == '{}' AND TOOL_NAME == '{}'\".format(prod, oper, tool))\n",
    "        for i in range(1, len(data_dict_list)):\n",
    "            prod, oper, tool = data_dict_list[i]['PRODG1'], data_dict_list[i]['OPER_NO'], data_dict_list[i]['TOOL_NAME']\n",
    "            df_m = df_run.filter(\"PRODG1 == '{}' AND OPER_NO == '{}' and TOOL_NAME == '{}'\".format(prod, oper, tool))\n",
    "            df_s = df_s.union(df_m)\n",
    "    else:\n",
    "        oper, tool = data_dict_list[0]['OPER_NO'], data_dict_list[0]['TOOL_NAME']\n",
    "        df_s = df_run.filter(\"OPER_NO == '{}' AND TOOL_NAME == '{}'\".format(oper, tool))\n",
    "        for i in range(1, len(data_dict_list)):\n",
    "            oper, tool = data_dict_list[i]['OPER_NO'], data_dict_list[i]['TOOL_NAME']\n",
    "            df_m = df_run.filter(\"OPER_NO == '{}' and TOOL_NAME == '{}'\".format(oper, tool))\n",
    "            df_s = df_s.union(df_m)\n",
    "    return df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4a7e6595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'PRODG1': 'C90WA12A',\n",
       "  'OPER_NO': '2F.CDS10_XX.TDS01',\n",
       "  'TOOL_NAME': 'DSA02_B'},\n",
       " {'PRODG1': 'C90WA15A',\n",
       "  'OPER_NO': '2F.CDS10_XX.TDS01',\n",
       "  'TOOL_NAME': 'DSA02_B'},\n",
       " {'PRODG1': 'C90WA20A',\n",
       "  'OPER_NO': '2F.CDS10_XX.TDS01',\n",
       "  'TOOL_NAME': 'DSA02_B'}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict_list_bs = get_data_list(common_res, grpby_list, big_or_small='big')\n",
    "data_dict_list_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ae81e8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129674"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_run_bs = get_train_data(df_run, data_dict_list_bs)\n",
    "df_run_bs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d486472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4b3f6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#########################获取传入的整个数据中的所有bad_wafer个数############\n",
    "############################################################################\n",
    "def get_all_bad_wafer_num(df):\n",
    "    \"\"\"\n",
    "    param df: 筛选后的数据\n",
    "    return: 数据中所有bad_wafer的数量\n",
    "    \"\"\"\n",
    "    return df.filter(\"label == 1\").select('WAFER_ID').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f7b0b5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_wafer_num_big_sample = get_all_bad_wafer_num(df_run_bs)\n",
    "bad_wafer_num_big_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26aaa44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d6c55a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#####################对good>=3和bad>=3的数据，用rf建模######################\n",
    "############################################################################\n",
    "def get_pivot_table(df, by):\n",
    "    \"\"\"\n",
    "    param df: 大样本组合的数据\n",
    "    param by: 分组字段\n",
    "    return: 表格透视后的结果\n",
    "    \"\"\"\n",
    "    if len(by) == 3:\n",
    "        df_pivot = df.dropna(axis=0).pivot_table(index=['WAFER_ID', 'label'], \n",
    "                                                     columns=['OPER_NO', 'TOOL_NAME', 'parametric_name', 'PRODG1'],\n",
    "                                                     values=['STATISTIC_RESULT'])\n",
    "    else:\n",
    "        df_pivot = df.dropna(axis=0).pivot_table(index=['WAFER_ID', 'label'], \n",
    "                                                     columns=['OPER_NO', 'TOOL_NAME', 'parametric_name'],\n",
    "                                                     values=['STATISTIC_RESULT'])\n",
    "    df_pivot.columns = df_pivot.columns.map('#'.join)\n",
    "    df_pivot = df_pivot.fillna(df_pivot.mean()).reset_index(drop=False)\n",
    "    return df_pivot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_rf_big_sample(df, by):\n",
    "    \"\"\"\n",
    "    param df: 大样本组合的数据\n",
    "    param by: 分组字段\n",
    "    return: RandomForest建模后的结果\n",
    "    \"\"\"\n",
    "    schema_all = StructType([\n",
    "        StructField(\"PRODG1\", StringType(), True),\n",
    "        StructField(\"OPER_NO\", StringType(), True),\n",
    "        StructField(\"TOOL_NAME\", StringType(), True),\n",
    "        StructField(\"bad_wafer\", IntegerType(), True),\n",
    "        StructField(\"roc_auc_score\", FloatType(), True),\n",
    "        StructField(\"features\", StringType(), True),\n",
    "        StructField(\"importance\", FloatType(), True)])\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_model_result(df_run):\n",
    "        # 表格透视\n",
    "        df_pivot = get_pivot_table(df=df_run, by=by)\n",
    "\n",
    "        # 定义自变量和因变量\n",
    "        X_train = df_pivot[df_pivot.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "        y_train = df_pivot[['label']]\n",
    "\n",
    "        z_ratio = y_train.value_counts(normalize=True)\n",
    "        good_ratio = z_ratio[0]\n",
    "        bad_ratio = z_ratio[1]\n",
    "        if abs(good_ratio - bad_ratio) > 0.7:\n",
    "            undersampler = ClusterCentroids(random_state=101)\n",
    "            X_train, y_train = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        # 网格搜索\n",
    "        pipe = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value=-999)),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', RandomForestClassifier())])\n",
    "        param_grid = {'model__n_estimators': [*range(50, 100, 10)],\n",
    "                      'model__max_depth': [*range(10, 50, 10)]}\n",
    "        grid = GridSearchCV(estimator=pipe, scoring='roc_auc', param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "        grid.fit(X_train.values, y_train.values.ravel())\n",
    "        roc_auc_score_ = grid.best_score_\n",
    "\n",
    "        # 特征重要度、结果汇总\n",
    "        small_importance_res = pd.DataFrame({\n",
    "            'features': X_train.columns,\n",
    "            'importance': grid.best_estimator_.steps[2][1].feature_importances_}).sort_values(by='importance',\n",
    "                                                                                              ascending=False)\n",
    "        if len(by) == 3:\n",
    "            small_sample_res = pd.DataFrame({\n",
    "                'PRODG1': df_run['PRODG1'].unique(),\n",
    "                'OPER_NO': df_run['OPER_NO'].unique(),\n",
    "                'TOOL_NAME': df_run['TOOL_NAME'].unique(),\n",
    "                'bad_wafer': sum(df_pivot['label']),\n",
    "                'roc_auc_score': roc_auc_score_})\n",
    "        else:\n",
    "            PRODG1 = 'grplen2'\n",
    "            small_sample_res = pd.DataFrame({\n",
    "                'PRODG1': PRODG1,\n",
    "                'OPER_NO': df_run['OPER_NO'].unique(),\n",
    "                'TOOL_NAME': df_run['TOOL_NAME'].unique(),\n",
    "                'bad_wafer': sum(df_pivot['label']),\n",
    "                'roc_auc_score': roc_auc_score_})\n",
    "        return pd.concat([small_importance_res, small_sample_res])\n",
    "    return df.groupby(by).apply(get_model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "254b6a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+---------+-------------+--------------------+-----------+\n",
      "|PRODG1|OPER_NO|TOOL_NAME|bad_wafer|roc_auc_score|            features| importance|\n",
      "+------+-------+---------+---------+-------------+--------------------+-----------+\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.090310596|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.043378346|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.031614035|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.030482898|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.029445337|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...| 0.02901291|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.026732197|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.022282088|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.020633213|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.020565132|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.019217685|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.017022232|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.016260069|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.016178478|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.015461712|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.014890661|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.012600787|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.012507779|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...|0.012478871|\n",
      "|  null|   null|     null|     null|         null|STATISTIC_RESULT#...| 0.01217653|\n",
      "+------+-------+---------+---------+-------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grpby_list = ['OPER_NO', 'TOOL_NAME']\n",
    "res = fit_rf_big_sample(df=df_run_bs, by=grpby_list)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567097e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cea8b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#########################对good>=3和bad>=3建模后的结果进行整合############################\n",
    "#####################################################################################\n",
    "def split_score_big_sample(df, by):\n",
    "    \"\"\"\n",
    "    param df: RandomForest建模后的结果\n",
    "    param by: 分组字段\n",
    "    return: roc_auc分数结果\n",
    "    \"\"\"\n",
    "    schema_all = StructType([StructField(\"PRODG1\", StringType(), True),\n",
    "                             StructField(\"OPER_NO\", StringType(), True),\n",
    "                             StructField(\"TOOL_NAME\", StringType(), True),\n",
    "                             StructField(\"bad_wafer\", IntegerType(), True),\n",
    "                             StructField(\"roc_auc_score\", FloatType(), True)])\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_result(model_results):\n",
    "        sample_res = model_results[['PRODG1', 'OPER_NO', 'TOOL_NAME', 'bad_wafer', 'roc_auc_score']].dropna(axis=0)\n",
    "        sample_res = sample_res[sample_res['roc_auc_score'] > 0.6]\n",
    "        return sample_res\n",
    "    return df.groupby(by).apply(get_result)\n",
    "\n",
    "\n",
    "\n",
    "def split_features(df, index) -> str:\n",
    "    \"\"\"\n",
    "    param df: RandomForest建模后的feature_importance_table\n",
    "    param index: 顺序值\n",
    "    return: 字段属性值\n",
    "    \"\"\"\n",
    "    return df['features'].apply(lambda x: x.split('#')[index])\n",
    "\n",
    "\n",
    "def get_split_feature_importance_table(df, by):\n",
    "    \"\"\"\n",
    "    param df: RandomForest建模后的feature_importance_table\n",
    "    param by: OPER_NO+TOOL_NAME+PRODG1或者OPER_NO+TOOL_NAME\n",
    "    return: 分裂features后的表\n",
    "    \"\"\"\n",
    "    df['STATISTIC_RESULT'] = split_features(df, 0)\n",
    "    df['OPER_NO'] = split_features(df, 1)\n",
    "    df['TOOL_NAME'] = split_features(df, 2)\n",
    "    df['parametric_name'] = split_features(df, 3)\n",
    "    df['step'] = split_features(df, 4)\n",
    "    df['stats'] = split_features(df, 5)\n",
    "\n",
    "    if 'PRODG1' in by:\n",
    "        df['PRODG1'] = split_features(df, 6)\n",
    "    else:\n",
    "        df = df.assign(PRODG1 = 'grplen2')\n",
    "\n",
    "    df = df.drop(['features', 'STATISTIC_RESULT'], axis=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_feature_stats(df):\n",
    "    \"\"\"\n",
    "    param df: 经过处理后的feature_importance_table\n",
    "    return: 新增一列，含有参数的所有统计特征:feature_stats\n",
    "    \"\"\"\n",
    "    feature_stats = df.groupby(['PRODG1', 'OPER_NO', 'TOOL_NAME', 'parametric_name', 'step'])['stats'].unique().reset_index()\n",
    "    feature_stats['stats'] = [feature_stats['stats'].iloc[i].tolist() for i in range(len(feature_stats))]\n",
    "    feature_stats['stats'] = feature_stats['stats'].apply(lambda x: \"#\".join(x))\n",
    "    feature_stats = feature_stats.assign(parametric_name=lambda x: x['parametric_name']+str('#')+x['step']).drop('step', axis=1)\n",
    "    return feature_stats\n",
    "    \n",
    "    \n",
    "def split_calculate_features_big_sample(df, by):\n",
    "    \"\"\"\n",
    "    param df: RandomForest建模后的结果\n",
    "    param by: 分组字段\n",
    "    return: features和importance结果\n",
    "    \"\"\"\n",
    "    schema_all = StructType([\n",
    "        StructField(\"PRODG1\", StringType(), True),\n",
    "        StructField(\"OPER_NO\", StringType(), True),\n",
    "        StructField(\"TOOL_NAME\", StringType(), True),\n",
    "        StructField(\"parametric_name\", StringType(), True),\n",
    "        StructField(\"importance\", FloatType(), True),\n",
    "        StructField(\"stats\", StringType(), True)])\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_result(model_results):\n",
    "        # 先从随机森林的模型结果中取出包含features和importance的dataframe\n",
    "        feature_importance_table = model_results[['features', 'importance']].dropna(axis=0)\n",
    "\n",
    "        # 分裂features\n",
    "        feature_importance_res_split = get_split_feature_importance_table(feature_importance_table, by)\n",
    "\n",
    "        # 去除importance为0的组合\n",
    "        feature_importance_res_split_drop = feature_importance_res_split.query(\"importance > 0\").reset_index(drop=True)\n",
    "\n",
    "        # 取每一种组合结果的前60%或者100%\n",
    "        feature_importance_res_split_nlargest = (feature_importance_res_split_drop.groupby(by=['PRODG1', 'OPER_NO', 'TOOL_NAME'])\n",
    "                                            .apply(lambda x: x.nlargest(int(x.shape[0]*0.6), 'importance') if x.shape[0]>1 else x.nlargest(int(x.shape[0]*1), 'importance'))\n",
    "                                            .reset_index(drop=True))\n",
    "\n",
    "        # 新增一列，含有参数的所有统计特征:feature_stats\n",
    "        feature_stats = add_feature_stats(feature_importance_res_split_drop)\n",
    "\n",
    "        # 对同一种组合里的同一个参数进行求和:feature_importance_groupby\n",
    "        feature_importance_groupby = (feature_importance_res_split_nlargest.groupby(['PRODG1', 'OPER_NO', 'TOOL_NAME',\n",
    "                                                            'parametric_name', 'step'])['importance'].sum().reset_index())\n",
    "        feature_importance_groupby = feature_importance_groupby.assign(parametric_name=lambda x: x['parametric_name'] + str('#') + x['step']).drop('step', axis=1)\n",
    "\n",
    "        # feature_stats和feature_importance_groupby连接\n",
    "        grpby_stats = pd.merge(feature_stats, feature_importance_groupby, on=['PRODG1', 'OPER_NO', 'TOOL_NAME', 'parametric_name']).dropna().reset_index(drop=True)\n",
    "        return grpby_stats\n",
    "    return df.groupby(by).apply(get_result)\n",
    "\n",
    "    \n",
    "\n",
    "def get_finall_results_big_sample(s_res, f_res, bad_wafer_num):\n",
    "    \"\"\"\n",
    "    param s_res: roc_auc分数结果\n",
    "    param f_res: features和importance结果\n",
    "    param bad_wafer_num: 数据中所有bad_wafer的数量\n",
    "    return: 最后的建模结果\n",
    "    \"\"\"\n",
    "    # feature_importance_groupby和sample_res连接\n",
    "    roc_auc_score_all = s_res.agg({\"roc_auc_score\": \"sum\"}).collect()[0][0]\n",
    "    s_res = s_res.withColumn(\"roc_auc_score_ratio\", col(\"roc_auc_score\")/roc_auc_score_all)\n",
    "    s_res = s_res.withColumn(\"bad_ratio\", col(\"bad_wafer\") / bad_wafer_num)\n",
    "\n",
    "    df_merge = s_res.join(f_res, on=['PRODG1', 'OPER_NO', 'TOOL_NAME'], how='left')\n",
    "    df_merge = df_merge.withColumn('weight_original', col('roc_auc_score_ratio') * col('bad_ratio') * col('importance'))\n",
    "\n",
    "    # 最后再次进行一次归一化\n",
    "    weight_all = df_merge.agg({\"weight_original\": \"sum\"}).collect()[0][0]\n",
    "    df_merge = df_merge.withColumn(\"weight\", col(\"weight_original\") / weight_all)\n",
    "\n",
    "    df_merge = df_merge.select(['PRODG1', 'OPER_NO', 'TOOL_NAME',\n",
    "                                'parametric_name', 'weight', 'stats']).orderBy('weight', ascending=False)\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2b5ebad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+---------+---------+-------------+\n",
      "|  PRODG1|          OPER_NO|TOOL_NAME|bad_wafer|roc_auc_score|\n",
      "+--------+-----------------+---------+---------+-------------+\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|      139|   0.95626736|\n",
      "|C90WA15A|2F.CDS10_XX.TDS01|  DSA02_B|       25|          1.0|\n",
      "|C90WA20A|2F.CDS10_XX.TDS01|  DSA02_B|       24|          1.0|\n",
      "+--------+-----------------+---------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_res = split_score_big_sample(df=res, by=['PRODG1', 'OPER_NO', 'TOOL_NAME'])\n",
    "s_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bd976674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+---------+--------------------+------------+--------------------+\n",
      "|  PRODG1|          OPER_NO|TOOL_NAME|     parametric_name|  importance|               stats|\n",
      "+--------+-----------------+---------+--------------------+------------+--------------------+\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|BUFFER_PRESSURE#B...| 0.003122571|                 MAX|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.005672672|                 MIN|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.020104207|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...|  0.01957967|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.026246237|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.026585378|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.030087773|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.011065792|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.006430261|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.012190461|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.020886185|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.017176429|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.023237493|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.023778388|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CUSTOMEQUATION#CU...| 0.056181174|DEPO_PRESSURE_ED_...|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|ECHUCK_CURRENT#DE...|0.0025149235|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|ECHUCK_CURRENT#DE...|0.0026286216|                MEAN|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|ECHUCK_VOLTAGE#DE...|0.0018815284|          RANGE#MEAN|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|ECHUCK_VOLTAGE#DE...|0.0027101098|          RANGE#MEAN|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|ECHUCK_VOLTAGE#DE...|   0.0026632|               RANGE|\n",
      "+--------+-----------------+---------+--------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_res = split_calculate_features_big_sample(df=res, by=grpby_list)\n",
    "f_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dfe31fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+---------+--------------------+--------------------+--------------------+\n",
      "|  PRODG1|          OPER_NO|TOOL_NAME|     parametric_name|              weight|               stats|\n",
      "+--------+-----------------+---------+--------------------+--------------------+--------------------+\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|   WAFER_COUNT#COUNT| 0.08853350503503596|                 MAX|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CUSTOMEQUATION#CU...|0.052482004041712264|DEPO_PRESSURE_ED_...|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.04096358252773476|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|WAFER_TEMPERATURE...| 0.03563017544411217|                MEAN|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...|0.035365053190280035|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...|0.029764044624862276|                MEAN|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|WAFER_TEMPERATURE...|0.025563498532413156|                MEAN|\n",
      "|C90WA15A|2F.CDS10_XX.TDS01|  DSA02_B|WAFER_TEMPERATURE...| 0.02189694223620189|            MEAN#MAX|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...|0.021354190114419604|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.01618710312764767|                MEAN|\n",
      "|C90WA20A|2F.CDS10_XX.TDS01|  DSA02_B|   WAFER_COUNT#COUNT|0.015289394060395128|                 MAX|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|RFREFLECTEDPOWER_...|0.015037008369532647|                MEAN|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...|0.014400622474093823|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|WAFER_TEMPERATURE...|0.013298857744122091|            MAX#MEAN|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...|0.012893169828325992|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...|0.012571390801170286|          MEAN#RANGE|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|RFREFLECTEDPOWER_...|0.010486449587078096|                MEAN|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|RFREFLECTEDPOWER_...|0.010191169359883422|                MEAN|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|WAFER_TEMPERATURE...|0.010155162377590055|                MEAN|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|CHAMBER_PRESSURE#...| 0.00966652399912603|          MEAN#RANGE|\n",
      "+--------+-----------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_res_bs = get_finall_results_big_sample(s_res=s_res, f_res=f_res, bad_wafer_num=bad_wafer_num_big_sample)\n",
    "model_res_bs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a960c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4b45ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#############################将建模后的结果增加特定的列####################################\n",
    "#####################################################################################\n",
    "def add_certain_column(df, by, request_id):\n",
    "    \"\"\"\n",
    "    param df: 最后的建模结果\n",
    "    param by: 分组字段, 手动增加一列add\n",
    "    param request_id: 传入的request_id\n",
    "    return: 最后的建模结果增加特定的列\n",
    "    \"\"\"\n",
    "    schema_all = StructType([\n",
    "        StructField(\"PRODG1\", StringType(), True),\n",
    "        StructField(\"OPER_NO\", StringType(), True),\n",
    "        StructField(\"TOOL_NAME\", StringType(), True),\n",
    "        StructField(\"stats\", StringType(), True),\n",
    "        StructField(\"parametric_name\", StringType(), True),\n",
    "        StructField(\"weight\", FloatType(), True),\n",
    "        StructField(\"request_id\", StringType(), True),\n",
    "        StructField(\"weight_percent\", FloatType(), True),\n",
    "        StructField(\"index_no\", IntegerType(), True)])\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_result(final_res):\n",
    "        final_res['weight'] = final_res['weight'].astype(float)\n",
    "        final_res = final_res.query(\"weight > 0\")\n",
    "        final_res['request_id'] = request_id\n",
    "        final_res['weight_percent'] = final_res['weight'] * 100\n",
    "        final_res = final_res.sort_values('weight', ascending=False)\n",
    "        final_res['index_no'] = [i + 1 for i in range(len(final_res))]\n",
    "        final_res = final_res.drop('add', axis=1)\n",
    "        # final_res['parametric_name'] = final_res['parametric_name'].str.replace(\"_\", \"+\")\n",
    "        final_res['PRODG1'] = final_res['PRODG1'].apply(lambda x: None if x == 'grplen2' else x)\n",
    "        return final_res\n",
    "    return df.groupby(by).apply(get_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "62ba5671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+---------+--------------------+--------------------+-----------+----------+--------------+--------+\n",
      "|  PRODG1|          OPER_NO|TOOL_NAME|               stats|     parametric_name|     weight|request_id|weight_percent|index_no|\n",
      "+--------+-----------------+---------+--------------------+--------------------+-----------+----------+--------------+--------+\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                 MAX|   WAFER_COUNT#COUNT| 0.12011421|       fff|      12.01142|       1|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|            MAX#MEAN|WAFER_TEMPERATURE...|0.030839466|       fff|     3.0839467|       2|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...|0.027421484|       fff|     2.7421484|       3|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...|0.026222682|       fff|     2.6222682|       4|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...|0.025790723|       fff|     2.5790722|       5|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|ECTH_PRESSURE_ED_...|CUSTOMEQUATION#CU...|0.025608715|       fff|     2.5608716|       6|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...|0.021936735|       fff|     2.1936734|       7|\n",
      "|C90WA20A|2F.CDS10_XX.TDS01|  DSA02_B|                 MAX|   WAFER_COUNT#COUNT|0.020335304|       fff|     2.0335305|       8|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|WAFER_TEMPERATURE...| 0.01962808|       fff|     1.9628079|       9|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...|0.017959863|       fff|     1.7959864|      10|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|RFREFLECTEDPOWER_...| 0.01779116|       fff|     1.7791159|      11|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...|0.017774161|       fff|     1.7774161|      12|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|RFREFLECTEDPOWER_...|0.017384999|       fff|     1.7384998|      13|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|WAFER_TEMPERATURE...|  0.0167908|       fff|       1.67908|      14|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...|0.015905723|       fff|     1.5905722|      15|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|RFREFLECTEDPOWER_...|0.013601196|       fff|     1.3601196|      16|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|WAFER_TEMPERATURE...|0.013352302|       fff|     1.3352301|      17|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...|0.012500545|       fff|     1.2500545|      18|\n",
      "|C90WA15A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|RFFORWARDPOWER_BI...|0.012119024|       fff|     1.2119024|      19|\n",
      "|C90WA15A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|RFFORWARDPOWER_BI...|0.010428312|       fff|     1.0428313|      20|\n",
      "+--------+-----------------+---------+--------------------+--------------------+-----------+----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_res_bs = model_res_bs.withColumn('add', lit(0))\n",
    "final_res_add_columns = add_certain_column(df=final_res_bs, by='add', request_id=request_id)\n",
    "final_res_add_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a41f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "56d4e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_big_data_model(df_run, data_dict_list_bs, grpby_list, request_id):\n",
    "\n",
    "    df1 = None\n",
    "    df2 = None\n",
    "    \n",
    "    # 1. 获取用于建模的大样本数据\n",
    "    df_run_bs = get_train_data(df_run, data_dict_list_bs)\n",
    "    if df_run_bs.count() == 0:\n",
    "        msg = '数据库中暂无此类数据!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    # 2. 获取所有bad wafer数量\n",
    "    bad_wafer_num_big_sample = get_all_bad_wafer_num(df_run_bs)\n",
    "    if bad_wafer_num_big_sample < 3:\n",
    "        msg = '数据库中实际BAD_WAFER数量小于3片, 请提供更多的BAD_WAFER数量!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "\n",
    "    # 3. 对挑选出的大样本数据进行建模\n",
    "    res = fit_rf_big_sample(df=df_run_bs, by=grpby_list)\n",
    "    if res.count() == 0:\n",
    "        msg = '算法内部暂时异常!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "\n",
    "    # 4. 将建模结果进行整合\n",
    "    s_res = split_score_big_sample(df=res, by=['PRODG1', 'OPER_NO', 'TOOL_NAME'])\n",
    "    if s_res.count() == 0:\n",
    "        msg = '算法运行评分结果较低, 暂无输出, 建议增加BAD_WAFER数量'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    f_res = split_calculate_features_big_sample(df=res, by=grpby_list)\n",
    "    if f_res.count() == 0:\n",
    "        msg = '算法结果求和暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "\n",
    "    model_res_bs = get_finall_results_big_sample(s_res=s_res, f_res=f_res, bad_wafer_num=bad_wafer_num_big_sample)\n",
    "    if model_res_bs.count() == 0:\n",
    "        msg = '算法结果拼接暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    # 7. 增加特定的列\n",
    "    final_res_bs = model_res_bs.withColumn('add', lit(0))\n",
    "    final_res_add_columns = add_certain_column(df=final_res_bs, by='add', request_id=request_id)\n",
    "    if final_res_add_columns.count() == 0:\n",
    "        msg = '算法结果增加列暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "    else:  \n",
    "        return df1, final_res_add_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dbc5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "#######################################正式调用以上函数#######################################\n",
    "##########################################################################################\n",
    "# request_id = 'sdd'\n",
    "# grpby_list = ['OPER_NO', 'TOOL_NAME']\n",
    "\n",
    "# 1. 解析json 为字典， df1为kafka输入的结果数据，获取到parse_dict, request_id, grpby_list\n",
    "# df2 = df1.toPandas() \n",
    "# parse_dict, request_id, grpby_list = get_some_info(df2)\n",
    "# print(type(parse_dict))\n",
    "# print(grpby_list)\n",
    "\n",
    "# 2. 从kafka 关键字映射都具体数据源中的字段,没有的可以删除\n",
    "# keyword_map_from_json_to_table: dict = {\n",
    "#     \"prodg1\": \"PRODG1\",\n",
    "#     \"waferId\": \"WAFER_ID\",\n",
    "#     \"dateRange\": \"START_TIME\",\n",
    "#     \"productId\": \"PRODUCT_ID\",\n",
    "#     \"operNo\": \"OPER_NO\",\n",
    "#     \"eqp\": \"EQP_NAME\",\n",
    "#     \"tool\": \"TOOL_NAME\",\n",
    "#     \"lot\": \"LOT_ID\",\n",
    "#     \"recipeName\": \"RECIPE_NAME\"}\n",
    "\n",
    "# # 3. 获取查询条件list\n",
    "# select_condition_list = parse_dict\n",
    "\n",
    "# # 4. 指定查询表名, 根据实际情况需要修改\n",
    "# table_name = \"etl.DWD_POC_CASE_FD_UVA_DATA_TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa450022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# spark = (SparkSession.builder\n",
    "#             .master(\"local[*]\")\n",
    "#             .config(\"spark.jars.packages\", \"ai.catboost:catboost-spark_3.3_2.12:1.2\")\n",
    "#             .appName(\"RF\")\n",
    "#             .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf19e79",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "78ca3bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170117, 16)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPER_NO IN ('2F.CDS10', 'XX.TDS01', '1C.CDS10', '1F.CDS10')\n",
    "df_pandas = pd.read_csv(\"DWD_POC_CASE_FD_UVA_DATA_CASE2_PROCESSED_FOUR_OPERNO.csv\")\n",
    "\n",
    "# 以上数据量太大，会产生Connection refused, 选择特定的站点\n",
    "df_pandas = df_pandas[df_pandas['OPER_NO'].isin(['2F.CDS10', 'XX.TDS01'])]\n",
    "df_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a688230e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/local/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "170117"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = ps.from_pandas(df_pandas).to_spark()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7909d07e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+--------+--------------+--------+---------+----------+-----------+--------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "| WAFER_ID|TOOL_ID|RUN_ID|EQP_NAME|    PRODUCT_ID|  PRODG1|TOOL_NAME|    LOT_ID|RECIPE_NAME| OPER_NO|         START_TIME|     parametric_name| CASE_INFO|STATUS|STATISTIC_RESULT|label|\n",
      "+---------+-------+------+--------+--------------+--------+---------+----------+-----------+--------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|RFREFLECTEDPOWER_...|2023-06-11|NORMAL|            13.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|CUSTOMEQUATION#CU...|2023-06-11|NORMAL|        0.005367|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|WAFER_TEMPERATURE...|2023-06-11|NORMAL|   490.822509734|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_NF3_TOP#ZERO...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_SIH4#ZERO_CH...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|HX_WATER_TEMPERAT...|2023-06-11|NORMAL|            75.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|X_LCF_RET#LCF_OUT...|2023-06-11|NORMAL|          5.9E-5|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_SIH4_SETTING...|2023-06-11|NORMAL|          129.54|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_SIH4_TOP#DEP...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_HE_SIDE#DEPO...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FI_FFU_FAN_PRESSU...|2023-06-11|NORMAL|           0.005|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_AR_TOP#ZERO_...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_NF3#ZERO_CHE...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_O2#ZERO_CHEC...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|CHAMBER_PRESSURE#...|2023-06-11|NORMAL|          6.1E-4|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_O2#DEPO_01#MEAN|2023-06-11|NORMAL|           300.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_SIH4_TOP_SET...|2023-06-11|NORMAL|           32.13|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|RFFORWARDPOWER_TO...|2023-06-11|NORMAL|             2.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|FLOW_HE_TOP#DEPO_...|2023-06-11|NORMAL|             0.0|    0|\n",
      "|NAZ926-04|   5986|261619|   DSA02|AEMNVC01N.0A01|C90WA30A|  DSA02_B|NAZ926.000|S8900DX0580|2F.CDS10|2023-06-11 20:57:01|RFFORWARDPOWER_TO...|2023-06-11|NORMAL|          5000.0|    0|\n",
      "+---------+-------+------+--------+--------------+--------+---------+----------+-----------+--------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "269da558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170117\n",
      "170117\n",
      "+--------+-----------------+---------+-----------+--------+-------+\n",
      "|  PRODG1|          OPER_NO|TOOL_NAME|wafer_count|good_num|bad_num|\n",
      "+--------+-----------------+---------+-----------+--------+-------+\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|        186|      47|    139|\n",
      "|C90WA01A|2F.CDS10_XX.TDS01|  DSA02_B|         50|       0|     50|\n",
      "|C90WA15A|2F.CDS10_XX.TDS01|  DSA02_B|         33|       8|     25|\n",
      "|C90WA20A|2F.CDS10_XX.TDS01|  DSA02_B|         49|      25|     24|\n",
      "+--------+-----------------+---------+-----------+--------+-------+\n",
      "\n",
      "data_dict_list_bs: [{'PRODG1': 'C90WA12A', 'OPER_NO': '2F.CDS10_XX.TDS01', 'TOOL_NAME': 'DSA02_B'}, {'PRODG1': 'C90WA15A', 'OPER_NO': '2F.CDS10_XX.TDS01', 'TOOL_NAME': 'DSA02_B'}, {'PRODG1': 'C90WA20A', 'OPER_NO': '2F.CDS10_XX.TDS01', 'TOOL_NAME': 'DSA02_B'}]\n",
      "****************大样本算法调用****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行成功\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [148], line 60\u001b[0m\n\u001b[1;32m     59\u001b[0m     df_kafka \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m运行成功\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestId\u001b[39m\u001b[38;5;124m\"\u001b[39m: request_id}, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 60\u001b[0m     df1 \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mcreateDataFrame(df_kafka)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [148], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     66\u001b[0m     df_kafka \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m主程序发生异常: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestId\u001b[39m\u001b[38;5;124m\"\u001b[39m: request_id}, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 67\u001b[0m     df1 \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mcreateDataFrame(df_kafka)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 主程序\n",
    "try:\n",
    "    # 从数据库中获取数据\n",
    "#     df1 = get_data_from_doris(select_condition_list=select_condition_list, table_name=table_name)\n",
    "#     print(df1.count())\n",
    "#     if df1.count() == 0:\n",
    "#         msg = '解析SQL获取数据异常: 数据库中可能没有数据!'\n",
    "#         df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "#         df1 = spark.createDataFrame(df_kafka)\n",
    "#         raise ValueError\n",
    "\n",
    "    # 1. 站点融合和数据预处理\n",
    "    df1 = integrate_operno(df=df1, merge_operno_list=merge_operno)\n",
    "    print(df1.count())\n",
    "    df_run = _pre_process(df1)\n",
    "    print(df_run.count())\n",
    "    if df_run.count() == 0:\n",
    "        msg = '该条件下数据库中暂无数据，请检查！'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "\n",
    "    # 2. 进行共性分析\n",
    "    common_res = commonality_analysis(df_run, grpby_list)\n",
    "    common_res.show()\n",
    "    if common_res.count() == 0:\n",
    "        msg = '共性分析结果异常!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "\n",
    "    # 3. 挑选出数据：bad和good要同时大于3\n",
    "    data_dict_list_bs = get_data_list(common_res, grpby_list, big_or_small='big')\n",
    "    print(\"data_dict_list_bs:\", data_dict_list_bs)\n",
    "    if len(data_dict_list_bs) != 0:\n",
    "        print(\"****************大样本算法调用****************\")\n",
    "        df1, final_res_add_columns = fit_big_data_model(df_run, data_dict_list_bs, grpby_list, request_id)\n",
    "    else:        \n",
    "        print(\"****************小样本算法调用****************\")\n",
    "        df1, final_res_add_columns = fit_small_data_model(df_run, common_res, grpby_list, request_id)\n",
    "    \n",
    "\n",
    "    if df1 is not None:\n",
    "        raise ValueError\n",
    "    else:\n",
    "        # final_res_add_columns 是最后的结果，要写回数据库\n",
    "        # ddd = final_res_add_columns.toPandas()\n",
    "        # user =\"root\"\n",
    "        # host = \"10.52.199.81\"\n",
    "        # password = \"Nexchip%40123\"\n",
    "        # db = \"etl\"\n",
    "        # port = 9030\n",
    "        # engine = create_engine(\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\".format(user = user,\n",
    "        #                                                                                     password = password,\n",
    "        #                                                                                     host = host,\n",
    "        #                                                                                     port = port,\n",
    "        #                                                                                     db = db))\n",
    "        # doris_stream_load_from_df(ddd, engine, \"results\")\n",
    "\n",
    "        # # 最终成功的话，就会输出下面这条\n",
    "        print(\"运行成功\")\n",
    "        df_kafka = pd.DataFrame({\"code\": 0, \"msg\": \"运行成功\", \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "\n",
    "except ValueError as ve:\n",
    "    pass\n",
    "\n",
    "except Exception as e:\n",
    "    df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f\"主程序发生异常: {str(e)}\", \"requestId\": request_id}, index=[0])\n",
    "    df1 = spark.createDataFrame(df_kafka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9e68ca39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "306bb66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+---------+--------------------+--------------------+------------+----------+--------------+--------+\n",
      "|  PRODG1|          OPER_NO|TOOL_NAME|               stats|     parametric_name|      weight|request_id|weight_percent|index_no|\n",
      "+--------+-----------------+---------+--------------------+--------------------+------------+----------+--------------+--------+\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                 MAX|   WAFER_COUNT#COUNT| 0.072545126|       fff|      7.254513|       1|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...|  0.03640122|       fff|      3.640122|       2|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|DEPO_PRESSURE_ED_...|CUSTOMEQUATION#CU...|  0.03630155|       fff|      3.630155|       3|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...| 0.030790603|       fff|     3.0790603|       4|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|WAFER_TEMPERATURE...| 0.025395073|       fff|     2.5395074|       5|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...| 0.024486447|       fff|     2.4486446|       6|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|RFREFLECTEDPOWER_...| 0.021473566|       fff|     2.1473567|       7|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...| 0.020948172|       fff|     2.0948172|       8|\n",
      "|C90WA20A|2F.CDS10_XX.TDS01|  DSA02_B|                 MAX|   WAFER_COUNT#COUNT| 0.020653633|       fff|     2.0653632|       9|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|RFREFLECTEDPOWER_...| 0.017898861|       fff|     1.7898862|      10|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|RFREFLECTEDPOWER_...| 0.016574346|       fff|     1.6574346|      11|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|OUTER_HE_LEAKRATE...| 0.015437273|       fff|     1.5437274|      12|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|            MEAN#MAX|WAFER_TEMPERATURE...|  0.01434065|       fff|      1.434065|      13|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...| 0.013673699|       fff|     1.3673699|      14|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|WAFER_TEMPERATURE...| 0.013061466|       fff|     1.3061466|      15|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|WAFER_TEMPERATURE...| 0.012420158|       fff|     1.2420158|      16|\n",
      "|C90WA15A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|CHAMBER_PRESSURE#...| 0.012020165|       fff|     1.2020165|      17|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|                MEAN|WAFER_TEMPERATURE...|0.0119614415|       fff|     1.1961441|      18|\n",
      "|C90WA12A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...| 0.011154876|       fff|     1.1154876|      19|\n",
      "|C90WA20A|2F.CDS10_XX.TDS01|  DSA02_B|          MEAN#RANGE|CHAMBER_PRESSURE#...| 0.010614341|       fff|     1.0614341|      20|\n",
      "+--------+-----------------+---------+--------------------+--------------------+------------+----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_res_add_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d308a828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb63e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268564f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0782d987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
