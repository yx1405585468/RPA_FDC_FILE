{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787d435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.dataframe\n",
    "from pca import pca\n",
    "from pyspark.sql.functions import max, countDistinct, when, lit, pandas_udf, PandasUDFType, monotonically_increasing_id, split\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, lit, col, collect_set, explode, countDistinct, when, monotonically_increasing_id, sum as spark_sum\n",
    "from typing import List, Dict\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField, BooleanType\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from src.exceptions.rca_base_exception import RCABaseException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5113af5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[scatterd] >WARNING> 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/python-3.9.13/bin/python3'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/python-3.9.13/bin/python3'\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"pandas_udf\") \\\n",
    "    .config('spark.sql.session.timeZone', 'Asia/Shanghai') \\\n",
    "    .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "    .config('spark.driver.memory', '8g') \\\n",
    "    .config('spark.driver.cores', '12') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .config('spark.executor.cores', '12') \\\n",
    "    .config('spark.cores.max', '12') \\\n",
    "    .config('spark.driver.host', '192.168.22.28') \\\n",
    "    .master(\"spark://192.168.12.47:7077,192.168.12.48:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf80357",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_spark shape: (550911, 16)\n",
      "+---------+-------+------+--------+--------------+--------+---------+----------+--------------------+--------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "| WAFER_ID|TOOL_ID|RUN_ID|EQP_NAME|    PRODUCT_ID|  PRODG1|TOOL_NAME|    LOT_ID|         RECIPE_NAME| OPER_NO|         START_TIME|     parametric_name| CASE_INFO|STATUS|STATISTIC_RESULT|label|\n",
      "+---------+-------+------+--------+--------------+--------+---------+----------+--------------------+--------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_10_CO...|2023-06-16|NORMAL|             0.0|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_3_C4F...|2023-06-16|NORMAL|             0.0|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_14_O2...|2023-06-16|NORMAL|             7.5|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|BOTTOMFLOWRATE#AO...|2023-06-16|NORMAL|          29.993|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_12_CH...|2023-06-16|NORMAL|             0.0|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_5_CHF...|2023-06-16|NORMAL|        123722.7|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|APC_POSITION#AOTU...|2023-06-16|NORMAL|             4.5|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|LO_RF_REF_POWER#A...|2023-06-16|NORMAL|          1.9329|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|EDGE_HE_PRESSURE#...|2023-06-16|NORMAL|            24.9|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|LO_C2_VAR_CAPACIT...|2023-06-16|NORMAL|             5.0|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_9_O2#...|2023-06-16|NORMAL|             0.0|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|LO_RF_VPP#STEP2_M...|2023-06-16|NORMAL|        901.9479|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_16_CH...|2023-06-16|NORMAL|             0.0|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|CHAMBER_PRESSURE#...|2023-06-16|NORMAL|             1.3|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|CENTER_GAS_PRESSU...|2023-06-16|NORMAL|             5.2|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|CENTER_HE_PRESSUR...|2023-06-16|NORMAL|             6.9|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|FLOWSPLITRATIO#AO...|2023-06-16|NORMAL|            50.4|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|LOWERBRINETEMP#AO...|2023-06-16|NORMAL|         59.9724|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_11_N2...|2023-06-16|NORMAL|             0.0|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_4_CH2...|2023-06-16|NORMAL|             0.0|    0|\n",
      "+---------+-------+------+--------+--------------+--------+---------+----------+--------------------+--------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pandas = pd.read_csv(\n",
    "    \"D:/Jupyterfiles/晶合MVAFDC_general开发/MVAanlysisDevelop/uva_algorithm/CASE1_DATA/DWD_POC_CASE_FD_UVA_DATA_CASE1_PROCESSED1.csv\")\n",
    "# df_pandas = pd.read_csv(\n",
    "#     \"D:/Jupyterfiles/晶合MVAFDC_general开发/MVAanlysisDevelop/uva_algorithm/small_samples_data/small2_labeled.csv\")\n",
    "df_spark = ps.from_pandas(df_pandas).to_spark()\n",
    "print(f\"df_spark shape: ({df_spark.count()}, {len(df_spark.columns)})\")\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2904647f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json_loads_dict = {\n",
    "    \"requestId\": \"uva\",\n",
    "    \"requestParam\": {'dateRange': [{'start': \"2023-12-01 00:00:00\", 'end': \"2024-01-15 00:00:00\"}],\n",
    "                     'lot': [],\n",
    "                     'operNo': [],\n",
    "                     'prodg1': [],\n",
    "                     'productId': [],\n",
    "                     'eqp': [],\n",
    "                     'tool': [],\n",
    "                     'recipeName': [],\n",
    "                     'waferId': {'good': [],\n",
    "                                 'bad': []},\n",
    "                     'uploadId': '20240110170016023',\n",
    "                     \"flagMergeAllProdg1\": \"0\",\n",
    "                     \"flagMergeAllProductId\": \"0\",\n",
    "                     \"flagMergeAllChamber\": \"0\",\n",
    "                     \"mergeProdg1\": [],\n",
    "                     # \"mergeProductId\": [{\"xx_cc\": [\"AFGN1501N.0C02\", \"AFKN2J01N.0U01\"]}],\n",
    "                     \"mergeProductId\": [],\n",
    "                     \"mergeEqp\": [],\n",
    "                     \"mergeChamber\": [],\n",
    "                     \"mergeOperno\": [],\n",
    "                     # 'mergeOperno': [{\"2F.CDS10_XX.TDS01\": [\"2F.CDS10\", \"XX.TDS01\"]},\n",
    "                     #                 {\"2F.CDS20_XX.CDS20\": [\"2F.CDS20\", \"XX.CDS20\"]}]\n",
    "                     }\n",
    "}\n",
    "df_ = pd.DataFrame({\"requestId\": [json_loads_dict[\"requestId\"]],\n",
    "                    \"requestParam\": [json.dumps(json_loads_dict[\"requestParam\"])]})\n",
    "\n",
    "request_id_ = df_[\"requestId\"].values[0]\n",
    "request_params = df_[\"requestParam\"].values[0]\n",
    "parse_dict = json.loads(request_params)\n",
    "\n",
    "merge_operno = list(parse_dict.get('mergeOperno')) if parse_dict.get('mergeOperno') else None\n",
    "merge_prodg1 = list(parse_dict.get('mergeProdg1')) if parse_dict.get('mergeProdg1') else None\n",
    "merge_product = list(parse_dict.get('mergeProductId')) if parse_dict.get('mergeProductId') else None\n",
    "merge_eqp = list(parse_dict.get('mergeEqp')) if parse_dict.get('mergeEqp') else None\n",
    "merge_chamber = list(parse_dict.get('mergeChamber')) if parse_dict.get('mergeChamber') else None\n",
    "\n",
    "\n",
    "grpby_list_ = ['OPER_NO', 'TOOL_NAME', 'PRODUCT_ID']\n",
    "# grpby_list = ['OPER_NO', 'TOOL_NAME']\n",
    "# grpby_list_ = ['PRODUCT_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7511b1aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPER_NO   TOOL_NAME  PRODUCT_ID    \n",
       "1F.EEK10  EKT72_PM1  AFGN1501N.0C02    157\n",
       "                     AFGN2T01N.0G01    157\n",
       "                     AFGN4201N.0B01    157\n",
       "                     AFGN5101N.0S01    157\n",
       "                     AFGNK401N.0A01    157\n",
       "                     AFKN2J01N.0U01     60\n",
       "                     AFKN4X01N.0B01     60\n",
       "                     AFKN6201N.0A01     60\n",
       "                     AFKN8401N.0D01     60\n",
       "                     AFKNBM01N.0B01     60\n",
       "                     AFKNJD01N.0A01     60\n",
       "                     AFKNML01N.0C01     60\n",
       "                     AFKNNW01N.0C01     60\n",
       "                     AMKNGW01N.0C01     60\n",
       "                     AMKNS301N.0A01     60\n",
       "                     AMKNS301N.0B01     62\n",
       "                     AMKNSE01N.0B01     62\n",
       "                     AMKNTJ01N.0A01     60\n",
       "                     AMKNWX01N.0B01     60\n",
       "                     AMKNXJ01N.0A01     60\n",
       "                     AMKNXY01N.0A01     60\n",
       "                     AMKNZC01N.0A01     62\n",
       "                     AMKNZD01N.0A01     60\n",
       "          EKT72_PM2  AFGN0301N.0C02    159\n",
       "                     AFGN1501N.0C02    157\n",
       "                     AFGN1B01N.0E01    157\n",
       "                     AFGN1S01N.0B02    157\n",
       "                     AFGN2T01N.0G01    159\n",
       "                     AFGN4201N.0B01    157\n",
       "                     AFGN4B01N.0B02    157\n",
       "                     AFGN6C01N.0C01    157\n",
       "                     AFGN8S01N.0C01    157\n",
       "                     AFGN8S01N.0D01    157\n",
       "                     AFGN8S01N.0F01    157\n",
       "                     AFGNRE01N.0C01    157\n",
       "                     AFKN4X01N.0B01     60\n",
       "                     AFKN9Z01N.0A01     60\n",
       "                     AFKNFV01N.0B01     60\n",
       "                     AFKNML01N.0A01     60\n",
       "                     AFKNML01N.0B01     62\n",
       "Name: parametric_name, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.groupby(grpby_list_)['parametric_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7791da55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4061"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_pandas.groupby(grpby_list_)['parametric_name'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62f8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb6d457d",
   "metadata": {},
   "source": [
    "---------------------  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab112c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessForUvaData:\n",
    "    def __init__(self,\n",
    "                 df: pyspark.sql.dataframe,\n",
    "                 grpby_list: list[str],\n",
    "                 merge_operno_list: List[Dict[str, List[str]]],\n",
    "                 merge_prodg1_list: List[Dict[str, List[str]]],\n",
    "                 merge_product_list: List[Dict[str, List[str]]],\n",
    "                 merge_eqp_list: List[Dict[str, List[str]]],\n",
    "                 merge_chamber_list: List[Dict[str, List[str]]]):\n",
    "        self.df = df\n",
    "        self.grpby_list = grpby_list\n",
    "        self.merge_operno_list = merge_operno_list\n",
    "        self.merge_prodg1_list = merge_prodg1_list\n",
    "        self.merge_product_list = merge_product_list\n",
    "        self.merge_eqp_list = merge_eqp_list\n",
    "        self.merge_chamber_list = merge_chamber_list\n",
    "\n",
    "    @staticmethod\n",
    "    def integrate_columns(df: pyspark.sql.dataframe,\n",
    "                          merge_operno_list: List[Dict[str, List[str]]],\n",
    "                          merge_prodg1_list: List[Dict[str, List[str]]],\n",
    "                          merge_product_list: List[Dict[str, List[str]]],\n",
    "                          merge_eqp_list: List[Dict[str, List[str]]],\n",
    "                          merge_chamber_list: List[Dict[str, List[str]]]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Integrate columns in the DataFrame based on the provided list.\n",
    "\n",
    "        :param df: The input DataFrame.\n",
    "        :param merge_operno_list: A list of dictionaries where each dictionary contains values to be merged.\n",
    "               Example: [{'2F.CDS10_XX.TDS01': ['2F.CDS10', 'XX.TDS01']},\n",
    "                         {'2F.CDS20_XX.CDS20': ['2F.CDS20', 'XX.CDS20']}]\n",
    "        :param merge_prodg1_list: A list of dictionaries for merging 'PRODG1' column in a similar fashion.\n",
    "        :param merge_product_list: A list of dictionaries for merging 'PRODUCT_ID' column in a similar fashion.\n",
    "        :param merge_eqp_list: A list of dictionaries for merging 'EQP_NAME' column in a similar fashion.\n",
    "        :param merge_chamber_list: A list of dictionaries for merging 'TOOL_NAME' column in a similar fashion.\n",
    "\n",
    "        :return: DataFrame with 'OPER_NO' and other specified columns integrated according to the merge rules.\n",
    "        \"\"\"\n",
    "        df_merged = PreprocessForUvaData.integrate_single_column(df, merge_operno_list, 'OPER_NO')\n",
    "        df_merged = PreprocessForUvaData.integrate_single_column(df_merged, merge_prodg1_list, 'PRODG1')\n",
    "        df_merged = PreprocessForUvaData.integrate_single_column(df_merged, merge_product_list, 'PRODUCT_ID')\n",
    "        df_merged = PreprocessForUvaData.integrate_single_column(df_merged, merge_eqp_list, 'EQP_NAME')\n",
    "        df_merged = PreprocessForUvaData.integrate_single_column(df_merged, merge_chamber_list, 'TOOL_NAME')\n",
    "        return df_merged\n",
    "\n",
    "    @staticmethod\n",
    "    def integrate_single_column(df: pyspark.sql.dataframe,\n",
    "                                merge_list: List[Dict[str, List[str]]],\n",
    "                                column_name: str) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Integrate columns in the DataFrame based on the provided list.\n",
    "\n",
    "        :param df: The input DataFrame.\n",
    "        :param merge_list: A list of dictionaries where each dictionary contains values to be merged.\n",
    "        :param column_name: The name of the column to be merged.\n",
    "\n",
    "        :return: DataFrame with specified column integrated according to the merge rules.\n",
    "        \"\"\"\n",
    "        splitter_comma = \",\"\n",
    "        if merge_list is not None and len(merge_list) > 0:\n",
    "            # Extract values from each dictionary in merge_operno_list and create a list\n",
    "            values_to_replace = [list(rule.values())[0] for rule in merge_list]\n",
    "            # Concatenate values from each dictionary\n",
    "            merged_values = [splitter_comma.join(list(rule.values())[0]) for rule in merge_list]\n",
    "\n",
    "            # Replace values in 'OPER_NO' column based on the rules defined in merge_operno_list\n",
    "            for values, replacement_value in zip(values_to_replace, merged_values):\n",
    "                df = df.withColumn(column_name,\n",
    "                                   when(col(column_name).isin(values), replacement_value).otherwise(col(column_name)))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def pre_process(df: pyspark.sql.dataframe) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Preprocess the data extracted from the database for a specific CASE.\n",
    "        :param df: Data for a specific CASE retrieved from the database.\n",
    "        :return: Preprocessed data with relevant columns and filters applied.\n",
    "        \"\"\"\n",
    "        # Select only the columns that will be used\n",
    "        df = df.select('WAFER_ID', 'TOOL_ID', 'RUN_ID', 'EQP_NAME', 'PRODUCT_ID', 'PRODG1', 'TOOL_NAME',\n",
    "                       'OPER_NO', 'parametric_name', 'STATISTIC_RESULT', 'label')\n",
    "        # Remove rows with missing values in 'STATISTIC_RESULT' column\n",
    "        df = df.filter(col('STATISTIC_RESULT').isNotNull())\n",
    "        # Drop duplicates based on all columns\n",
    "        df1 = df.dropDuplicates()\n",
    "        # Select the rows with the latest 'RUN_ID' for each combination of 'WAFER_ID', 'OPER_NO', 'TOOL_ID'\n",
    "        df2 = df1.groupBy('WAFER_ID', 'OPER_NO', 'TOOL_ID').agg(max('RUN_ID').alias('RUN_ID'))\n",
    "        df_run = df1.join(df2.dropDuplicates(subset=['WAFER_ID', 'OPER_NO', 'TOOL_ID', 'RUN_ID']),\n",
    "                          on=['WAFER_ID', 'OPER_NO', 'TOOL_ID', 'RUN_ID'], how='inner')\n",
    "        return df_run\n",
    "\n",
    "    @staticmethod\n",
    "    def commonality_analysis(df_run: pyspark.sql.dataframe, grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Perform commonality analysis on preprocessed data.\n",
    "        :param df_run: Preprocessed data after data preprocessing.\n",
    "        :param grpby_list: List of columns ['PRODG1', 'EQP_NAME', 'OPER_NO', 'PRODUCT_ID', 'TOOL_NAME'] for grouping.\n",
    "                Example: grpby_list = ['PRODG1', 'TOOL_NAME', 'OPER_NO'], grpby_list = ['PRODUCT_ID', 'OPER_NO']\n",
    "        :return: Results of commonality analysis, showing the top ten combinations with the highest number of bad wafers.\n",
    "        \"\"\"\n",
    "        common_res = (df_run.groupBy(grpby_list)\n",
    "                      .agg(countDistinct('WAFER_ID', when(df_run['label'] == 0, 1)).alias('good_num'),\n",
    "                           countDistinct('WAFER_ID', when(df_run['label'] == 1, 1)).alias('bad_num'))\n",
    "                      .na.fill(0))\n",
    "        return common_res\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_list(common_res: pyspark.sql.dataframe,\n",
    "                      grpby_list: List[str]) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Get a list of dictionaries for corresponding groups based on commonality analysis.\n",
    "\n",
    "        :param common_res: Result of commonality analysis.\n",
    "        :param grpby_list:  List of columns ['PRODG1', 'EQP_NAME', 'OPER_NO', 'PRODUCT_ID', 'TOOL_NAME'] for grouping.\n",
    "        :return: List of dictionaries for corresponding groups.\n",
    "                Example: [{'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFKN2J01N.0U01'},\n",
    "                          {'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFKN4X01N.0B01'},\n",
    "                          {'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFGN1501N.0C02'}]\n",
    "        \"\"\"\n",
    "        data_list = common_res.select(grpby_list).collect()\n",
    "        data_dict_list = [row.asDict() for row in data_list]\n",
    "        return data_dict_list\n",
    "\n",
    "    @staticmethod\n",
    "    def get_train_data(df_run: pyspark.sql.dataframe, data_dict_list: List[Dict[str, str]]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Get the actual combination data for modeling from the original data.\n",
    "\n",
    "        :param df_run: Preprocessed data after data preprocessing.\n",
    "        :param data_dict_list: List of dictionaries with filtering conditions.\n",
    "               Example: [{'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFKN2J01N.0U01'},\n",
    "                          {'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFKN4X01N.0B01'},\n",
    "                          {'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFGN1501N.0C02'}]\n",
    "        :return: Filtered data for modeling.\n",
    "        \"\"\"\n",
    "        first_data_dict = data_dict_list[0]\n",
    "        conditions = \" AND \".join([\"{} == '{}'\".format(col_, first_data_dict[col_]) for col_ in first_data_dict])\n",
    "        df_s = df_run.filter(conditions)\n",
    "\n",
    "        for i in range(1, len(data_dict_list)):\n",
    "            data_dict = data_dict_list[i]\n",
    "            conditions = \" AND \".join([\"{} == '{}'\".format(col_, data_dict[col_]) for col_ in data_dict])\n",
    "            df_m = df_run.filter(conditions)\n",
    "            df_s = df_s.union(df_m)\n",
    "        return df_s\n",
    "\n",
    "    @staticmethod\n",
    "    def get_all_bad_wafer_num(df: pyspark.sql.dataframe) -> int:\n",
    "        \"\"\"\n",
    "        Get the number of distinct bad WAFER in the DataFrame.\n",
    "        \"\"\"\n",
    "        return df.filter(\"label == 1\").select('WAFER_ID').distinct().count()\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_unique_params_within_groups(df: pyspark.sql.dataframe, grpby_list) -> pyspark.sql.dataframe:\n",
    "        grouped = df.groupby(*grpby_list).agg(collect_set('parametric_name').alias('unique_values'))\n",
    "        exploded = grouped.select(*grpby_list, explode(col('unique_values')).alias('parametric_name'))\n",
    "        unique_params_within_groups = exploded.dropDuplicates()\n",
    "        return unique_params_within_groups\n",
    "\n",
    "    @staticmethod\n",
    "    def add_feature_stats_within_groups(df: pyspark.sql.dataframe, grpby_list) -> pyspark.sql.dataframe:\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"parametric_name\", StringType(), True),\n",
    "                              StructField(\"stats\", StringType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_result(df) -> pd.DataFrame:\n",
    "            grpby_list_extend = grpby_list + ['parametric_name', 'step']\n",
    "            feature_stats = df.groupby(grpby_list_extend)['stats'].unique().reset_index()\n",
    "            feature_stats['stats'] = [feature_stats['stats'].iloc[i].tolist() for i in range(len(feature_stats))]\n",
    "            feature_stats['stats'] = feature_stats['stats'].apply(lambda x: \"#\".join(x))\n",
    "            feature_stats = feature_stats.assign(\n",
    "                parametric_name=lambda x: x['parametric_name'] + str('#') + x['step']).drop(\n",
    "                'step', axis=1)\n",
    "            return feature_stats\n",
    "        return df.groupby(grpby_list).apply(get_result)\n",
    "\n",
    "    def run(self):\n",
    "        df_integrate_columns = self.integrate_columns(df=self.df,\n",
    "                                                      merge_operno_list=self.merge_operno_list,\n",
    "                                                      merge_prodg1_list=self.merge_prodg1_list,\n",
    "                                                      merge_product_list=self.merge_product_list,\n",
    "                                                      merge_eqp_list=self.merge_eqp_list,\n",
    "                                                      merge_chamber_list=self.merge_chamber_list)\n",
    "        # 按照grpby_list分组，获取原始数据中的所有parametric_name的唯一值\n",
    "        unique_params_within_groups = self.extract_unique_params_within_groups(df=df_integrate_columns,\n",
    "                                                                               grpby_list=self.grpby_list)\n",
    "        unique_params_within_groups = (unique_params_within_groups.withColumn('step', split(col('parametric_name'), '#').getItem(1))\n",
    "                                        .withColumn('stats', split(col('parametric_name'), '#').getItem(2))\n",
    "                                        .withColumn('parametric_name', split(col('parametric_name'), '#').getItem(0)))\n",
    "        # 按照grpby_list分组，获取parametric_name的所有stats值\n",
    "        add_parametric_stats_df = self.add_feature_stats_within_groups(df=unique_params_within_groups, grpby_list=self.grpby_list)\n",
    "\n",
    "        # 数据预处理和共性分析\n",
    "        df_run = self.pre_process(df_integrate_columns)\n",
    "        common_res = self.commonality_analysis(df_run=df_run, grpby_list=self.grpby_list)\n",
    "        common_res = common_res.withColumn(\"conditions_satisfied\", when((col('good_num') >= 1) & (col('bad_num') >= 1), True).otherwise(False))\n",
    "\n",
    "        grps_large = common_res.filter(\"good_num > 50 AND bad_num > 50\")\n",
    "        print(\"grps_large:\")\n",
    "        grps_large.show(50)\n",
    "        if grps_large.isEmpty():\n",
    "            grps_less = common_res.filter(\"good_num >= 1 AND bad_num >= 1\")\n",
    "            print(\"grps_less:\")\n",
    "            grps_less.show(50)\n",
    "            if grps_less.isEmpty():\n",
    "                msg = f\"按照{'+'.join(self.grpby_list)}分组后的数据, 没有组合满足条件good >= 1且bad >= 1, 无法进行分析.\"\n",
    "                raise RCABaseException(msg)\n",
    "            else:\n",
    "                data_dict_list = self.get_data_list(common_res=grps_less, grpby_list=self.grpby_list)\n",
    "                train_data = self.get_train_data(df_run=df_run, data_dict_list=data_dict_list)\n",
    "                big_or_small = 'small'\n",
    "                bad_wafer_num = self.get_all_bad_wafer_num(train_data)\n",
    "                return common_res, train_data, bad_wafer_num, big_or_small, add_parametric_stats_df\n",
    "\n",
    "        else:\n",
    "            data_dict_list = self.get_data_list(common_res=grps_large, grpby_list=self.grpby_list)\n",
    "            train_data = self.get_train_data(df_run=df_run, data_dict_list=data_dict_list)\n",
    "            big_or_small = 'big'\n",
    "            bad_wafer_num = self.get_all_bad_wafer_num(train_data)\n",
    "            add_parametric_name_useless = None\n",
    "        return common_res, train_data, bad_wafer_num, big_or_small, add_parametric_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a546bd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grps_large:\n",
      "+--------+---------+--------------+--------+-------+--------------------+\n",
      "| OPER_NO|TOOL_NAME|    PRODUCT_ID|good_num|bad_num|conditions_satisfied|\n",
      "+--------+---------+--------------+--------+-------+--------------------+\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|     234|    725|                true|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|      75|     75|                true|\n",
      "+--------+---------+--------------+--------+-------+--------------------+\n",
      "\n",
      "common_res: 40\n",
      "+--------+---------+--------------+--------+-------+--------------------+\n",
      "| OPER_NO|TOOL_NAME|    PRODUCT_ID|good_num|bad_num|conditions_satisfied|\n",
      "+--------+---------+--------------+--------+-------+--------------------+\n",
      "|1F.EEK10|EKT72_PM1|AFKN8401N.0D01|       0|      1|               false|\n",
      "|1F.EEK10|EKT72_PM2|AFKNML01N.0B01|     250|      0|               false|\n",
      "|1F.EEK10|EKT72_PM1|AFGN5101N.0S01|       0|    101|               false|\n",
      "|1F.EEK10|EKT72_PM2|AFGN6C01N.0C01|     125|      0|               false|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|     234|    725|                true|\n",
      "|1F.EEK10|EKT72_PM1|AFKN4X01N.0B01|       0|    299|               false|\n",
      "|1F.EEK10|EKT72_PM1|AMKNXY01N.0A01|       1|     74|                true|\n",
      "|1F.EEK10|EKT72_PM2|AFGN1S01N.0B02|     525|      0|               false|\n",
      "|1F.EEK10|EKT72_PM2|AFGN1501N.0C02|      25|      0|               false|\n",
      "|1F.EEK10|EKT72_PM2|AFKNFV01N.0B01|      50|      0|               false|\n",
      "|1F.EEK10|EKT72_PM1|AFGN2T01N.0G01|       0|      1|               false|\n",
      "|1F.EEK10|EKT72_PM2|AFKN4X01N.0B01|     175|      0|               false|\n",
      "|1F.EEK10|EKT72_PM1|AMKNXJ01N.0A01|       0|      3|               false|\n",
      "|1F.EEK10|EKT72_PM1|AFGN4201N.0B01|       0|     75|               false|\n",
      "|1F.EEK10|EKT72_PM2|AFGN8S01N.0D01|     151|      0|               false|\n",
      "|1F.EEK10|EKT72_PM1|AMKNTJ01N.0A01|      75|      0|               false|\n",
      "|1F.EEK10|EKT72_PM1|AMKNS301N.0A01|       0|     25|               false|\n",
      "|1F.EEK10|EKT72_PM2|AFKNML01N.0A01|       1|      0|               false|\n",
      "|1F.EEK10|EKT72_PM1|AMKNWX01N.0B01|      25|      0|               false|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|      75|     75|                true|\n",
      "+--------+---------+--------------+--------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "train_data: 81090\n",
      "bad_wafer_num: 800\n",
      "big_or_small: big\n",
      "add_parametric_stats_df: 3691\n",
      "+--------+---------+--------------+--------------------+----------+\n",
      "| OPER_NO|TOOL_NAME|    PRODUCT_ID|     parametric_name|     stats|\n",
      "+--------+---------+--------------+--------------------+----------+\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|APC_POSITION#AOTU...|MEAN#RANGE|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|APC_POSITION#AOTU...|RANGE#MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|APC_POSITION#AOTU...|MEAN#RANGE|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|BOTTOMFLOWRATE#AO...|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|BOTTOMFLOWRATE#AO...|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|BOTTOMFLOWRATE#AO...|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|CENTER_GAS_PRESSU...|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|CENTER_GAS_PRESSU...|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|CENTER_GAS_PRESSU...|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|CENTER_HE_PRESSUR...|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|CENTER_HE_PRESSUR...|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|CENTER_HE_PRESSUR...|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|CHAMBER_PRESSURE#...|MEAN#RANGE|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|CHAMBER_PRESSURE#...|MEAN#RANGE|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|CHAMBER_PRESSURE#...|RANGE#MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|EDGE_GAS_PRESSURE...|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|EDGE_GAS_PRESSURE...|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|EDGE_GAS_PRESSURE...|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|EDGE_HE_FLOW#AOTU...|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|EDGE_HE_FLOW#AOTU...|      MEAN|\n",
      "+--------+---------+--------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "common_res, train_data, bad_wafer_num, big_or_small, add_parametric_stats_df = PreprocessForUvaData(df=df_spark,\n",
    "                                                                                                grpby_list=grpby_list_,\n",
    "                                                                                                merge_operno_list=merge_operno,\n",
    "                                                                                                merge_prodg1_list=merge_prodg1,\n",
    "                                                                                                merge_product_list=merge_product,\n",
    "                                                                                                merge_eqp_list=merge_eqp,\n",
    "                                                                                                merge_chamber_list=merge_chamber).run()\n",
    "print(\"common_res:\", common_res.count())\n",
    "common_res.show()\n",
    "\n",
    "print(\"train_data:\", train_data.count())\n",
    "print(\"bad_wafer_num:\", bad_wafer_num)\n",
    "print(\"big_or_small:\", big_or_small)\n",
    "\n",
    "print(\"add_parametric_stats_df:\", add_parametric_stats_df.count())\n",
    "add_parametric_stats_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ab9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92e336c2",
   "metadata": {},
   "source": [
    "------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a6517",
   "metadata": {},
   "source": [
    "#### 查看表格透视是否会按列删除数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pandas = train_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1766616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_cols = ['WAFER_ID', 'label']\n",
    "columns_cols = grpby_list + ['parametric_name']\n",
    "df_pivot = train_data_pandas.dropna(axis=0).pivot_table(index=index_cols,\n",
    "                                         columns=columns_cols,\n",
    "                                         values=['STATISTIC_RESULT'])\n",
    "df_pivot.columns = df_pivot.columns.map('#'.join)\n",
    "print(df_pivot.shape)\n",
    "\n",
    "df_pivot = df_pivot.fillna(df_pivot.mean()).reset_index(drop=False)\n",
    "print(df_pivot.shape)\n",
    "\n",
    "# Remove completely identical columns\n",
    "for column in df_pivot.columns.difference(index_cols):\n",
    "    if df_pivot[column].nunique() == 1:\n",
    "        df_pivot = df_pivot.drop(column, axis=1)\n",
    "print(df_pivot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a94473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a846e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de9fef70",
   "metadata": {},
   "source": [
    "------------------   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91e1ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitModelForUvaData:\n",
    "    @staticmethod\n",
    "    def get_pivot_table(df: pd.DataFrame, grpby_list: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Pivot the DataFrame based on specified grouping columns.\n",
    "\n",
    "        Parameters:\n",
    "        - df: Data for modeling.\n",
    "        - grpby_list: List of grouping columns.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Result of pivoting the table.\n",
    "        \"\"\"\n",
    "        index_cols = ['WAFER_ID', 'label']\n",
    "        columns_cols = grpby_list + ['parametric_name']\n",
    "        df_pivot = df.dropna(axis=0).pivot_table(index=index_cols,\n",
    "                                                 columns=columns_cols,\n",
    "                                                 values=['STATISTIC_RESULT'])\n",
    "        df_pivot.columns = df_pivot.columns.map('#'.join)\n",
    "        df_pivot = df_pivot.fillna(df_pivot.mean()).reset_index(drop=False)\n",
    "        # Remove completely identical columns\n",
    "        for column in df_pivot.columns.difference(index_cols):\n",
    "            if df_pivot[column].nunique() == 1:\n",
    "                df_pivot = df_pivot.drop(column, axis=1)\n",
    "        return df_pivot\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pipe_params(model):\n",
    "        common_steps = [\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value=-999)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]\n",
    "        models = {\n",
    "            'rf': (RandomForestClassifier(random_state=2024), {\n",
    "                'model__n_estimators': [*range(10, 60, 10)],\n",
    "                'model__max_depth': [*range(5, 50, 10)],\n",
    "                'model__min_samples_split': [2, 5],\n",
    "                'model__min_samples_leaf': [1, 3]\n",
    "            }),\n",
    "\n",
    "            'decisionTree': (DecisionTreeClassifier(random_state=2024), {\n",
    "                'model__max_depth': [None, 5, 10, 15],\n",
    "                'model__min_samples_split': [2, 5, 10],\n",
    "                'model__min_samples_leaf': [1, 2, 4]\n",
    "            }),\n",
    "\n",
    "            'svc': (LinearSVC(random_state=2024, fit_intercept=False), {\n",
    "                'model__loss': ['hinge', 'squared_hinge'],\n",
    "                'model__C': [0.1, 0.5, 1, 10, 50]\n",
    "            }),\n",
    "\n",
    "            'logistic': (LogisticRegression(random_state=2024, fit_intercept=False, solver='liblinear'), {\n",
    "                'model__penalty': ['l1', 'l2'],\n",
    "                'model__C': [0.1, 0.5, 1, 10, 50]\n",
    "            }),\n",
    "\n",
    "            'sgd': (SGDClassifier(random_state=2024, fit_intercept=False), {\n",
    "                'model__loss': ['hinge', 'log_loss', 'perceptron', 'huber'],\n",
    "                'model__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "                'model__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "                'model__max_iter': [100, 500, 1000]\n",
    "            })\n",
    "        }\n",
    "\n",
    "        if model in models:\n",
    "            model_class, param_grid = models[model]\n",
    "            steps = common_steps + [('model', model_class)]\n",
    "            pipe = Pipeline(steps)\n",
    "            return pipe, param_grid\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    @staticmethod\n",
    "    def construct_features_when_not_satisfied(df_run, df_pivot, x_train, grpby_list, model_condition):\n",
    "        x_len = len(x_train.columns)\n",
    "\n",
    "        if model_condition == 'classification':\n",
    "            \n",
    "            small_importance_res = pd.DataFrame({\"features\": x_train.columns, \n",
    "                                                 \"importance\": [0.0] * x_len})\n",
    "            \n",
    "            sample_res_dict = {'bad_wafer': sum(df_pivot['label']), \n",
    "                               'roc_auc_score': 0.0, \n",
    "                               'algorithm_satisfied': 'FALSE', \n",
    "                               'x_train_shape': str(x_train.shape)} \n",
    "            sample_res_dict.update({col_: df_run[col_].values[0] for col_ in grpby_list})\n",
    "            small_sample_res = pd.DataFrame(sample_res_dict, index=[0])\n",
    "            res_top_select = pd.concat([small_importance_res, small_sample_res])\n",
    "            return res_top_select\n",
    "        \n",
    "        elif model_condition == 'pca':\n",
    "    \n",
    "            res_top_select = pd.DataFrame({\"features\": x_train.columns,\n",
    "                                           \"importance\":[0.0] * x_len,\n",
    "                                           \"bad_wafer\": sum(df_pivot['label']),\n",
    "                                           \"algorithm_satisfied\": ['FALSE'] * x_len,\n",
    "                                           \"x_train_shape\": [str(x_train.shape)] * x_len})\n",
    "            for col_ in grpby_list:\n",
    "                res_top_select[col_] = df_run[col_].values[0]\n",
    "            return res_top_select\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_classification_model(df: pyspark.sql.dataframe, grpby_list: List[str], model) -> pyspark.sql.dataframe:\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"bad_wafer\", IntegerType(), True),\n",
    "                              StructField(\"roc_auc_score\", FloatType(), True),\n",
    "                              StructField(\"features\", StringType(), True),\n",
    "                              StructField(\"importance\", FloatType(), True),\n",
    "                              StructField(\"algorithm_satisfied\", StringType(), True),\n",
    "                              StructField(\"x_train_shape\", StringType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_model_result(df_run: pd.DataFrame) -> pd.DataFrame:\n",
    "            # Pivot the table\n",
    "            df_pivot = FitModelForUvaData.get_pivot_table(df=df_run, grpby_list=grpby_list)\n",
    "\n",
    "            # Define independent and dependent variables\n",
    "            x_train = df_pivot[df_pivot.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "            y_train = df_pivot[['label']]\n",
    "\n",
    "            if x_train.shape[1] > 1 and y_train['label'].nunique() > 1:\n",
    "                z_ratio = y_train.value_counts(normalize=True)\n",
    "                good_ratio = z_ratio[0]\n",
    "                bad_ratio = z_ratio[1]\n",
    "                if abs(good_ratio - bad_ratio) > 0.7:\n",
    "                    undersampler = ClusterCentroids(random_state=1024)\n",
    "                    x_train, y_train = undersampler.fit_resample(x_train, y_train)\n",
    "\n",
    "                pipe, param_grid = FitModelForUvaData.get_pipe_params(model=model)\n",
    "                try:\n",
    "                    grid = GridSearchCV(estimator=pipe, scoring='roc_auc', param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "                    grid.fit(x_train.values, y_train.values.ravel())\n",
    "                except ValueError:\n",
    "                    return pd.DataFrame()\n",
    "\n",
    "                best_est = grid.best_estimator_.steps[-1][-1]\n",
    "                if hasattr(best_est, 'feature_importances_'):\n",
    "                    small_importance_res = pd.DataFrame(\n",
    "                        {'features': x_train.columns, 'importance': best_est.feature_importances_})\n",
    "                else:\n",
    "                    small_importance_res = pd.DataFrame(\n",
    "                        {'features': x_train.columns, 'importance': abs(best_est.coef_.ravel())})\n",
    "\n",
    "                sample_res_dict = {'bad_wafer': sum(df_pivot['label']), \n",
    "                                   'roc_auc_score': grid.best_score_,\n",
    "                                   'algorithm_satisfied': 'TRUE', \n",
    "                                   'x_train_shape': str(x_train.shape)}\n",
    "                sample_res_dict.update({col_: df_run[col_].values[0] for col_ in grpby_list})\n",
    "                small_sample_res = pd.DataFrame(sample_res_dict, index=[0])\n",
    "                res_top_select = pd.concat([small_importance_res, small_sample_res])\n",
    "                return res_top_select\n",
    "            else:\n",
    "                res_top_select = FitModelForUvaData.construct_features_when_not_satisfied(df_run, df_pivot, x_train, grpby_list, 'classification')\n",
    "                return res_top_select\n",
    "\n",
    "        return df.groupby(grpby_list).apply(get_model_result)\n",
    "\n",
    "    @staticmethod\n",
    "    def construct_features_when_satisfy_pca(df_run, df_pivot, x_train, grpby_list) -> pd.DataFrame:\n",
    "        # 得到PCA算法结果res_top_select\n",
    "        n_components = min(min(x_train.shape) - 2, 20)\n",
    "        model = pca(n_components=n_components, verbose=None)\n",
    "        results = model.fit_transform(x_train)\n",
    "        res_top = results['topfeat']\n",
    "        res_top_select = res_top[res_top['type'] == 'best'][['feature', 'loading']]\n",
    "        res_top_select['importance'] = abs(res_top_select['loading'])\n",
    "        res_top_select = res_top_select.rename(columns={'feature': 'features'}).drop(\"loading\", axis=1).drop_duplicates()\n",
    "        \n",
    "        # 增加算法为0的features\n",
    "        has_importance_features = res_top_select['features'].values\n",
    "        zero_importance_features = x_train.columns.difference(has_importance_features).to_list()\n",
    "        len_f = len(zero_importance_features)\n",
    "        zero_df = pd.DataFrame({'features': zero_importance_features, 'importance': [0.0] * len_f})\n",
    "        res_top_select = res_top_select.append(zero_df, ignore_index=True)\n",
    "        \n",
    "        # 合并二者的结果\n",
    "        res_top_select['bad_wafer'] = sum(df_pivot['label'])\n",
    "        for col_ in grpby_list:\n",
    "            res_top_select[col_] = df_run[col_].values[0]\n",
    "        res_top_select['x_train_shape'] = str(x_train.shape)\n",
    "        res_top_select['algorithm_satisfied'] = 'TRUE'\n",
    "        return res_top_select\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_pca_model(df: pyspark.sql.dataframe, grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "        # Dynamically build schema according to the grpby_list\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"features\", StringType(), True),\n",
    "                              StructField(\"importance\", FloatType(), True),\n",
    "                              StructField(\"bad_wafer\", IntegerType(), True),\n",
    "                              StructField(\"algorithm_satisfied\", StringType(), True),\n",
    "                              StructField(\"x_train_shape\", StringType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_model_result(df_run: pd.DataFrame) -> pd.DataFrame:\n",
    "            df_pivot = FitModelForUvaData.get_pivot_table(df=df_run, grpby_list=grpby_list)\n",
    "            df_pivot_copy = df_pivot.copy()\n",
    "            df_pivot_all = pd.concat([df_pivot, df_pivot_copy], axis=0)\n",
    "\n",
    "            x_train = df_pivot_all[df_pivot_all.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "\n",
    "            if min(x_train.shape) > 50:\n",
    "                res_top_select = FitModelForUvaData.construct_features_when_satisfy_pca(df_run, df_pivot, x_train, grpby_list)\n",
    "                return res_top_select\n",
    "            else:\n",
    "                res_top_select = FitModelForUvaData.construct_features_when_not_satisfied(df_run, df_pivot, x_train, grpby_list, 'pca')\n",
    "                return res_top_select\n",
    "\n",
    "        return df.groupby(grpby_list).apply(get_model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb18aa52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************Call Big Sample Algorithm****************\n",
      "+-------+---------+----------+---------+-------------+--------------------+------------+-------------------+-------------+\n",
      "|OPER_NO|TOOL_NAME|PRODUCT_ID|bad_wafer|roc_auc_score|            features|  importance|algorithm_satisfied|x_train_shape|\n",
      "+-------+---------+----------+---------+-------------+--------------------+------------+-------------------+-------------+\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|0.0010027731|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|         0.0|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|         0.0|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|         0.0|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|         0.0|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|         0.0|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|         0.0|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|         0.0|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|         0.0|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|         0.0|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|         0.0|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|  0.00329016|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|4.2908327E-4|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|  0.00135003|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|0.0017186456|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|         0.0|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|  0.00183466|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|         0.0|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|         0.0|               null|         null|\n",
      "|   null|     null|      null|     null|         null|STATISTIC_RESULT#...|0.0010968434|               null|         null|\n",
      "+-------+---------+----------+---------+-------------+--------------------+------------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if big_or_small == 'big':\n",
    "    print(\"****************Call Big Sample Algorithm****************\")\n",
    "    result = FitModelForUvaData.fit_classification_model(df=train_data, grpby_list=grpby_list_, model='rf')\n",
    "    result.show()\n",
    "    result_pandas = result.toPandas()\n",
    "\n",
    "else:\n",
    "    print(\"****************Call Small Sample Algorithm****************\")\n",
    "    result = FitModelForUvaData.fit_pca_model(df=train_data, grpby_list=grpby_list_)\n",
    "    result.show()\n",
    "    result_pandas = result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b967cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OPER_NO</th>\n",
       "      <th>TOOL_NAME</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>bad_wafer</th>\n",
       "      <th>roc_auc_score</th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "      <th>algorithm_satisfied</th>\n",
       "      <th>x_train_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFGN1501N....</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFGN1501N....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFGN1501N....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFGN1501N....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFGN1501N....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFKN2J01N....</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFKN2J01N....</td>\n",
       "      <td>0.066542</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFKN2J01N....</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFKN2J01N....</td>\n",
       "      <td>0.010159</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>EKT72_PM1</td>\n",
       "      <td>AFKN2J01N.0U01</td>\n",
       "      <td>725.0</td>\n",
       "      <td>0.868782</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>(959, 31)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OPER_NO  TOOL_NAME      PRODUCT_ID  bad_wafer  roc_auc_score  \\\n",
       "0        None       None            None        NaN            NaN   \n",
       "1        None       None            None        NaN            NaN   \n",
       "2        None       None            None        NaN            NaN   \n",
       "3        None       None            None        NaN            NaN   \n",
       "4        None       None            None        NaN            NaN   \n",
       "..        ...        ...             ...        ...            ...   \n",
       "111      None       None            None        NaN            NaN   \n",
       "112      None       None            None        NaN            NaN   \n",
       "113      None       None            None        NaN            NaN   \n",
       "114      None       None            None        NaN            NaN   \n",
       "115  1F.EEK10  EKT72_PM1  AFKN2J01N.0U01      725.0       0.868782   \n",
       "\n",
       "                                              features  importance  \\\n",
       "0    STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFGN1501N....    0.001003   \n",
       "1    STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFGN1501N....    0.000000   \n",
       "2    STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFGN1501N....    0.000000   \n",
       "3    STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFGN1501N....    0.000000   \n",
       "4    STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFGN1501N....    0.000000   \n",
       "..                                                 ...         ...   \n",
       "111  STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFKN2J01N....    0.004563   \n",
       "112  STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFKN2J01N....    0.066542   \n",
       "113  STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFKN2J01N....    0.002735   \n",
       "114  STATISTIC_RESULT#1F.EEK10#EKT72_PM1#AFKN2J01N....    0.010159   \n",
       "115                                               None         NaN   \n",
       "\n",
       "    algorithm_satisfied x_train_shape  \n",
       "0                  None          None  \n",
       "1                  None          None  \n",
       "2                  None          None  \n",
       "3                  None          None  \n",
       "4                  None          None  \n",
       "..                  ...           ...  \n",
       "111                None          None  \n",
       "112                None          None  \n",
       "113                None          None  \n",
       "114                None          None  \n",
       "115                TRUE     (959, 31)  \n",
       "\n",
       "[116 rows x 9 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d27c070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPER_NO   TOOL_NAME  PRODUCT_ID      x_train_shape\n",
       "1F.EEK10  EKT72_PM1  AFGN1501N.0C02  (150, 83)        1\n",
       "                     AFKN2J01N.0U01  (959, 31)        1\n",
       "Name: x_train_shape, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_pandas.groupby(grpby_list_)['x_train_shape'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e1302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0481ac83",
   "metadata": {},
   "source": [
    "-----------------   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "94d4c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetFinalResultsForUvaData:\n",
    "    def __init__(self, df: pyspark.sql.dataframe, grpby_list: List[str], request_id: str,\n",
    "                 grps_all: pyspark.sql.dataframe, bad_wafer_num: int, big_or_small: str,\n",
    "                 add_parametric_stats_df: pyspark.sql.dataframe) -> pyspark.sql.dataframe:\n",
    "        self.df = df\n",
    "        self.grpby_list = grpby_list\n",
    "        self.request_id = request_id\n",
    "        self.grps_all = grps_all\n",
    "        self.bad_wafer_num = bad_wafer_num\n",
    "        self.big_or_small = big_or_small\n",
    "        self.add_parametric_stats_df = add_parametric_stats_df\n",
    "\n",
    "    @staticmethod\n",
    "    def split_score_big_sample(df: pyspark.sql.dataframe, grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "        select_expr = grpby_list + ['bad_wafer', 'roc_auc_score', 'algorithm_satisfied', 'x_train_shape']\n",
    "        selected_df = df.select(*select_expr)\n",
    "        sample_res = selected_df.dropna()\n",
    "        return sample_res\n",
    "\n",
    "    @staticmethod\n",
    "    def split_features(df: pd.DataFrame, index: int) -> str:\n",
    "        \"\"\"\n",
    "        Split the 'features' column based on the specified index.\n",
    "\n",
    "        Parameters:\n",
    "        - df: RandomForest modeling results with 'features' column.\n",
    "        - index: Order value.\n",
    "\n",
    "        Returns:\n",
    "        - str: Field attribute value.\n",
    "        \"\"\"\n",
    "        return df['features'].apply(lambda x: x.split('#')[index])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_split_feature_importance_table(df: pd.DataFrame, grpby_list: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the table after splitting the 'features' column based on the specified grouping columns.\n",
    "\n",
    "        Parameters:\n",
    "        - df: RandomForest modeling results with 'features' column.\n",
    "        - grpby_list: List of grouping columns.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Table after splitting features.\n",
    "        \"\"\"\n",
    "        n_feats = len(grpby_list)\n",
    "        for i in range(n_feats):\n",
    "            df[grpby_list[i]] = GetFinalResultsForUvaData.split_features(df, i + 1)\n",
    "\n",
    "        df['parametric_name'] = GetFinalResultsForUvaData.split_features(df, n_feats + 1)\n",
    "        df['step'] = GetFinalResultsForUvaData.split_features(df, n_feats + 2)\n",
    "        df['stats'] = GetFinalResultsForUvaData.split_features(df, n_feats + 3)\n",
    "        df = df.drop(['features'], axis=1).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def add_feature_stats(df: pd.DataFrame, grpby_list: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Add a column with all statistical features of parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - df: Feature importance table after processing.\n",
    "        - grpby_list: List of grouping columns.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: New column containing all statistical features: 'feature_stats'.\n",
    "        \"\"\"\n",
    "        grpby_list_extend = grpby_list + ['parametric_name', 'step']\n",
    "        feature_stats = df.groupby(grpby_list_extend)['stats'].unique().reset_index()\n",
    "        feature_stats['stats'] = [feature_stats['stats'].iloc[i].tolist() for i in range(len(feature_stats))]\n",
    "        feature_stats['stats'] = feature_stats['stats'].apply(lambda x: \"#\".join(x))\n",
    "        feature_stats = feature_stats.assign(\n",
    "            parametric_name=lambda x: x['parametric_name'] + str('#') + x['step']).drop(\n",
    "            'step', axis=1)\n",
    "        return feature_stats\n",
    "\n",
    "    @staticmethod\n",
    "    def split_calculate_features_big_sample(df: pyspark.sql.dataframe, grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Split and calculate features based on the specified grouping columns.\n",
    "\n",
    "        Parameters:\n",
    "        - df: Results after RandomForest modeling.\n",
    "        - grpby_list: List of grouping columns.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Features importance results.\n",
    "        \"\"\"\n",
    "        # Dynamically build schema\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"parametric_name\", StringType(), True),\n",
    "                              StructField(\"importance\", FloatType(), True),\n",
    "                              StructField(\"stats\", StringType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_result(model_results: pd.DataFrame) -> pd.DataFrame:\n",
    "            # Extract 'features' and 'importance' from the RandomForest model results\n",
    "            feature_importance_table = model_results[['features', 'importance']].dropna(axis=0)\n",
    "\n",
    "            # Split features\n",
    "            feature_importance_res_split = GetFinalResultsForUvaData.get_split_feature_importance_table(\n",
    "                df=feature_importance_table,\n",
    "                grpby_list=grpby_list)\n",
    "\n",
    "            # Remove combinations with importance equal to 0\n",
    "            feature_importance_res_split_drop = feature_importance_res_split.query(\"importance >= 0\").reset_index(\n",
    "                drop=True)\n",
    "\n",
    "            # Take the top 60% or 100% of each combination result\n",
    "            feature_importance_res_split_nlargest = (feature_importance_res_split_drop.groupby(by=grpby_list)\n",
    "                                                     .apply(\n",
    "                lambda x: x.nlargest(int(x.shape[0] * 0.6), 'importance') if x.shape[0] > 1 else x.nlargest(\n",
    "                    int(x.shape[0] * 1), 'importance'))\n",
    "                                                     .reset_index(drop=True))\n",
    "\n",
    "            # Add a column with all statistical features: 'feature_stats'\n",
    "            feature_stats = GetFinalResultsForUvaData.add_feature_stats(df=feature_importance_res_split_drop,\n",
    "                                                                        grpby_list=grpby_list)\n",
    "\n",
    "            # Sum the importance for the same combination and parameter: 'feature_importance_groupby'\n",
    "            feature_importance_groupby = (\n",
    "                feature_importance_res_split_nlargest.groupby(grpby_list + ['parametric_name', 'step'])['importance']\n",
    "                .sum().reset_index())\n",
    "            feature_importance_groupby = (\n",
    "                feature_importance_groupby.assign(parametric_name=lambda x: x['parametric_name'] + str('#') + x['step'])\n",
    "                .drop('step', axis=1))\n",
    "\n",
    "            # Connect 'feature_stats' and 'feature_importance_groupby'\n",
    "            grpby_stats = pd.merge(feature_stats, feature_importance_groupby,\n",
    "                                   on=grpby_list + ['parametric_name']).dropna().reset_index(drop=True)\n",
    "            return grpby_stats\n",
    "\n",
    "        return df.groupby(grpby_list).apply(get_result)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_final_results_big_sample(s_res: pyspark.sql.dataframe, f_res: pyspark.sql.dataframe, grpby_list: List[str],\n",
    "                                     bad_wafer_num: int) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Get the final modeling results.\n",
    "\n",
    "        Parameters:\n",
    "        - s_res: ROC AUC scores result.\n",
    "        - f_res: Features importance result.\n",
    "        - grpby_list: List of grouping columns.\n",
    "        - bad_wafer_num: Number of bad wafers in the data.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Final modeling result.\n",
    "        \"\"\"\n",
    "        roc_auc_score_all = s_res.agg({\"roc_auc_score\": \"sum\"}).collect()[0][0]\n",
    "        s_res = s_res.withColumn(\"roc_auc_score_ratio\", col(\"roc_auc_score\") / roc_auc_score_all)\n",
    "        s_res = s_res.withColumn(\"bad_ratio\", col(\"bad_wafer\") / bad_wafer_num)\n",
    "\n",
    "        df_merge = s_res.join(f_res, on=grpby_list, how='left')\n",
    "        df_merge = df_merge.withColumn('weight_original',\n",
    "                                       col('roc_auc_score_ratio') * col('bad_ratio') * col('importance'))\n",
    "\n",
    "        # Normalize again\n",
    "        weight_all = df_merge.agg({\"weight_original\": \"sum\"}).collect()[0][0]\n",
    "        df_merge = df_merge.withColumn(\"weight\", col(\"weight_original\") / weight_all)\n",
    "        df_merge = df_merge.select(grpby_list + ['parametric_name', 'weight', 'stats'])\n",
    "        return df_merge\n",
    "\n",
    "    @staticmethod\n",
    "    def split_calculate_features_small_sample(df: pyspark.sql.dataframe,\n",
    "                                              grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Calculate features and importance after PCA modeling on a small sample.\n",
    "\n",
    "        Parameters:\n",
    "        - df: PCA modeling results (pyspark.sql.dataframe).\n",
    "        - grpby_list: List of grouping columns (List[str]).\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Dataframe containing features, importance and other information after PCA modeling.\n",
    "        \"\"\"\n",
    "        # Dynamically build schema\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"parametric_name\", StringType(), True),\n",
    "                              StructField(\"importance\", FloatType(), True),\n",
    "                              # StructField(\"bad_wafer\", IntegerType(), True),\n",
    "                              StructField(\"stats\", StringType(), True),\n",
    "                              StructField(\"algorithm_satisfied\", StringType(), True),\n",
    "                              StructField(\"x_train_shape\", StringType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_result(model_results: pd.DataFrame) -> pd.DataFrame:\n",
    "            feature_importance_table = model_results[\n",
    "                ['features', 'importance', 'algorithm_satisfied', 'x_train_shape']].dropna(axis=0)\n",
    "            # Split features\n",
    "            feature_importance_res_split = GetFinalResultsForUvaData.get_split_feature_importance_table(\n",
    "                df=feature_importance_table,\n",
    "                grpby_list=grpby_list)\n",
    "\n",
    "            # Add a column with all statistical features containing parameters: feature_stats\n",
    "            feature_stats = GetFinalResultsForUvaData.add_feature_stats(df=feature_importance_res_split,\n",
    "                                                                        grpby_list=grpby_list)\n",
    "\n",
    "            # Sum the same parameter in the same combination: feature_importance_groupby\n",
    "            feature_importance_groupby = (\n",
    "                feature_importance_res_split.groupby(\n",
    "                    grpby_list + ['parametric_name', 'step', 'algorithm_satisfied', 'x_train_shape'])[\n",
    "                    'importance'].sum().reset_index())\n",
    "            feature_importance_groupby = feature_importance_groupby.assign(\n",
    "                parametric_name=lambda x: x['parametric_name'] + str('#') + x['step']).drop('step', axis=1)\n",
    "\n",
    "            # Connect feature_stats and feature_importance_groupby\n",
    "            grpby_stats = pd.merge(feature_stats, feature_importance_groupby,\n",
    "                                   on=grpby_list + ['parametric_name']).dropna().reset_index(drop=True)\n",
    "            return grpby_stats\n",
    "\n",
    "        return df.groupby(grpby_list).apply(get_result)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_final_results_small_sample(f_res: pyspark.sql.dataframe,\n",
    "                                       grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Get the final modeling results for a small sample.\n",
    "\n",
    "        Parameters:\n",
    "        - f_res: Features and importance results (pyspark.sql.dataframe).\n",
    "        - grpby_list: List of grouping columns (List[str]).\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame: Final modeling results with weights and statistics.\n",
    "        \"\"\"\n",
    "        # f_res = f_res.withColumn(\"bad_ratio\", col(\"bad_wafer\") / bad_wafer_num)\n",
    "        # df_merge = f_res.withColumn('weight_original', col('importance') * col('bad_ratio'))\n",
    "\n",
    "        # Normalize weights again\n",
    "        weight_all = f_res.agg({\"importance\": \"sum\"}).collect()[0][0]\n",
    "        df_merge = f_res.withColumn(\"weight\", col(\"importance\") / weight_all)\n",
    "\n",
    "        # Select columns\n",
    "        df_merge = df_merge.select(grpby_list + ['parametric_name', 'weight', 'stats'])\n",
    "        return df_merge\n",
    "\n",
    "    @staticmethod\n",
    "    def add_certain_column(df: pyspark.sql.dataframe, request_id: str) -> pyspark.sql.dataframe:\n",
    "        df = (df.withColumn(\"weight_percent\", col(\"weight\") * 100)\n",
    "              .withColumn(\"good_num\", df[\"good_num\"].cast(FloatType()))\n",
    "              .withColumn(\"bad_num\", df[\"bad_num\"].cast(FloatType()))\n",
    "              .withColumn(\"request_id\", lit(request_id)))\n",
    "        df = df.orderBy(col(\"weight\").desc())\n",
    "        df = df.withColumn('index_no', monotonically_increasing_id() + 1)\n",
    "        info_list = ['PRODUCT_ID', 'OPER_NO', 'EQP_NAME', 'PRODG1', 'TOOL_NAME']\n",
    "        for column in info_list:\n",
    "            if column not in df.columns:\n",
    "                df = df.withColumn(column, lit(None).cast(StringType()))\n",
    "        return df\n",
    "\n",
    "    def run(self):\n",
    "        if self.big_or_small == 'big':\n",
    "            s_res = self.split_score_big_sample(df=self.df, grpby_list=self.grpby_list)\n",
    "            m = s_res.filter('algorithm_satisfied==True').count()\n",
    "            if m == 0:\n",
    "                from src.exceptions.rca_base_exception import RCABaseException\n",
    "                msg = f\"按照{'+'.join(self.grpby_list)}分组后的数据, 每个组合中只有一种sensor_name, 无法进行差异性分析.\"\n",
    "                raise RCABaseException(msg)\n",
    "            print(\"s_res:\", s_res.count())\n",
    "            s_res.show()\n",
    "\n",
    "            f_res = self.split_calculate_features_big_sample(df=self.df, grpby_list=self.grpby_list)\n",
    "#             f_res_pandas = f_res.toPandas()\n",
    "#             f_res_pandas.to_csv(\"f_res_pandas.csv\")\n",
    "            print(\"f_res:\", f_res.count())\n",
    "            f_res.show()\n",
    "\n",
    "            res_all = self.get_final_results_big_sample(s_res=s_res, f_res=f_res, grpby_list=self.grpby_list,\n",
    "                                                        bad_wafer_num=self.bad_wafer_num)\n",
    "            print(\"res_all:\", res_all.count())\n",
    "            res_all.show()\n",
    "\n",
    "        elif self.big_or_small == 'small':\n",
    "            df = self.df.filter('algorithm_satisfied==True')\n",
    "            if df.count() == 0:\n",
    "                from src.exceptions.rca_base_exception import RCABaseException\n",
    "                msg = f\"按照{'+'.join(self.grpby_list)}分组后的数据, 每个组合中只有一种sensor_name, 无法进行差异性分析.\"\n",
    "                raise RCABaseException(msg)\n",
    " \n",
    "            f_res = self.split_calculate_features_small_sample(df=self.df, grpby_list=self.grpby_list)\n",
    "            print(\"f_res:\", f_res.count())\n",
    "            f_res.show()\n",
    "#             f_res = f_res.withColumn(\"algorithm_satisfied\", f_res[\"algorithm_satisfied\"].cast(StringType()))\n",
    "            f_res_pandas = f_res.toPandas()\n",
    "            f_res_pandas.to_csv(\"f_res_pandas_pca.csv\")\n",
    "\n",
    "            res_all = self.get_final_results_small_sample(f_res=f_res, grpby_list=self.grpby_list)\n",
    "            print(\"res_all:\", res_all.count())\n",
    "            res_all.show()\n",
    "            \n",
    "        missing_rows = self.add_parametric_stats_df.join(res_all,\n",
    "                                                         on=self.grpby_list + ['parametric_name'],\n",
    "                                                         how='left_anti')\n",
    "        missing_rows = missing_rows.withColumn('weight', lit(0))\n",
    "#             print(\"missing_rows:\", missing_rows.count())\n",
    "#             missing_rows.show()\n",
    "\n",
    "        res_update_missing_features = res_all.unionByName(missing_rows, allowMissingColumns=True)\n",
    "#             print(\"res_update_missing_features:\", res_update_missing_features.count())\n",
    "#             res_update_missing_features.show()\n",
    "\n",
    "        res_update_wafer_num = res_update_missing_features.join(self.grps_all.select(self.grpby_list + [\"good_num\", 'bad_num']),\n",
    "                                                                on=self.grpby_list, how='left')\n",
    "        print(\"res_update_wafer_num:\", res_update_wafer_num.count())\n",
    "        res_update_wafer_num.show()\n",
    "\n",
    "        final_res = self.add_certain_column(df=res_update_wafer_num, request_id=self.request_id)\n",
    "        print(\"final_res:\", final_res.count())\n",
    "        final_res.show()\n",
    "        return final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec3b4b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_res: 2\n",
      "+--------+---------+--------------+---------+-------------+-------------------+-------------+\n",
      "| OPER_NO|TOOL_NAME|    PRODUCT_ID|bad_wafer|roc_auc_score|algorithm_satisfied|x_train_shape|\n",
      "+--------+---------+--------------+---------+-------------+-------------------+-------------+\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|       75|          1.0|               TRUE|    (150, 83)|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|      725|   0.86878246|               TRUE|    (959, 31)|\n",
      "+--------+---------+--------------+---------+-------------+-------------------+-------------+\n",
      "\n",
      "f_res: 56\n",
      "+--------+---------+--------------+--------------------+------------+----------+\n",
      "| OPER_NO|TOOL_NAME|    PRODUCT_ID|     parametric_name|  importance|     stats|\n",
      "+--------+---------+--------------+--------------------+------------+----------+\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|APC_POSITION#AOTU...|0.0010027731|MEAN#RANGE|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|APC_POSITION#AOTU...|         0.0|MEAN#RANGE|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|CENTER_GAS_PRESSU...|  0.00329016|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|CHAMBER_PRESSURE#...|0.0017791132|MEAN#RANGE|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|CHAMBER_PRESSURE#...|0.0017186456|MEAN#RANGE|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|CHAMBER_PRESSURE#...|  0.00183466|MEAN#RANGE|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|EDGE_GAS_PRESSURE...|0.0010968434|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|EDGE_GAS_PRESSURE...|5.6725356E-4|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|EDGE_HE_FLOW#AOTU...| 0.013067171|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|ESC_CURRENT#AOTU_...| 0.026949108|  MAX#MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|ESC_CURRENT#AOTU_...|  0.13963464|  MAX#MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|ESC_CURRENT#AOTU_...| 0.100588545|  MAX#MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|ESC_VOLTAGE#AOTU_...| 0.094224855|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|FLOWSPLITEDGE#AOT...|0.0049665268|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|FLOWSPLITEDGE#AOT...|1.5474614E-4|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|LOWERBRINETEMP#AO...|0.0013693706|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|LOWER_TEMPERATURE...|7.6858344E-4|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|LO_C1_VAR_CAPACIT...|2.6028111E-4|     RANGE|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|LO_C1_VAR_CAPACIT...|4.4475406E-4|     RANGE|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|LO_C2_VAR_CAPACIT...|0.0022940165|     RANGE|\n",
      "+--------+---------+--------------+--------------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "res_all: 56\n",
      "+--------+---------+--------------+--------------------+--------------------+----------+\n",
      "| OPER_NO|TOOL_NAME|    PRODUCT_ID|     parametric_name|              weight|     stats|\n",
      "+--------+---------+--------------+--------------------+--------------------+----------+\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|WALL_TEMPERATURE#...|0.009376439637299732|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|PROCESS_GAS_8_O2#...|0.061414931760924066|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|PROCESS_GAS_5_CHF...|0.009427759027894838|       SUM|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_VPP#STEP2_MINI| 0.13419978693912496|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_VPP#STEP2_M...| 0.16996345400317459|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_VPP#AOTU_ST...| 0.21112468593671857|MEAN#SLOPE|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_REF_POWER#A...|  0.0774639073203066|  MAX#MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_POWER#AOTU_...|0.009525489700410625|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_C1_VAR_CAPACIT...| 0.05153036878388525|     SLOPE|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LOWER_TEMPERATURE...|0.051702333095865305|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|FLOWSPLITEDGE#AOT...| 0.02022397439582506|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|FLOWSPLITCENTER#A...|0.017910372765686522|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|EDGE_HE_FLOW#AOTU...|  0.0270210585180127|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|EDGE_GAS_PRESSURE...|0.014057387976907334|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|CHAMBER_PRESSURE#...|0.010205193986723675|MEAN#RANGE|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|CENTER_GAS_PRESSU...|0.014954764734022047|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|WALL_TEMPERATURE#...|2.470827087685727...|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|WALL_TEMPERATURE#...|1.617025940418591E-4|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|UPPER_TEMPERATURE...|2.988526675905936E-4|      MEAN|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|UPPER_TEMPERATURE...|2.243225163095254...|      MEAN|\n",
      "+--------+---------+--------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "res_update_wafer_num: 3691\n",
      "+--------+---------+--------------+--------------------+--------------------+----------+--------+-------+\n",
      "| OPER_NO|TOOL_NAME|    PRODUCT_ID|     parametric_name|              weight|     stats|good_num|bad_num|\n",
      "+--------+---------+--------------+--------------------+--------------------+----------+--------+-------+\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|WALL_TEMPERATURE#...|0.009376439637299732|      MEAN|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|PROCESS_GAS_8_O2#...|0.061414931760924066|      MEAN|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|PROCESS_GAS_5_CHF...|0.009427759027894838|       SUM|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_VPP#STEP2_MINI| 0.13419978693912496|      MEAN|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_VPP#STEP2_M...| 0.16996345400317459|      MEAN|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_VPP#AOTU_ST...| 0.21112468593671857|MEAN#SLOPE|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_REF_POWER#A...|  0.0774639073203066|  MAX#MEAN|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_POWER#AOTU_...|0.009525489700410625|      MEAN|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_C1_VAR_CAPACIT...| 0.05153036878388525|     SLOPE|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LOWER_TEMPERATURE...|0.051702333095865305|      MEAN|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|FLOWSPLITEDGE#AOT...| 0.02022397439582506|      MEAN|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|FLOWSPLITCENTER#A...|0.017910372765686522|      MEAN|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|EDGE_HE_FLOW#AOTU...|  0.0270210585180127|      MEAN|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|EDGE_GAS_PRESSURE...|0.014057387976907334|      MEAN|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|CHAMBER_PRESSURE#...|0.010205193986723675|MEAN#RANGE|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|CENTER_GAS_PRESSU...|0.014954764734022047|      MEAN|     234|    725|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|WALL_TEMPERATURE#...|2.470827087685727...|      MEAN|      75|     75|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|WALL_TEMPERATURE#...|1.617025940418591E-4|      MEAN|      75|     75|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|UPPER_TEMPERATURE...|2.988526675905936E-4|      MEAN|      75|     75|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|UPPER_TEMPERATURE...|2.243225163095254...|      MEAN|      75|     75|\n",
      "+--------+---------+--------------+--------------------+--------------------+----------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "final_res: 3691\n",
      "+--------+---------+--------------+--------------------+--------------------+----------+--------+-------+------------------+----------+--------+--------+------+\n",
      "| OPER_NO|TOOL_NAME|    PRODUCT_ID|     parametric_name|              weight|     stats|good_num|bad_num|    weight_percent|request_id|index_no|EQP_NAME|PRODG1|\n",
      "+--------+---------+--------------+--------------------+--------------------+----------+--------+-------+------------------+----------+--------+--------+------+\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_VPP#AOTU_ST...| 0.21112468593671857|MEAN#SLOPE|   234.0|  725.0|21.112468593671856|       uva|       1|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_VPP#STEP2_M...| 0.16996345400317459|      MEAN|   234.0|  725.0| 16.99634540031746|       uva|       2|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_VPP#STEP2_MINI| 0.13419978693912496|      MEAN|   234.0|  725.0|13.419978693912496|       uva|       3|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_REF_POWER#A...|  0.0774639073203066|  MAX#MEAN|   234.0|  725.0|  7.74639073203066|       uva|       4|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|PROCESS_GAS_8_O2#...|0.061414931760924066|      MEAN|   234.0|  725.0| 6.141493176092407|       uva|       5|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LOWER_TEMPERATURE...|0.051702333095865305|      MEAN|   234.0|  725.0|  5.17023330958653|       uva|       6|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_C1_VAR_CAPACIT...| 0.05153036878388525|     SLOPE|   234.0|  725.0| 5.153036878388525|       uva|       7|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|LO_RF_VPP#AOTU_ST...| 0.03546084098530108|MEAN#SLOPE|    75.0|   75.0|3.5460840985301076|       uva|       8|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|EDGE_HE_FLOW#AOTU...|  0.0270210585180127|      MEAN|   234.0|  725.0|  2.70210585180127|       uva|       9|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|FLOWSPLITEDGE#AOT...| 0.02022397439582506|      MEAN|   234.0|  725.0|2.0223974395825057|       uva|      10|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|FLOWSPLITCENTER#A...|0.017910372765686522|      MEAN|   234.0|  725.0|1.7910372765686522|       uva|      11|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|ESC_CURRENT#AOTU_...|0.015345580109951924|  MAX#MEAN|    75.0|   75.0|1.5345580109951924|       uva|      12|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|CENTER_GAS_PRESSU...|0.014954764734022047|      MEAN|   234.0|  725.0|1.4954764734022046|       uva|      13|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|EDGE_GAS_PRESSURE...|0.014057387976907334|      MEAN|   234.0|  725.0|1.4057387976907334|       uva|      14|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|ESC_CURRENT#AOTU_...|0.011054488981676928|  MAX#MEAN|    75.0|   75.0|1.1054488981676929|       uva|      15|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFGN1501N.0C02|ESC_VOLTAGE#AOTU_...|0.010355131617137029|      MEAN|    75.0|   75.0| 1.035513161713703|       uva|      16|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|CHAMBER_PRESSURE#...|0.010205193986723675|MEAN#RANGE|   234.0|  725.0|1.0205193986723675|       uva|      17|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|LO_RF_POWER#AOTU_...|0.009525489700410625|      MEAN|   234.0|  725.0|0.9525489700410625|       uva|      18|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|PROCESS_GAS_5_CHF...|0.009427759027894838|       SUM|   234.0|  725.0|0.9427759027894839|       uva|      19|    null|  null|\n",
      "|1F.EEK10|EKT72_PM1|AFKN2J01N.0U01|WALL_TEMPERATURE#...|0.009376439637299732|      MEAN|   234.0|  725.0|0.9376439637299733|       uva|      20|    null|  null|\n",
      "+--------+---------+--------------+--------------------+--------------------+----------+--------+-------+------------------+----------+--------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_res = GetFinalResultsForUvaData(df=result, grpby_list=grpby_list_, request_id=request_id_,\n",
    "                                      grps_all=common_res,\n",
    "                                      bad_wafer_num=bad_wafer_num, big_or_small=big_or_small,\n",
    "                                      add_parametric_stats_df=add_parametric_stats_df).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85479902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16efc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d827515",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res_pandas = final_res.toPandas()\n",
    "final_res_pandas.to_csv(\"final_res_pandas1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9306ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OPER_NO</th>\n",
       "      <th>TOOL_NAME</th>\n",
       "      <th>parametric_name</th>\n",
       "      <th>stats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>EKT72_PM1</td>\n",
       "      <td>APC_POSITION#AOTU_STEP_2</td>\n",
       "      <td>MEAN#RANGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>EKT72_PM1</td>\n",
       "      <td>APC_POSITION#AOTU_STEP_4</td>\n",
       "      <td>MEAN#RANGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>EKT72_PM1</td>\n",
       "      <td>APC_POSITION#AOTU_STEP_6</td>\n",
       "      <td>RANGE#MEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>EKT72_PM1</td>\n",
       "      <td>APC_POSITION#AOTU_STEP_7</td>\n",
       "      <td>MEAN#RANGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>EKT72_PM1</td>\n",
       "      <td>AUTO_CHECK_LEAK_RATEA#WINDOW_1</td>\n",
       "      <td>MEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>EKT72_PM2</td>\n",
       "      <td>UPPER_TEMPERATURE#AOTU_STEP_7</td>\n",
       "      <td>MEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>EKT72_PM2</td>\n",
       "      <td>WALL_TEMPERATURE#AOTU_STEP_2</td>\n",
       "      <td>MEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>EKT72_PM2</td>\n",
       "      <td>WALL_TEMPERATURE#AOTU_STEP_4</td>\n",
       "      <td>MEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>EKT72_PM2</td>\n",
       "      <td>WALL_TEMPERATURE#AOTU_STEP_6</td>\n",
       "      <td>MEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>EKT72_PM2</td>\n",
       "      <td>WALL_TEMPERATURE#AOTU_STEP_7</td>\n",
       "      <td>MEAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>378 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OPER_NO  TOOL_NAME                 parametric_name       stats\n",
       "0    1F.EEK10  EKT72_PM1        APC_POSITION#AOTU_STEP_2  MEAN#RANGE\n",
       "1    1F.EEK10  EKT72_PM1        APC_POSITION#AOTU_STEP_4  MEAN#RANGE\n",
       "2    1F.EEK10  EKT72_PM1        APC_POSITION#AOTU_STEP_6  RANGE#MEAN\n",
       "3    1F.EEK10  EKT72_PM1        APC_POSITION#AOTU_STEP_7  MEAN#RANGE\n",
       "4    1F.EEK10  EKT72_PM1  AUTO_CHECK_LEAK_RATEA#WINDOW_1        MEAN\n",
       "..        ...        ...                             ...         ...\n",
       "373  1F.EEK10  EKT72_PM2   UPPER_TEMPERATURE#AOTU_STEP_7        MEAN\n",
       "374  1F.EEK10  EKT72_PM2    WALL_TEMPERATURE#AOTU_STEP_2        MEAN\n",
       "375  1F.EEK10  EKT72_PM2    WALL_TEMPERATURE#AOTU_STEP_4        MEAN\n",
       "376  1F.EEK10  EKT72_PM2    WALL_TEMPERATURE#AOTU_STEP_6        MEAN\n",
       "377  1F.EEK10  EKT72_PM2    WALL_TEMPERATURE#AOTU_STEP_7        MEAN\n",
       "\n",
       "[378 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_parametric_stats_df_pandas = add_parametric_stats_df.toPandas()\n",
    "add_parametric_stats_df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8199ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_parametric_stats_df_pandas.to_csv(\"add_parametric_stats_df_pandas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dccb25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OPER_NO</th>\n",
       "      <th>TOOL_NAME</th>\n",
       "      <th>parametric_name</th>\n",
       "      <th>stats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>EKT72_PM1</td>\n",
       "      <td>WALL_TEMPERATURE#AOTU_STEP_2</td>\n",
       "      <td>MEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>1F.EEK10</td>\n",
       "      <td>EKT72_PM2</td>\n",
       "      <td>WALL_TEMPERATURE#AOTU_STEP_2</td>\n",
       "      <td>MEAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      OPER_NO  TOOL_NAME               parametric_name stats\n",
       "185  1F.EEK10  EKT72_PM1  WALL_TEMPERATURE#AOTU_STEP_2  MEAN\n",
       "374  1F.EEK10  EKT72_PM2  WALL_TEMPERATURE#AOTU_STEP_2  MEAN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_parametric_stats_df_pandas[add_parametric_stats_df_pandas['parametric_name'] == 'WALL_TEMPERATURE#AOTU_STEP_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f00ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91f798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0689c19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa690138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36e709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099cbbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364ed2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ed1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e118482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ea054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4d631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990eaf14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bed9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4301d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c969c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e616c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
