{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7541854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from pca import pca\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, max, col, countDistinct, when, rank, lit\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# from backend_spark.doris_common.doris_client import DorisClient\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5d4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/python-3.9.13/bin/python3'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"pandas_udf\") \\\n",
    "    .config('spark.sql.session.timeZone', 'Asia/Shanghai') \\\n",
    "    .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "    .config('spark.driver.memory', '1024m') \\\n",
    "    .config('spark.driver.cores', '3') \\\n",
    "    .config('spark.executor.memory', '1024m') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.cores.max', '2') \\\n",
    "    .config('spark.driver.host','192.168.22.28') \\\n",
    "    .master(\"spark://192.168.12.47:7077,192.168.12.48:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2377dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "##############################从kafka消息读取需要的资料#########################\n",
    "############################################################################\n",
    "def get_some_info(df:pd.DataFrame):\n",
    "    if len(df) > 0:\n",
    "        df = df.head(1)\n",
    "\n",
    "    request_id = df[\"requestId\"].values[0]\n",
    "    request_params = df[\"requestParam\"].values[0]\n",
    "    # 避免存在单引号，因为json 引号只有双引号\n",
    "    request_params = request_params.replace('\\'', \"\\\"\")   \n",
    "    parse_dict = json.loads(request_params)\n",
    "    merge_prodg1 = parse_dict[0]['mergeProdg1']\n",
    "    \n",
    "    try:\n",
    "        merge_operno = list(parse_dict[0]['mergeOperno'])\n",
    "    except KeyError:\n",
    "        merge_operno = None\n",
    "\n",
    "    if merge_prodg1 == '1':\n",
    "        grpby_list = ['OPER_NO', 'TOOL_NAME']\n",
    "    elif merge_prodg1 == '0':\n",
    "        grpby_list = ['PRODG1', 'OPER_NO', 'TOOL_NAME']\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return parse_dict, request_id, grpby_list, merge_operno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c7cb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550911, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = pd.read_csv(\"D:/Jupyterfiles/晶合MVAFDC_general开发/MVAanlysisDevelop/uva_algorithm/CASE1_DATA/DWD_POC_CASE_FD_UVA_DATA_CASE1_PROCESSED1.csv\")\n",
    "df_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51cc0d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EKT72_PM1', 'EKT72_PM2'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas['TOOL_NAME'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f0a20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_pandas[df_pandas['TOOL_NAME']  == 'EKT72_PM2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas[df_pandas['label']==1]['WAFER_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4336cb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\pandas\\internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\pandas\\utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "550911"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = ps.from_pandas(df_pandas).to_spark()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ae65eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+--------+--------------+--------+---------+----------+--------------------+--------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "| WAFER_ID|TOOL_ID|RUN_ID|EQP_NAME|    PRODUCT_ID|  PRODG1|TOOL_NAME|    LOT_ID|         RECIPE_NAME| OPER_NO|         START_TIME|     parametric_name| CASE_INFO|STATUS|STATISTIC_RESULT|label|\n",
      "+---------+-------+------+--------+--------------+--------+---------+----------+--------------------+--------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_10_CO...|2023-06-16|NORMAL|             0.0|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_3_C4F...|2023-06-16|NORMAL|             0.0|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_14_O2...|2023-06-16|NORMAL|             7.5|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|BOTTOMFLOWRATE#AO...|2023-06-16|NORMAL|          29.993|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_12_CH...|2023-06-16|NORMAL|             0.0|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_5_CHF...|2023-06-16|NORMAL|        123722.7|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|APC_POSITION#AOTU...|2023-06-16|NORMAL|             4.5|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|LO_RF_REF_POWER#A...|2023-06-16|NORMAL|          1.9329|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|EDGE_HE_PRESSURE#...|2023-06-16|NORMAL|            24.9|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|LO_C2_VAR_CAPACIT...|2023-06-16|NORMAL|             5.0|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_9_O2#...|2023-06-16|NORMAL|             0.0|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|LO_RF_VPP#STEP2_M...|2023-06-16|NORMAL|        901.9479|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_16_CH...|2023-06-16|NORMAL|             0.0|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|CHAMBER_PRESSURE#...|2023-06-16|NORMAL|             1.3|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|CENTER_GAS_PRESSU...|2023-06-16|NORMAL|             5.2|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|CENTER_HE_PRESSUR...|2023-06-16|NORMAL|             6.9|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|FLOWSPLITRATIO#AO...|2023-06-16|NORMAL|            50.4|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|LOWERBRINETEMP#AO...|2023-06-16|NORMAL|         59.9724|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_11_N2...|2023-06-16|NORMAL|             0.0|    0|\n",
      "|NGE186-07|  11341|149770|   EKT72|AFKN2J01N.0U01|L11CD02A|EKT72_PM1|NGE186.000|NEW-DRM/P1/110NM/...|1F.EEK10|2023-06-16 02:15:06|PROCESS_GAS_4_CH2...|2023-06-16|NORMAL|             0.0|    0|\n",
      "+---------+-------+------+--------+--------------+--------+---------+----------+--------------------+--------+-------------------+--------------------+----------+------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23053032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5f712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1c8dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "##################################FDC数据预处理#############################\n",
    "############################################################################\n",
    "def _pre_process(df):\n",
    "    \"\"\"\n",
    "    param df: 从数据库中读取出来的某个CASE数据\n",
    "    return: 数据预处理，后面要根据实际情况统一添加\n",
    "    \"\"\"\n",
    "    # 只选出会用到的列\n",
    "    df = df.select('WAFER_ID', 'TOOL_ID', 'RUN_ID', 'EQP_NAME', 'PRODUCT_ID', 'PRODG1', 'TOOL_NAME',\n",
    "                   'OPER_NO', 'parametric_name', 'STATISTIC_RESULT', 'label')\n",
    "    # 剔除NA值\n",
    "    df = df.filter(col('STATISTIC_RESULT').isNotNull())\n",
    "    # 按照所有的行进行去重\n",
    "    df1 = df.dropDuplicates()\n",
    "    # 选最新的RUN\n",
    "    df2 = df1.groupBy('WAFER_ID', 'OPER_NO', 'TOOL_ID').agg(max('RUN_ID').alias('RUN_ID'))\n",
    "    df_run = df1.join(df2.dropDuplicates(subset=['WAFER_ID', 'OPER_NO', 'TOOL_ID', 'RUN_ID']),\n",
    "                      on=['WAFER_ID', 'OPER_NO', 'TOOL_ID', 'RUN_ID'], how='inner')\n",
    "    return df_run\n",
    "\n",
    "\n",
    "\n",
    "def commonality_analysis(df_run, grpby_list):\n",
    "    \"\"\"\n",
    "    param df_run: 数据预处理后的数据\n",
    "    return: 共性分析后的结果， 返回bad wafer前十的组合\n",
    "    \"\"\"\n",
    "    grps = (df_run.groupBy(grpby_list)\n",
    "            .agg(countDistinct('WAFER_ID').alias('wafer_count'),\n",
    "                 countDistinct('WAFER_ID', when(df_run['label'] == 0, 1)).alias('good_num'),\n",
    "                 countDistinct('WAFER_ID', when(df_run['label'] == 1, 1)).alias('bad_num'))\n",
    "            .orderBy('bad_num', ascending=False))\n",
    "\n",
    "    # 单站点+单腔室的情况\n",
    "    if grps.count() == 1:\n",
    "        return grps\n",
    "    else:\n",
    "        grps = grps.filter(grps['bad_num'] > 0)\n",
    "        window_sep = Window().orderBy(col(\"bad_num\").desc())\n",
    "        ranked_df = grps.withColumn(\"rank\", rank().over(window_sep))\n",
    "        grpss = ranked_df.filter(col(\"rank\") <= 10).drop(\"rank\")\n",
    "        return grpss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba0ae6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550896\n"
     ]
    }
   ],
   "source": [
    "df_run = _pre_process(df1)\n",
    "print(df_run.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e501f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpby_list =['PRODG1', 'TOOL_NAME', 'OPER_NO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df751eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpby_list =['TOOL_NAME', 'OPER_NO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d267d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpby_list =['EQP_NAME', 'OPER_NO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e0beafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpby_list =['PRODUCT_ID', 'EQP_NAME', 'PRODG1', 'OPER_NO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5de44f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpby_list =['OPER_NO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e1a51a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+-------+\n",
      "| OPER_NO|wafer_count|good_num|bad_num|\n",
      "+--------+-----------+--------+-------+\n",
      "|1F.EEK10|       6094|    3409|   2685|\n",
      "+--------+-----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "common_res = commonality_analysis(df_run, grpby_list)\n",
    "common_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grpby_list1 = ['PRODG1', 'OPER_NO', 'TOOL_NAME']\n",
    "# common_res1 = commonality_analysis(df_run, grpby_list1)\n",
    "# common_res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "501fa47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#################################获取样本数据#########################\n",
    "############################################################################\n",
    "def get_data_list(common_res, grpby_list, big_or_small='big'):\n",
    "    \"\"\"\n",
    "    param common_res: 共性分析后的结果, 按照大样本或者小样本条件筛选出组合\n",
    "    param grpby_list: 按照PRODG1+OPER_NO+TOOL_NAME分组或OPER_NO+TOOL_NAME分组\n",
    "    param big_or_small: big或者small\n",
    "    return: 对应组合的字典形式, 包在一个大列表中\n",
    "    \"\"\"\n",
    "    assert big_or_small in ['big', 'small'], \"只能选择big或者small, 请检查拼写\"\n",
    "    if big_or_small == 'big':\n",
    "        good_bad_grps = common_res.filter(\"good_num >= 3 AND bad_num >= 3\")\n",
    "    else:\n",
    "        good_bad_grps = common_res.filter(\"bad_num >= 1 AND wafer_count >=2\")\n",
    "    good_bad_grps = good_bad_grps.orderBy(col(\"bad_num\").desc(), col(\"wafer_count\").desc(), col(\"good_num\").desc()).limit(5)\n",
    "\n",
    "    data_list = good_bad_grps[grpby_list].collect()\n",
    "    data_dict_list = [row.asDict() for row in data_list]\n",
    "    return data_dict_list\n",
    "\n",
    "\n",
    "def get_train_data(df_run, data_dict_list):\n",
    "    \"\"\"\n",
    "    param df_run: 数据预处理后的数据\n",
    "    param data_dict: 筛选后的字典结果\n",
    "    return: 从原始数据中过滤出真正用来建模的组合数据\n",
    "    \"\"\"\n",
    "    # 获取第一个数据字典\n",
    "    first_data_dict = data_dict_list[0]\n",
    "\n",
    "    # 动态构建过滤条件\n",
    "    conditions = \" AND \".join([\"{} == '{}'\".format(col, first_data_dict[col]) for col in first_data_dict])\n",
    "    print(\"conditions1\", conditions)\n",
    "    df_s = df_run.filter(conditions)\n",
    "\n",
    "    for i in range(1, len(data_dict_list)):\n",
    "        data_dict = data_dict_list[i]\n",
    "        conditions = \" AND \".join([\"{} == '{}'\".format(col, data_dict[col]) for col in data_dict])\n",
    "        print(\"conditions2\", conditions)\n",
    "        df_m = df_run.filter(conditions)\n",
    "        df_s = df_s.union(df_m)\n",
    "    return df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4a7e6595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict_list_bs = get_data_list(common_res, grpby_list, big_or_small='big')\n",
    "data_dict_list_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ae81e8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditions1 \n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\nSyntax error, unexpected empty statement(line 1, pos 0)\n\n== SQL ==\n\n^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29040\\2952250650.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_run_bs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_run\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dict_list_bs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_run_bs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29040\\2432429408.py\u001b[0m in \u001b[0;36mget_train_data\u001b[1;34m(df_run, data_dict_list)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mconditions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" AND \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"{} == '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_data_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfirst_data_dict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"conditions1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconditions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mdf_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconditions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dict_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, condition)\u001b[0m\n\u001b[0;32m   2075\u001b[0m         \"\"\"\n\u001b[0;32m   2076\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2078\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mParseException\u001b[0m: \nSyntax error, unexpected empty statement(line 1, pos 0)\n\n== SQL ==\n\n^^^\n"
     ]
    }
   ],
   "source": [
    "df_run_bs = get_train_data(df_run, data_dict_list_bs)\n",
    "df_run_bs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599cd60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b3f6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#########################获取传入的整个数据中的所有bad_wafer个数############\n",
    "############################################################################\n",
    "def get_all_bad_wafer_num(df):\n",
    "    \"\"\"\n",
    "    param df: 筛选后的数据\n",
    "    return: 数据中所有bad_wafer的数量\n",
    "    \"\"\"\n",
    "    return df.filter(\"label == 1\").select('WAFER_ID').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f7b0b5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1099"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_wafer_num_big_sample = get_all_bad_wafer_num(df_run_bs)\n",
    "bad_wafer_num_big_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2fcf22c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRODUCT_ID', 'EQP_NAME', 'PRODG1', 'OPER_NO']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grpby_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d6c55a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#####################对good>=3和bad>=3的数据，用rf建模######################\n",
    "############################################################################\n",
    "def get_pivot_table(df, grpby_list):\n",
    "    \"\"\"\n",
    "    param df: 大样本组合的数据\n",
    "    param grpby_list: 分组字段\n",
    "    return: 表格透视后的结果\n",
    "    \"\"\"\n",
    "    index_cols = ['WAFER_ID', 'label']\n",
    "    columns_cols = grpby_list + ['parametric_name']\n",
    "    df_pivot = df.dropna(axis=0).pivot_table(index=index_cols, \n",
    "                                             columns=columns_cols,\n",
    "                                             values=['STATISTIC_RESULT'])\n",
    "    df_pivot.columns = df_pivot.columns.map('#'.join)\n",
    "    df_pivot = df_pivot.fillna(df_pivot.mean()).reset_index(drop=False)\n",
    "    return df_pivot\n",
    "\n",
    "\n",
    "def fit_rf_big_sample(df, grpby_list):\n",
    "    \"\"\"\n",
    "    param df: 大样本组合的数据\n",
    "    param grpby_list: 分组字段\n",
    "    return: RandomForest建模后的结果\n",
    "    \"\"\"\n",
    "    # 动态构建 schema_all\n",
    "    struct_fields  = [StructField(col, StringType(), True) for col in grpby_list]\n",
    "    struct_fields.extend([StructField(\"bad_wafer\", IntegerType(), True),\n",
    "                          StructField(\"roc_auc_score\", FloatType(), True),\n",
    "                          StructField(\"features\", StringType(), True),\n",
    "                          StructField(\"importance\", FloatType(), True)])\n",
    "    schema_all = StructType(struct_fields)\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_model_result(df_run):\n",
    "        # 表格透视\n",
    "        df_pivot = get_pivot_table(df=df_run, grpby_list=grpby_list)\n",
    "\n",
    "        # 定义自变量和因变量\n",
    "        X_train = df_pivot[df_pivot.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "        y_train = df_pivot[['label']]\n",
    "\n",
    "        z_ratio = y_train.value_counts(normalize=True)\n",
    "        good_ratio = z_ratio[0]\n",
    "        bad_ratio = z_ratio[1]\n",
    "        if abs(good_ratio - bad_ratio) > 0.7:\n",
    "            undersampler = ClusterCentroids(random_state=101)\n",
    "            X_train, y_train = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        # 网格搜索\n",
    "        pipe = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value=-999)),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', RandomForestClassifier(random_state=2024))])\n",
    "        param_grid = {'model__n_estimators': [*range(50, 100, 10)],\n",
    "                      'model__max_depth': [*range(10, 50, 10)]}\n",
    "        grid = GridSearchCV(estimator=pipe, scoring='roc_auc', param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "        grid.fit(X_train.values, y_train.values.ravel())\n",
    "        roc_auc_score_ = grid.best_score_\n",
    "\n",
    "        # 特征重要度、结果汇总\n",
    "        small_importance_res = pd.DataFrame({\n",
    "            'features': X_train.columns,\n",
    "            'importance': grid.best_estimator_.steps[2][1].feature_importances_})\n",
    "        \n",
    "        sample_res_dict = {'bad_wafer': sum(df_pivot['label']),\n",
    "                           'roc_auc_score': roc_auc_score_}\n",
    "        sample_res_dict.update({col: df_run[col].unique() for col in grpby_list})\n",
    "        small_sample_res = pd.DataFrame(sample_res_dict)\n",
    "    \n",
    "        return pd.concat([small_importance_res, small_sample_res])\n",
    "    return df.groupby(grpby_list).apply(get_model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "254b6a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+-------+---------+-------------+--------------------+------------+\n",
      "|PRODUCT_ID|EQP_NAME|PRODG1|OPER_NO|bad_wafer|roc_auc_score|            features|  importance|\n",
      "+----------+--------+------+-------+---------+-------------+--------------------+------------+\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|0.0015422927|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|      4.0E-4|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...| 0.009099345|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|3.3039282E-4|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|0.0069474624|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|4.9058086E-4|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...| 6.418834E-4|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|         0.0|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|5.8603828E-6|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|0.0015721461|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|0.0014058767|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|0.0115505885|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|         0.0|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|         0.0|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|         0.0|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|         0.0|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|0.0072822515|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|         0.0|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...|6.3219067E-4|\n",
      "|      null|    null|  null|   null|     null|         null|STATISTIC_RESULT#...| 4.368198E-4|\n",
      "+----------+--------+------+-------+---------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = fit_rf_big_sample(df=df_run_bs, grpby_list=grpby_list)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "73cf279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "respp = res.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_table = respp[['features', 'importance']].dropna(axis=0)\n",
    "feature_importance_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f63dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_table['features'].iloc[0].split(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1710363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpby_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abbd89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feats = len(grpby_list)\n",
    "for i in range(n_feats):\n",
    "    df[grpby_list[i]] = split_features(df, i + 1)\n",
    "    \n",
    "df['parametric_name'] = split_features(df, n_feats + 1)\n",
    "df['step'] = split_features(df, n_feats + 2)\n",
    "df['stats'] = split_features(df, n_feats + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a8b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a9516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cea8b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#########################对good>=3和bad>=3建模后的结果进行整合############################\n",
    "#####################################################################################\n",
    "def split_score_big_sample(df, grpby_list):\n",
    "    \"\"\"\n",
    "    param df: RandomForest建模后的结果\n",
    "    param grpby_list: 分组字段\n",
    "    return: roc_auc分数结果\n",
    "    \"\"\"\n",
    "    # 动态构建 schema_all\n",
    "    struct_fields  = [StructField(col, StringType(), True) for col in grpby_list]\n",
    "    struct_fields.extend([StructField(\"bad_wafer\", IntegerType(), True),\n",
    "                          StructField(\"roc_auc_score\", FloatType(), True)])\n",
    "    schema_all = StructType(struct_fields)\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_result(model_results):\n",
    "        select_expr = grpby_list + ['bad_wafer', 'roc_auc_score']\n",
    "        sample_res = model_results[select_expr].dropna(axis=0)\n",
    "        sample_res = sample_res[sample_res['roc_auc_score'] > 0.6]\n",
    "        return sample_res\n",
    "    return df.groupby(grpby_list).apply(get_result)\n",
    "\n",
    "\n",
    "\n",
    "def split_features(df, index) -> str:\n",
    "    \"\"\"\n",
    "    param df: RandomForest建模后的feature_importance_table\n",
    "    param index: 顺序值\n",
    "    return: 字段属性值\n",
    "    \"\"\"\n",
    "    return df['features'].apply(lambda x: x.split('#')[index])\n",
    "\n",
    "\n",
    "def get_split_feature_importance_table(df, grpby_list):\n",
    "    \"\"\"\n",
    "    param df: RandomForest建模后的feature_importance_table\n",
    "    param grpby_list: OPER_NO+TOOL_NAME+PRODG1或者OPER_NO+TOOL_NAME\n",
    "    return: 分裂features后的表\n",
    "    \"\"\"\n",
    "    n_feats = len(grpby_list)\n",
    "    for i in range(n_feats):\n",
    "        df[grpby_list[i]] = split_features(df, i + 1)\n",
    "\n",
    "    df['parametric_name'] = split_features(df, n_feats + 1)\n",
    "    df['step'] = split_features(df, n_feats + 2)\n",
    "    df['stats'] = split_features(df, n_feats + 3)\n",
    "    df = df.drop(['features'], axis=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_feature_stats(df, grpby_list):\n",
    "    \"\"\"\n",
    "    param df: 经过处理后的feature_importance_table\n",
    "    return: 新增一列，含有参数的所有统计特征:feature_stats\n",
    "    \"\"\"\n",
    "    grpby_list_extend = grpby_list + ['parametric_name', 'step']\n",
    "    feature_stats = df.groupby(grpby_list_extend)['stats'].unique().reset_index()\n",
    "    feature_stats['stats'] = [feature_stats['stats'].iloc[i].tolist() for i in range(len(feature_stats))]\n",
    "    feature_stats['stats'] = feature_stats['stats'].apply(lambda x: \"#\".join(x))\n",
    "    feature_stats = feature_stats.assign(parametric_name=lambda x: x['parametric_name']+str('#')+x['step']).drop('step', axis=1)\n",
    "    return feature_stats\n",
    "    \n",
    "    \n",
    "def split_calculate_features_big_sample(df, grpby_list):\n",
    "    \"\"\"\n",
    "    param df: RandomForest建模后的结果\n",
    "    param grpby_list: 分组字段\n",
    "    return: features和importance结果\n",
    "    \"\"\"\n",
    "    # 动态构建 schema_all\n",
    "    struct_fields  = [StructField(col, StringType(), True) for col in grpby_list]\n",
    "    struct_fields.extend([StructField(\"parametric_name\", StringType(), True),\n",
    "                          StructField(\"importance\", FloatType(), True),\n",
    "                          StructField(\"stats\", StringType(), True)])\n",
    "    schema_all = StructType(struct_fields)\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_result(model_results):\n",
    "        # 先从随机森林的模型结果中取出包含features和importance的dataframe\n",
    "        feature_importance_table = model_results[['features', 'importance']].dropna(axis=0)\n",
    "\n",
    "        # 分裂features\n",
    "        feature_importance_res_split = get_split_feature_importance_table(df=feature_importance_table, grpby_list=grpby_list)\n",
    "\n",
    "        # 去除importance为0的组合\n",
    "        feature_importance_res_split_drop = feature_importance_res_split.query(\"importance > 0\").reset_index(drop=True)\n",
    "\n",
    "        # 取每一种组合结果的前60%或者100%\n",
    "        feature_importance_res_split_nlargest = (feature_importance_res_split_drop.groupby(by=grpby_list)\n",
    "                                            .apply(lambda x: x.nlargest(int(x.shape[0]*0.6), 'importance') if x.shape[0]>1 else x.nlargest(int(x.shape[0]*1), 'importance'))\n",
    "                                            .reset_index(drop=True))\n",
    "\n",
    "        # 新增一列，含有参数的所有统计特征:feature_stats\n",
    "        feature_stats = add_feature_stats(df=feature_importance_res_split_drop, grpby_list=grpby_list)\n",
    "\n",
    "        # 对同一种组合里的同一个参数进行求和:feature_importance_groupby\n",
    "        feature_importance_groupby = (feature_importance_res_split_nlargest.groupby(grpby_list + ['parametric_name', 'step'])['importance']\n",
    "                                                                           .sum().reset_index())\n",
    "        feature_importance_groupby = (feature_importance_groupby.assign(parametric_name=lambda x: x['parametric_name'] + str('#') + x['step'])\n",
    "                                                                .drop('step', axis=1))\n",
    "\n",
    "        # feature_stats和feature_importance_groupby连接\n",
    "        grpby_stats = pd.merge(feature_stats, feature_importance_groupby, on=grpby_list + ['parametric_name']).dropna().reset_index(drop=True)\n",
    "        return grpby_stats\n",
    "    return df.groupby(grpby_list).apply(get_result)\n",
    "\n",
    "    \n",
    "\n",
    "def get_finall_results_big_sample(s_res, f_res, grpby_list, bad_wafer_num):\n",
    "    \"\"\"\n",
    "    param s_res: roc_auc分数结果\n",
    "    param f_res: features和importance结果\n",
    "    param bad_wafer_num: 数据中所有bad_wafer的数量\n",
    "    return: 最后的建模结果\n",
    "    \"\"\"\n",
    "    # feature_importance_groupby和sample_res连接\n",
    "    roc_auc_score_all = s_res.agg({\"roc_auc_score\": \"sum\"}).collect()[0][0]\n",
    "    s_res = s_res.withColumn(\"roc_auc_score_ratio\", col(\"roc_auc_score\")/roc_auc_score_all)\n",
    "    s_res = s_res.withColumn(\"bad_ratio\", col(\"bad_wafer\") / bad_wafer_num)\n",
    "\n",
    "    df_merge = s_res.join(f_res, on=grpby_list, how='left')\n",
    "    df_merge = df_merge.withColumn('weight_original', col('roc_auc_score_ratio') * col('bad_ratio') * col('importance'))\n",
    "\n",
    "    # 最后再次进行一次归一化\n",
    "    weight_all = df_merge.agg({\"weight_original\": \"sum\"}).collect()[0][0]\n",
    "    df_merge = df_merge.withColumn(\"weight\", col(\"weight_original\") / weight_all)\n",
    "    df_merge = df_merge.select(grpby_list + ['parametric_name', 'weight', 'stats']).orderBy('weight', ascending=False)\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5f30bd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRODUCT_ID', 'EQP_NAME', 'PRODG1', 'OPER_NO']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grpby_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2b5ebad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\sql\\pandas\\group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------+---------+-------------+\n",
      "|    PRODUCT_ID|EQP_NAME|  PRODG1| OPER_NO|bad_wafer|roc_auc_score|\n",
      "+--------------+--------+--------+--------+---------+-------------+\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|       75|   0.99627453|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|      725|    0.8621095|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|      299|          1.0|\n",
      "+--------------+--------+--------+--------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_res = split_score_big_sample(df=res, grpby_list=grpby_list)\n",
    "s_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "486755c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------+--------------------+------------+----------+\n",
      "|    PRODUCT_ID|EQP_NAME|  PRODG1| OPER_NO|     parametric_name|  importance|     stats|\n",
      "+--------------+--------+--------+--------+--------------------+------------+----------+\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|APC_POSITION#AOTU...| 0.009099345|MEAN#RANGE|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|APC_POSITION#AOTU...|0.0069474624|MEAN#RANGE|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|CENTER_GAS_PRESSU...|0.0115505885|      MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|CHAMBER_PRESSURE#...|0.0072822515|     RANGE|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|CHAMBER_PRESSURE#...|0.0033773556|MEAN#RANGE|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|EDGE_GAS_PRESSURE...|0.0036432669|      MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|EDGE_GAS_PRESSURE...|0.0064301947|      MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|EDGE_HE_FLOW#AOTU...| 0.006049666|      MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|EDGE_HE_FLOW#AOTU...|0.0051699295|      MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|EDGE_HE_FLOW#AOTU...| 0.012448553|      MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|ESC_CURRENT#AOTU_...|  0.11816658|  MAX#MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|ESC_CURRENT#AOTU_...|  0.07030856|  MAX#MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|ESC_CURRENT#AOTU_...| 0.115626015|  MAX#MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|ESC_VOLTAGE#AOTU_...| 0.036389466|      MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|ESC_VOLTAGE#AOTU_...|  0.01570193|      MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|ESC_VOLTAGE#AOTU_...|0.0053210454|      MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|FLOWSPLITEDGE#AOT...| 0.009323658|      MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|LOWER_TEMPERATURE...|  0.00860199|      MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|LOWER_TEMPERATURE...| 0.017347885|      MEAN|\n",
      "|AFGN1501N.0C02|   EKT72|L15RB03A|1F.EEK10|LOWER_TEMPERATURE...|0.0076284045|      MEAN|\n",
      "+--------------+--------+--------+--------+--------------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_res = split_calculate_features_big_sample(df=res, grpby_list=grpby_list)\n",
    "f_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c9e62613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_res_pandas = f_res.toPandas()\n",
    "# f_res_pandas.sort_values('importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dfe31fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------+--------------------+--------------------+----------+\n",
      "|    PRODUCT_ID|EQP_NAME|  PRODG1| OPER_NO|     parametric_name|              weight|     stats|\n",
      "+--------------+--------+--------+--------+--------------------+--------------------+----------+\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|LO_RF_VPP#AOTU_ST...| 0.10312683627148242|MEAN#SLOPE|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|LO_RF_VPP#STEP2_MINI| 0.09257263611968515|      MEAN|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|LO_RF_VPP#STEP2_M...| 0.07835981532533769|      MEAN|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|LO_RF_REF_POWER#A...|0.049023116038479884|  MAX#MEAN|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|PROCESS_GAS_8_O2#...| 0.04796866126622622|      MEAN|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|LO_RF_POWER#AOTU_...| 0.04223486028603683|      MEAN|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|CENTER_GAS_PRESSU...|  0.0416582018575752|      MEAN|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|LOWER_TEMPERATURE...|0.038791287698234575|      MEAN|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|APC_POSITION#AOTU...| 0.03359729720614707|MEAN#RANGE|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|ESC_CURRENT#AOTU_...| 0.03275761339166505|  MAX#MEAN|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|ESC_VOLTAGE#AOTU_...|0.030518050938868467|      MEAN|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|EDGE_GAS_PRESSURE...|0.028313289682804867|      MEAN|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|LO_C1_VAR_CAPACIT...|0.027335436788262696|     SLOPE|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|LOWER_TEMPERATURE...| 0.02705986441497498|      MEAN|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|EDGE_HE_FLOW#AOTU...| 0.02345941972554529|      MEAN|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|LO_C2_VAR_CAPACIT...| 0.02135013411236546|     RANGE|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|FLOWSPLITCENTER#A...| 0.02091409423875942|      MEAN|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|FLOWSPLITEDGE#AOT...|0.020211644543673444|      MEAN|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|FLOWSPLITCENTER#A...|0.019898759846897145|      MEAN|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|CENTER_GAS_PRESSU...| 0.01841974042082995|      MEAN|\n",
      "+--------------+--------+--------+--------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_res_bs = get_finall_results_big_sample(s_res=s_res, f_res=f_res, grpby_list=grpby_list, bad_wafer_num=bad_wafer_num_big_sample)\n",
    "model_res_bs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a960c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b45ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#############################将建模后的结果增加特定的列####################################\n",
    "#####################################################################################\n",
    "def add_certain_column(df, by, request_id, grpby_list):\n",
    "    \"\"\"\n",
    "    param df: 最后的建模结果\n",
    "    param by: 分组字段, 手动增加一列add\n",
    "    param request_id: 传入的request_id\n",
    "    return: 最后的建模结果增加特定的列\n",
    "    \"\"\"\n",
    "    # 动态构建 schema_all\n",
    "    struct_fields  = [StructField(col, StringType(), True) for col in grpby_list]\n",
    "    struct_fields.extend([StructField(\"stats\", StringType(), True),\n",
    "                            StructField(\"parametric_name\", StringType(), True),\n",
    "                            StructField(\"weight\", FloatType(), True),\n",
    "                            StructField(\"request_id\", StringType(), True),\n",
    "                            StructField(\"weight_percent\", FloatType(), True),\n",
    "                            StructField(\"index_no\", IntegerType(), True)])\n",
    "    schema_all = StructType(struct_fields)\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_result(final_res):\n",
    "        final_res['weight'] = final_res['weight'].astype(float)\n",
    "        final_res = final_res.query(\"weight > 0\")\n",
    "        final_res['request_id'] = request_id\n",
    "        final_res['weight_percent'] = final_res['weight'] * 100\n",
    "        final_res = final_res.sort_values('weight', ascending=False)\n",
    "        final_res['index_no'] = [i + 1 for i in range(len(final_res))]\n",
    "        final_res = final_res.drop('add', axis=1)\n",
    "        return final_res\n",
    "    return df.groupby(by).apply(get_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "62ba5671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "|    PRODUCT_ID|EQP_NAME|  PRODG1| OPER_NO|     stats|     parametric_name|     weight|request_id|weight_percent|index_no|\n",
      "+--------------+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|MEAN#SLOPE|LO_RF_VPP#AOTU_ST...| 0.10312684|        sf|     10.312684|       1|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|      MEAN|LO_RF_VPP#STEP2_MINI| 0.09257264|        sf|      9.257263|       2|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|      MEAN|LO_RF_VPP#STEP2_M...| 0.07835981|        sf|     7.8359814|       3|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|  MAX#MEAN|LO_RF_REF_POWER#A...|0.049023118|        sf|      4.902312|       4|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|      MEAN|PROCESS_GAS_8_O2#...| 0.04796866|        sf|      4.796866|       5|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|      MEAN|LO_RF_POWER#AOTU_...| 0.04223486|        sf|      4.223486|       6|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|      MEAN|CENTER_GAS_PRESSU...|  0.0416582|        sf|       4.16582|       7|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|      MEAN|LOWER_TEMPERATURE...|0.038791288|        sf|     3.8791287|       8|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|MEAN#RANGE|APC_POSITION#AOTU...|0.033597298|        sf|     3.3597298|       9|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|  MAX#MEAN|ESC_CURRENT#AOTU_...|0.032757614|        sf|     3.2757614|      10|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|      MEAN|ESC_VOLTAGE#AOTU_...|0.030518051|        sf|      3.051805|      11|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|      MEAN|EDGE_GAS_PRESSURE...| 0.02831329|        sf|     2.8313289|      12|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|     SLOPE|LO_C1_VAR_CAPACIT...|0.027335437|        sf|     2.7335436|      13|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|      MEAN|LOWER_TEMPERATURE...|0.027059864|        sf|     2.7059865|      14|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|      MEAN|EDGE_HE_FLOW#AOTU...| 0.02345942|        sf|      2.345942|      15|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|     RANGE|LO_C2_VAR_CAPACIT...|0.021350134|        sf|     2.1350133|      16|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|      MEAN|FLOWSPLITCENTER#A...|0.020914095|        sf|     2.0914094|      17|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|      MEAN|FLOWSPLITEDGE#AOT...|0.020211644|        sf|     2.0211644|      18|\n",
      "|AFKN4X01N.0B01|   EKT72|L11TG05A|1F.EEK10|      MEAN|FLOWSPLITCENTER#A...| 0.01989876|        sf|      1.989876|      19|\n",
      "|AFKN2J01N.0U01|   EKT72|L11CD02A|1F.EEK10|      MEAN|CENTER_GAS_PRESSU...| 0.01841974|        sf|      1.841974|      20|\n",
      "+--------------+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "request_id = 'sf'\n",
    "final_res_bs = model_res_bs.withColumn('add', lit(0))\n",
    "final_res_add_columns = add_certain_column(df=final_res_bs, by='add', request_id=request_id, grpby_list=grpby_list)\n",
    "final_res_add_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a41f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa3f799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc4beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "394cd258",
   "metadata": {},
   "source": [
    "--------------------------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_big_data_model(df_run, data_dict_list_bs, grpby_list, request_id):\n",
    "\n",
    "    df1 = None\n",
    "    df2 = None\n",
    "    \n",
    "    # 1. 获取用于建模的大样本数据\n",
    "    df_run_bs = get_train_data(df_run, data_dict_list_bs)\n",
    "    if df_run_bs.count() == 0:\n",
    "        msg = '数据库中暂无此类数据!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    # 2. 获取所有bad wafer数量\n",
    "    bad_wafer_num_big_sample = get_all_bad_wafer_num(df_run_bs)\n",
    "    if bad_wafer_num_big_sample < 3:\n",
    "        msg = '数据库中实际BAD_WAFER数量小于3片, 请提供更多的BAD_WAFER数量!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "\n",
    "    # 3. 对挑选出的大样本数据进行建模\n",
    "    res = fit_rf_big_sample(df=df_run_bs, by=grpby_list)\n",
    "    if res.count() == 0:\n",
    "        msg = '算法内部暂时异常!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "\n",
    "    # 4. 将建模结果进行整合\n",
    "    s_res = split_score_big_sample(df=res, by=['PRODG1', 'OPER_NO', 'TOOL_NAME'])\n",
    "    if s_res.count() == 0:\n",
    "        msg = '算法运行评分结果较低, 暂无输出, 建议增加BAD_WAFER数量'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    f_res = split_calculate_features_big_sample(df=res, by=grpby_list)\n",
    "    if f_res.count() == 0:\n",
    "        msg = '算法结果求和暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "\n",
    "    model_res_bs = get_finall_results_big_sample(s_res=s_res, f_res=f_res, bad_wafer_num=bad_wafer_num_big_sample)\n",
    "    if model_res_bs.count() == 0:\n",
    "        msg = '算法结果拼接暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    # 7. 增加特定的列\n",
    "    final_res_bs = model_res_bs.withColumn('add', lit(0))\n",
    "    final_res_add_columns = add_certain_column(df=final_res_bs, by='add', request_id=request_id)\n",
    "    if final_res_add_columns.count() == 0:\n",
    "        msg = '算法结果增加列暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "    else:  \n",
    "        return df1, final_res_add_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269da558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主程序\n",
    "try:\n",
    "    # 从数据库中获取数据\n",
    "#     df1 = get_data_from_doris(select_condition_list=select_condition_list, table_name=table_name)\n",
    "#     print(df1.count())\n",
    "#     if df1.count() == 0:\n",
    "#         msg = '解析SQL获取数据异常: 数据库中可能没有数据!'\n",
    "#         df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "#         df1 = spark.createDataFrame(df_kafka)\n",
    "#         raise ValueError\n",
    "\n",
    "    # 1. 站点融合和数据预处理\n",
    "    df1 = integrate_operno(df=df1, merge_operno_list=merge_operno)\n",
    "    print(df1.count())\n",
    "    df_run = _pre_process(df1)\n",
    "    print(df_run.count())\n",
    "    if df_run.count() == 0:\n",
    "        msg = '该条件下数据库中暂无数据，请检查！'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "\n",
    "    # 2. 进行共性分析\n",
    "    common_res = commonality_analysis(df_run, grpby_list)\n",
    "    common_res.show()\n",
    "    if common_res.count() == 0:\n",
    "        msg = '共性分析结果异常!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "\n",
    "    # 3. 挑选出数据：bad和good要同时大于3\n",
    "    data_dict_list_bs = get_data_list(common_res, grpby_list, big_or_small='big')\n",
    "    print(\"data_dict_list_bs:\", data_dict_list_bs)\n",
    "    if len(data_dict_list_bs) != 0:\n",
    "        print(\"****************大样本算法调用****************\")\n",
    "        df1, final_res_add_columns = fit_big_data_model(df_run, data_dict_list_bs, grpby_list, request_id)\n",
    "    else:        \n",
    "        print(\"****************小样本算法调用****************\")\n",
    "        df1, final_res_add_columns = fit_small_data_model(df_run, common_res, grpby_list, request_id)\n",
    "    \n",
    "\n",
    "    if df1 is not None:\n",
    "        raise ValueError\n",
    "    else:\n",
    "        # final_res_add_columns 是最后的结果，要写回数据库\n",
    "        # ddd = final_res_add_columns.toPandas()\n",
    "        # user =\"root\"\n",
    "        # host = \"10.52.199.81\"\n",
    "        # password = \"Nexchip%40123\"\n",
    "        # db = \"etl\"\n",
    "        # port = 9030\n",
    "        # engine = create_engine(\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\".format(user = user,\n",
    "        #                                                                                     password = password,\n",
    "        #                                                                                     host = host,\n",
    "        #                                                                                     port = port,\n",
    "        #                                                                                     db = db))\n",
    "        # doris_stream_load_from_df(ddd, engine, \"results\")\n",
    "\n",
    "        # # 最终成功的话，就会输出下面这条\n",
    "        print(\"运行成功\")\n",
    "        df_kafka = pd.DataFrame({\"code\": 0, \"msg\": \"运行成功\", \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "\n",
    "except ValueError as ve:\n",
    "    pass\n",
    "\n",
    "except Exception as e:\n",
    "    df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f\"主程序发生异常: {str(e)}\", \"requestId\": request_id}, index=[0])\n",
    "    df1 = spark.createDataFrame(df_kafka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d308a828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb63e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268564f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0782d987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
