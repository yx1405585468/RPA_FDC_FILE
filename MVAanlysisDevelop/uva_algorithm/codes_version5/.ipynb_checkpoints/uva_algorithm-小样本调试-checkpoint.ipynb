{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7541854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from pca import pca\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, max, col, countDistinct, when, rank, lit\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# from backend_spark.doris_common.doris_client import DorisClient\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16370081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.dataframe\n",
    "from typing import List, Dict, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a7ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/python-3.9.13/bin/python3'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"pandas_udf\") \\\n",
    "    .config('spark.sql.session.timeZone', 'Asia/Shanghai') \\\n",
    "    .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "    .config('spark.driver.memory', '1024m') \\\n",
    "    .config('spark.driver.cores', '3') \\\n",
    "    .config('spark.executor.memory', '1024m') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.cores.max', '2') \\\n",
    "    .config('spark.driver.host','192.168.22.28') \\\n",
    "    .master(\"spark://192.168.12.47:7077,192.168.12.48:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ccd1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2377dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "##############################从kafka消息读取需要的资料#########################\n",
    "############################################################################\n",
    "def get_some_info(df:pd.DataFrame):\n",
    "    if len(df) > 0:\n",
    "        df = df.head(1)\n",
    "\n",
    "    request_id = df[\"requestId\"].values[0]\n",
    "    request_params = df[\"requestParam\"].values[0]\n",
    "    # 避免存在单引号，因为json 引号只有双引号\n",
    "    request_params = request_params.replace('\\'', \"\\\"\")   \n",
    "    parse_dict = json.loads(request_params)\n",
    "    grpby_list = parse_dict[0]['grpby_list']\n",
    "    \n",
    "    try:\n",
    "        merge_operno = list(parse_dict[0]['mergeOperno'])\n",
    "    except KeyError:\n",
    "        merge_operno = None\n",
    "\n",
    "    return parse_dict, request_id, grpby_list, merge_operno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af460397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\pandas\\internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\pandas\\utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "# 真正的kafka消息里全都是双引号\n",
    "json_loads_dict = {\n",
    "    \"requestId\": \"small\",\n",
    "    \"requestParam\": [\n",
    "        {'dateRange': [{'start': \"2023-12-01 00:00:00\", 'end': \"2024-01-15 00:00:00\"}], \n",
    "         'lot': [], \n",
    "         'operNo': [\"1G.EEG1R\",\"1G.PPB10\"], \n",
    "         'prodg1': [], \n",
    "         'productId': [], \n",
    "         'eqp': [], \n",
    "         'tool': [], \n",
    "         'recipeName': [], \n",
    "         'waferId': {'good': [\"NBX392-15\",\"NBX392-20\",\"NBX392-24\",\"NBX391-24\",\"NBX391-25\",\"NBX548-09\",\n",
    "                     \"NBX391-01\",\"NBX391-02\",\"NBX391-13\",\"NBX391-17\"], \n",
    "                     'bad': [\"NBX500-10\",\"NBX500-01\",\"NBX500-09\"]}, \n",
    "         'uploadId': '20240110170016023', \n",
    "         'grpby_list': ['OPER_NO', 'TOOL_NAME'],\n",
    "#          'mergeOperno': [{\"2F.CDS10_XX.TDS01\": [\"2F.CDS10\", \"XX.TDS01\"]},\n",
    "#                            {\"2F.CDS20_XX.CDS20\": [\"2F.CDS20\", \"XX.CDS20\"]}]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_pa = pd.DataFrame({\n",
    "    \"requestId\": [json_loads_dict[\"requestId\"]], \n",
    "    \"requestParam\": [json.dumps(json_loads_dict[\"requestParam\"])]})\n",
    "\n",
    "df1 = ps.from_pandas(df_pa).to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbd293ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse_dict是： [{'dateRange': [{'start': '2023-12-01 00:00:00', 'end': '2024-01-15 00:00:00'}], 'lot': [], 'operNo': ['1G.EEG1R', '1G.PPB10'], 'prodg1': [], 'productId': [], 'eqp': [], 'tool': [], 'recipeName': [], 'waferId': {'good': ['NBX392-15', 'NBX392-20', 'NBX392-24', 'NBX391-24', 'NBX391-25', 'NBX548-09', 'NBX391-01', 'NBX391-02', 'NBX391-13', 'NBX391-17'], 'bad': ['NBX500-10', 'NBX500-01', 'NBX500-09']}, 'uploadId': '20240110170016023', 'grpby_list': ['OPER_NO', 'TOOL_NAME']}]\n",
      "parse_dict的类型是： <class 'list'>\n",
      "request_id是： small\n",
      "grpby_list是： ['OPER_NO', 'TOOL_NAME']\n",
      "merge_operno是： None\n"
     ]
    }
   ],
   "source": [
    "#  1. 解析json 为字典， df1为kafka输入的结果数据，获取到parse_dict, request_id, grpby_list\n",
    "df2 = df1.toPandas() \n",
    "parse_dict, request_id, grpby_list, merge_operno = get_some_info(df2)\n",
    "print(\"parse_dict是：\", parse_dict)\n",
    "print(\"parse_dict的类型是：\", type(parse_dict))\n",
    "print(\"request_id是：\", request_id)\n",
    "print(\"grpby_list是：\", grpby_list)\n",
    "print(\"merge_operno是：\", merge_operno)\n",
    "\n",
    "# 2. 从kafka 关键字映射都具体数据源中的字段,没有的可以删除\n",
    "# keyword_map_from_json_to_table: dict = {\n",
    "#     \"prodg1\": \"PRODG1\",\n",
    "#     \"waferId\": \"WAFER_ID\",\n",
    "#     \"dateRange\": \"START_TIME\",\n",
    "#     \"productId\": \"PRODUCT_ID\",\n",
    "#     \"operNo\": \"OPER_NO\",\n",
    "#     \"eqp\": \"EQP_NAME\",\n",
    "#     \"tool\": \"TOOL_NAME\",\n",
    "#     \"lot\": \"LOT_ID\",\n",
    "#     \"recipeName\": \"RECIPE_NAME\"}\n",
    "\n",
    "# # 3. 获取查询条件list\n",
    "# select_condition_list = parse_dict\n",
    "\n",
    "# # 4. 指定查询表名, 根据实际情况需要修改\n",
    "# table_name = \"etl.DWD_POC_CASE_FD_UVA_DATA_TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "86c7cb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3921, 39)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = pd.read_csv(\"D:/Jupyterfiles/晶合MVAFDC_general开发/MVAanlysisDevelop/uva_algorithm/small_samples_data/small1_labeled.csv\")\n",
    "df_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "893f55d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOOL_ID</th>\n",
       "      <th>TOOL_NAME</th>\n",
       "      <th>RUN_ID</th>\n",
       "      <th>EQP_NAME</th>\n",
       "      <th>CASE_INFO</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>PRODG1</th>\n",
       "      <th>OPER_NO</th>\n",
       "      <th>LOT_ID</th>\n",
       "      <th>WAFER_ID</th>\n",
       "      <th>...</th>\n",
       "      <th>UPPER_OUTLIER</th>\n",
       "      <th>RULES_ENABLED</th>\n",
       "      <th>ALARM_RULE</th>\n",
       "      <th>RESULT</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>REGION</th>\n",
       "      <th>ERROR_MSG</th>\n",
       "      <th>STATISTIC_RESULT</th>\n",
       "      <th>VERSION</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6739</td>\n",
       "      <td>SCT07_4-1</td>\n",
       "      <td>1516389</td>\n",
       "      <td>SCT07</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>2U.WSC50</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.495300</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.495300</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6739</td>\n",
       "      <td>SCT07_4-1</td>\n",
       "      <td>1516389</td>\n",
       "      <td>SCT07</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>2U.WSC50</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>...</td>\n",
       "      <td>100.310000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.785714</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>UPPER_NORMAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.785714</td>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6739</td>\n",
       "      <td>SCT07_4-1</td>\n",
       "      <td>1516389</td>\n",
       "      <td>SCT07</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>2U.WSC50</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>...</td>\n",
       "      <td>81.668838</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.930769</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>LOWER_NORMAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.930769</td>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6740</td>\n",
       "      <td>SCT07_4-2</td>\n",
       "      <td>1498463</td>\n",
       "      <td>SCT07</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>2U.WSC50</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>NBX293-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.272773</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>LOWER_NORMAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.272773</td>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6740</td>\n",
       "      <td>SCT07_4-2</td>\n",
       "      <td>1498463</td>\n",
       "      <td>SCT07</td>\n",
       "      <td>2023-12-08</td>\n",
       "      <td>AFPNR901N.0B0L</td>\n",
       "      <td>L2800Z2N</td>\n",
       "      <td>2U.WSC50</td>\n",
       "      <td>NBX293.200</td>\n",
       "      <td>NBX293-07</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.045500</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.045500</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3916</th>\n",
       "      <td>5444</td>\n",
       "      <td>DGA01_A1</td>\n",
       "      <td>1187372</td>\n",
       "      <td>DGA01</td>\n",
       "      <td>2023-12-16</td>\n",
       "      <td>AFPZM801N.0A01</td>\n",
       "      <td>L2800Z3N</td>\n",
       "      <td>2U.CDG20</td>\n",
       "      <td>NBX265.000</td>\n",
       "      <td>NBX265-07</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>UPPER_NORMAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>5444</td>\n",
       "      <td>DGA01_A1</td>\n",
       "      <td>1187372</td>\n",
       "      <td>DGA01</td>\n",
       "      <td>2023-12-16</td>\n",
       "      <td>AFPZM801N.0A01</td>\n",
       "      <td>L2800Z3N</td>\n",
       "      <td>2U.CDG20</td>\n",
       "      <td>NBX265.000</td>\n",
       "      <td>NBX265-07</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.185000</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.185000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>5444</td>\n",
       "      <td>DGA01_A1</td>\n",
       "      <td>1187372</td>\n",
       "      <td>DGA01</td>\n",
       "      <td>2023-12-16</td>\n",
       "      <td>AFPZM801N.0A01</td>\n",
       "      <td>L2800Z3N</td>\n",
       "      <td>2U.CDG20</td>\n",
       "      <td>NBX265.000</td>\n",
       "      <td>NBX265-07</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>No Match Found for Start criteria [ Step_Name ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3919</th>\n",
       "      <td>5444</td>\n",
       "      <td>DGA01_A1</td>\n",
       "      <td>1187372</td>\n",
       "      <td>DGA01</td>\n",
       "      <td>2023-12-16</td>\n",
       "      <td>AFPZM801N.0A01</td>\n",
       "      <td>L2800Z3N</td>\n",
       "      <td>2U.CDG20</td>\n",
       "      <td>NBX265.000</td>\n",
       "      <td>NBX265-07</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>No Match Found for Start criteria [ Step_Numbe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3920</th>\n",
       "      <td>5444</td>\n",
       "      <td>DGA01_A1</td>\n",
       "      <td>1187372</td>\n",
       "      <td>DGA01</td>\n",
       "      <td>2023-12-16</td>\n",
       "      <td>AFPZM801N.0A01</td>\n",
       "      <td>L2800Z3N</td>\n",
       "      <td>2U.CDG20</td>\n",
       "      <td>NBX265.000</td>\n",
       "      <td>NBX265-07</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>No Match Found for Start criteria [ Step_Numbe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3921 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TOOL_ID  TOOL_NAME   RUN_ID EQP_NAME   CASE_INFO      PRODUCT_ID  \\\n",
       "0        6739  SCT07_4-1  1516389    SCT07  2023-12-08  AFPNR901N.0B0L   \n",
       "1        6739  SCT07_4-1  1516389    SCT07  2023-12-08  AFPNR901N.0B0L   \n",
       "2        6739  SCT07_4-1  1516389    SCT07  2023-12-08  AFPNR901N.0B0L   \n",
       "3        6740  SCT07_4-2  1498463    SCT07  2023-12-08  AFPNR901N.0B0L   \n",
       "4        6740  SCT07_4-2  1498463    SCT07  2023-12-08  AFPNR901N.0B0L   \n",
       "...       ...        ...      ...      ...         ...             ...   \n",
       "3916     5444   DGA01_A1  1187372    DGA01  2023-12-16  AFPZM801N.0A01   \n",
       "3917     5444   DGA01_A1  1187372    DGA01  2023-12-16  AFPZM801N.0A01   \n",
       "3918     5444   DGA01_A1  1187372    DGA01  2023-12-16  AFPZM801N.0A01   \n",
       "3919     5444   DGA01_A1  1187372    DGA01  2023-12-16  AFPZM801N.0A01   \n",
       "3920     5444   DGA01_A1  1187372    DGA01  2023-12-16  AFPZM801N.0A01   \n",
       "\n",
       "        PRODG1   OPER_NO      LOT_ID   WAFER_ID  ... UPPER_OUTLIER  \\\n",
       "0     L2800Z2N  2U.WSC50  NBX293.200  NBX293-06  ...           NaN   \n",
       "1     L2800Z2N  2U.WSC50  NBX293.200  NBX293-06  ...    100.310000   \n",
       "2     L2800Z2N  2U.WSC50  NBX293.200  NBX293-06  ...     81.668838   \n",
       "3     L2800Z2N  2U.WSC50  NBX293.200  NBX293-07  ...      0.440000   \n",
       "4     L2800Z2N  2U.WSC50  NBX293.200  NBX293-07  ...           NaN   \n",
       "...        ...       ...         ...        ...  ...           ...   \n",
       "3916  L2800Z3N  2U.CDG20  NBX265.000  NBX265-07  ...           NaN   \n",
       "3917  L2800Z3N  2U.CDG20  NBX265.000  NBX265-07  ...           NaN   \n",
       "3918  L2800Z3N  2U.CDG20  NBX265.000  NBX265-07  ...           NaN   \n",
       "3919  L2800Z3N  2U.CDG20  NBX265.000  NBX265-07  ...           NaN   \n",
       "3920  L2800Z3N  2U.CDG20  NBX265.000  NBX265-07  ...           NaN   \n",
       "\n",
       "     RULES_ENABLED ALARM_RULE     RESULT  STATUS        REGION  \\\n",
       "0                1        NaN  72.495300  NORMAL        NORMAL   \n",
       "1                1        NaN  78.785714  NORMAL  UPPER_NORMAL   \n",
       "2                1        NaN  79.930769  NORMAL  LOWER_NORMAL   \n",
       "3                1        NaN   0.272773  NORMAL  LOWER_NORMAL   \n",
       "4                1        NaN  71.045500  NORMAL        NORMAL   \n",
       "...            ...        ...        ...     ...           ...   \n",
       "3916             1        NaN   0.025200  NORMAL  UPPER_NORMAL   \n",
       "3917             1        NaN   4.185000  NORMAL        NORMAL   \n",
       "3918             1        NaN        NaN   ERROR         ERROR   \n",
       "3919             1        NaN        NaN   ERROR         ERROR   \n",
       "3920             1        NaN        NaN   ERROR         ERROR   \n",
       "\n",
       "                                              ERROR_MSG  STATISTIC_RESULT  \\\n",
       "0                                                   NaN         72.495300   \n",
       "1                                                   NaN         78.785714   \n",
       "2                                                   NaN         79.930769   \n",
       "3                                                   NaN          0.272773   \n",
       "4                                                   NaN         71.045500   \n",
       "...                                                 ...               ...   \n",
       "3916                                                NaN          0.025200   \n",
       "3917                                                NaN          4.185000   \n",
       "3918  No Match Found for Start criteria [ Step_Name ...               NaN   \n",
       "3919  No Match Found for Start criteria [ Step_Numbe...               NaN   \n",
       "3920  No Match Found for Start criteria [ Step_Numbe...               NaN   \n",
       "\n",
       "      VERSION  label  \n",
       "0           4      1  \n",
       "1         114      1  \n",
       "2         114      1  \n",
       "3         114      1  \n",
       "4           4      1  \n",
       "...       ...    ...  \n",
       "3916        6      1  \n",
       "3917        2      1  \n",
       "3918        2      1  \n",
       "3919       56      1  \n",
       "3920       56      1  \n",
       "\n",
       "[3921 rows x 39 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6aa9a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pandas[df_pandas['label'] == 0]['WAFER_ID'].unique()\n",
    "# df_pandas[df_pandas['label'] == 1]['WAFER_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4336cb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\pandas\\internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\pandas\\utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3921"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = ps.from_pandas(df_pandas).to_spark()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cf01ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####自己打标签\n",
    "############################################\n",
    "######## 1. 客户只定义了bad_wafer = []是什么  ########\n",
    "############################################\n",
    "# 将传进来的BAD_WAFER, 用 | 连接起来，\n",
    "# F.col('WAFER_ID').like('NDJ065%') | F.col('WAFER_ID').like('NDJ067%') 作为条件传入增加label\n",
    "# 同时将isin模式也作为条件传入增加label\n",
    "\n",
    "def get_label_single(df, bad_wafer):\n",
    "    like_conditions = [f\"col('WAFER_ID').like('{bad}')\" for bad in bad_wafer]\n",
    "    all_like_conditions = \" | \".join(like_conditions)\n",
    "    isin_conditions = \"col('WAFER_ID').isin(bad_wafer)\"\n",
    "    df = df.withColumn('label', \n",
    "                when( eval(all_like_conditions) | eval(isin_conditions), int(1)).otherwise(int(0)))\n",
    "    return df \n",
    "\n",
    "\n",
    "############################################\n",
    "## 2. 客户定义了bad_wafer = [] 和 good_wafer = []######\n",
    "############################################\n",
    "# 将传进来的BAD_WAFER, 用 | 连接起来，\n",
    "# 将传进来的GOOD_WAFER, 也用 | 连接起来，\n",
    "# 同时将isin模式也作为条件传入增加label\n",
    "\n",
    "def get_label_double(df, bad_wafer, good_wafer):\n",
    "    good_like_conditions = [f\"col('WAFER_ID').like('{good}')\" for good in good_wafer]\n",
    "    all_good_like_conditions = \" | \".join(good_like_conditions)\n",
    "    good_isin_conditions = \"col('WAFER_ID').isin(good_wafer)\"\n",
    "\n",
    "    bad_like_conditions = [f\"col('WAFER_ID').like('{bad}')\" for bad in bad_wafer]\n",
    "    all_bad_like_conditions = \" | \".join(bad_like_conditions)\n",
    "    bad_isin_conditions = \"col('WAFER_ID').isin(bad_wafer)\"\n",
    "\n",
    "    df = df.withColumn('label',  when(eval(all_good_like_conditions) | eval(good_isin_conditions), int(0)).when(eval(all_bad_like_conditions) | eval(bad_isin_conditions), int(1)).otherwise(222333))\n",
    "    df = df.filter(df['label'] != int(222333))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812d1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4e8accea",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "##########################融合OPER_NO字段##########################\n",
    "###################################################################\n",
    "def integrate_operno(df: pyspark.sql.dataframe,\n",
    "                     merge_operno_list: List[Dict[str, List[str]]]) -> pyspark.sql.dataframe:\n",
    "    \"\"\"\n",
    "    Integrate the 'OPER_NO' column in the DataFrame based on the provided merge_operno_list.\n",
    "    :param df: The input DataFrame.\n",
    "    :param merge_operno_list: A list of dictionaries where each dictionary contains values to be merged.\n",
    "           Example: [{'2F.CDS10_XX.TDS01': ['2F.CDS10', 'XX.TDS01']},\n",
    "                     {'2F.CDS20_XX.CDS20': ['2F.CDS20', 'XX.CDS20']}]\n",
    "    :return: DataFrame with 'OPER_NO' column integrated according to the merge_operno_list.\n",
    "    \"\"\"\n",
    "    if merge_operno_list is not None and len(merge_operno_list) > 0:\n",
    "        # Extract values from each dictionary in merge_operno_list and create a list\n",
    "        values_to_replace = [list(rule.values())[0] for rule in merge_operno_list]\n",
    "        # Concatenate values from each dictionary\n",
    "        merged_values = [\"_\".join(list(rule.values())[0]) for rule in merge_operno_list]\n",
    "\n",
    "        # Replace values in 'OPER_NO' column based on the rules defined in merge_operno_list\n",
    "        for values, replacement_value in zip(values_to_replace, merged_values):\n",
    "            df = df.withColumn(\"OPER_NO\",\n",
    "                               when(col(\"OPER_NO\").isin(values), replacement_value).otherwise(col(\"OPER_NO\")))\n",
    "        return df\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1c8dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "##################################FDC数据预处理###############################\n",
    "############################################################################\n",
    "def pre_process(df: pyspark.sql.dataframe) -> pyspark.sql.dataframe:\n",
    "    \"\"\"\n",
    "     Preprocess the data extracted from the database for a specific CASE.\n",
    "    :param df: Data for a specific CASE retrieved from the database.\n",
    "    :return: Preprocessed data with relevant columns and filters applied.\n",
    "    \"\"\"\n",
    "    # Select only the columns that will be used\n",
    "    df = df.select('WAFER_ID', 'TOOL_ID', 'RUN_ID', 'EQP_NAME', 'PRODUCT_ID', 'PRODG1', 'TOOL_NAME',\n",
    "                   'OPER_NO', 'parametric_name', 'STATISTIC_RESULT', 'label')\n",
    "    # Remove rows with missing values in 'STATISTIC_RESULT' column\n",
    "    df = df.filter(col('STATISTIC_RESULT').isNotNull())\n",
    "    # Drop duplicates based on all columns\n",
    "    df1 = df.dropDuplicates()\n",
    "    # Select the rows with the latest 'RUN_ID' for each combination of 'WAFER_ID', 'OPER_NO', 'TOOL_ID'\n",
    "    df2 = df1.groupBy('WAFER_ID', 'OPER_NO', 'TOOL_ID').agg(max('RUN_ID').alias('RUN_ID'))\n",
    "    df_run = df1.join(df2.dropDuplicates(subset=['WAFER_ID', 'OPER_NO', 'TOOL_ID', 'RUN_ID']),\n",
    "                      on=['WAFER_ID', 'OPER_NO', 'TOOL_ID', 'RUN_ID'], how='inner')\n",
    "    return df_run\n",
    "\n",
    "\n",
    "\n",
    "def commonality_analysis(df_run: pyspark.sql.dataframe, grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "    \"\"\"\n",
    "    Perform commonality analysis on preprocessed data.\n",
    "    :param df_run: Preprocessed data after data preprocessing.\n",
    "    :param grpby_list: List of columns ['PRODG1', 'EQP_NAME', 'OPER_NO', 'PRODUCT_ID', 'TOOL_NAME'] for grouping.\n",
    "            Example: grpby_list = ['PRODG1', 'TOOL_NAME', 'OPER_NO'], grpby_list = ['PRODUCT_ID', 'OPER_NO']\n",
    "    :return: Results of commonality analysis, showing the top ten combinations with the highest number of bad wafers.\n",
    "    \"\"\"\n",
    "    grps = (df_run.groupBy(grpby_list)\n",
    "            .agg(countDistinct('WAFER_ID').alias('wafer_count'),\n",
    "                 countDistinct('WAFER_ID', when(df_run['label'] == 0, 1)).alias('good_num'),\n",
    "                 countDistinct('WAFER_ID', when(df_run['label'] == 1, 1)).alias('bad_num'))\n",
    "            .orderBy('bad_num', ascending=False))\n",
    "\n",
    "    # Handle the case of a single OPER_NO or single TOOL_NAME\n",
    "    if grps.count() == 1:\n",
    "        return grps\n",
    "    else:\n",
    "        # Filter out groups with no bad wafers\n",
    "        grps = grps.filter(grps['bad_num'] > 0)\n",
    "        # Rank the groups based on the number of bad wafers\n",
    "        window_sep = Window().orderBy(col(\"bad_num\").desc())\n",
    "        ranked_df = grps.withColumn(\"rank\", rank().over(window_sep))\n",
    "        # Select the top ten groups and remove the 'rank' column\n",
    "        grpss = ranked_df.filter(col(\"rank\") <= 10).drop(\"rank\")\n",
    "        return grpss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ba0ae6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2769\n"
     ]
    }
   ],
   "source": [
    "df_run = pre_process(df1)\n",
    "print(df_run.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8815d5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EQP_NAME', 'OPER_NO', 'TOOL_NAME']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grpby_list = ['EQP_NAME', 'OPER_NO', 'TOOL_NAME']\n",
    "grpby_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9344e68d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+-----------+--------+-------+\n",
      "|EQP_NAME| OPER_NO|      TOOL_NAME|wafer_count|good_num|bad_num|\n",
      "+--------+--------+---------------+-----------+--------+-------+\n",
      "|   EUT0B|1V.EEU10|      EUT0B_PM6|          5|       0|      5|\n",
      "|   MTA03|2U.CMT10|        MTA03_4|          4|       0|      4|\n",
      "|   WTL0A|1V.WWK20|       WTL0A_L1|          2|       0|      2|\n",
      "|   DGA03|2U.CDG10|       DGA03_C1|          2|       0|      2|\n",
      "|   PBT01|1V.PPB10|PBT01_CGHA_4-14|          2|       0|      2|\n",
      "|   DGA03|2U.CDG10|       DGA03_C2|          2|       0|      2|\n",
      "|   MTN01|2U.CMT20|        MTN01_6|          2|       0|      2|\n",
      "|   SCT06|2U.WSC50|      SCT06_5-3|          2|       0|      2|\n",
      "|   DBA71|2U.CDB10|       DBA71_B1|          2|       0|      2|\n",
      "|   SCT07|2U.WSC40|      SCT07_4-4|          2|       0|      2|\n",
      "|   DBA51|2U.CDB20|       DBA51_B2|          2|       0|      2|\n",
      "|   SCT07|2U.WSC10|      SCT07_4-3|          2|       0|      2|\n",
      "|   MTN52|2U.CMT20|        MTN52_1|          2|       0|      2|\n",
      "|   PBT01|1V.PPB10|PBT01_CGHA_4-13|          2|       0|      2|\n",
      "|   MTA03|2U.CMT10|        MTA03_E|          2|       0|      2|\n",
      "|   SCT07|2U.WSC40|      SCT07_4-1|          2|       0|      2|\n",
      "|   DAA03|2U.CDA10|       DAA03_B2|          2|       0|      2|\n",
      "|   MTA03|2U.CMT10|        MTA03_F|          2|       0|      2|\n",
      "|   SCT07|2U.WSC20|      SCT07_5-4|          2|       0|      2|\n",
      "|   DBA71|2U.CDB10|       DBA71_A1|          2|       0|      2|\n",
      "+--------+--------+---------------+-----------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "common_res = commonality_analysis(df_run, grpby_list)\n",
    "common_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267d955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "501fa47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#################################获取样本数据#########################\n",
    "############################################################################\n",
    "def get_data_list(common_res: pyspark.sql.dataframe,\n",
    "                  grpby_list: List[str],\n",
    "                  big_or_small: str = 'big') -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Get a list of dictionaries for corresponding groups based on commonality analysis.\n",
    "\n",
    "    :param common_res: Result of commonality analysis.\n",
    "    :param grpby_list:  List of columns ['PRODG1', 'EQP_NAME', 'OPER_NO', 'PRODUCT_ID', 'TOOL_NAME'] for grouping.\n",
    "    :param big_or_small: 'big' or 'small'.\n",
    "    :return: List of dictionaries for corresponding groups.\n",
    "            Example: [{'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFKN2J01N.0U01'},\n",
    "                      {'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFKN4X01N.0B01'},\n",
    "                      {'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFGN1501N.0C02'}]\n",
    "    \"\"\"\n",
    "    assert big_or_small in ['big', 'small'], \"Choose only 'big' or 'small'. Please check the spelling.\"\n",
    "\n",
    "    # Filter groups based on big or small sample conditions\n",
    "    if big_or_small == 'big':\n",
    "        good_bad_grps = common_res.filter(\"good_num >= 3 AND bad_num >= 3\")\n",
    "    else:\n",
    "        good_bad_grps = common_res.filter(\"bad_num >= 1 AND wafer_count >= 2\")\n",
    "\n",
    "    # Order the results and limit to the top 10 groups\n",
    "    good_bad_grps = good_bad_grps.orderBy(col(\"bad_num\").desc(), col(\"wafer_count\").desc(),\n",
    "                                          col(\"good_num\").desc()).limit(10)\n",
    "\n",
    "    # Collect the data and convert it into a list of dictionaries\n",
    "    data_list = good_bad_grps[grpby_list].collect()\n",
    "    data_dict_list = [row.asDict() for row in data_list]\n",
    "    return data_dict_list\n",
    "\n",
    "\n",
    "def get_train_data(df_run: pyspark.sql.dataframe, data_dict_list: List[Dict[str, str]]) -> pyspark.sql.dataframe:\n",
    "    \"\"\"\n",
    "    Get the actual combination data for modeling from the original data.\n",
    "\n",
    "    :param df_run: Preprocessed data after data preprocessing.\n",
    "    :param data_dict_list: List of dictionaries with filtering conditions.\n",
    "           Example: [{'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFKN2J01N.0U01'},\n",
    "                      {'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFKN4X01N.0B01'},\n",
    "                      {'OPER_NO': '1F.EEK10', 'PRODUCT_ID': 'AFGN1501N.0C02'}]\n",
    "    :return: Filtered data for modeling.\n",
    "    \"\"\"\n",
    "    # Get the filtering conditions for the first data dictionary\n",
    "    first_data_dict = data_dict_list[0]\n",
    "    conditions = \" AND \".join([\"{} == '{}'\".format(col_, first_data_dict[col_]) for col_ in first_data_dict])\n",
    "    # Filter the data for the first condition\n",
    "    df_s = df_run.filter(conditions)\n",
    "\n",
    "    # Loop through the remaining data dictionaries and filter the data accordingly\n",
    "    for i in range(1, len(data_dict_list)):\n",
    "        data_dict = data_dict_list[i]\n",
    "        conditions = \" AND \".join([\"{} == '{}'\".format(col_, data_dict[col_]) for col_ in data_dict])\n",
    "        df_m = df_run.filter(conditions)\n",
    "        df_s = df_s.union(df_m)\n",
    "    return df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4a7e6595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'EQP_NAME': 'EUT0B', 'OPER_NO': '1V.EEU10', 'TOOL_NAME': 'EUT0B_PM6'},\n",
       " {'EQP_NAME': 'MTA03', 'OPER_NO': '2U.CMT10', 'TOOL_NAME': 'MTA03_4'},\n",
       " {'EQP_NAME': 'WTL0A', 'OPER_NO': '1V.WWK20', 'TOOL_NAME': 'WTL0A_L1'},\n",
       " {'EQP_NAME': 'DGA03', 'OPER_NO': '2U.CDG10', 'TOOL_NAME': 'DGA03_C1'},\n",
       " {'EQP_NAME': 'DGA03', 'OPER_NO': '2U.CDG10', 'TOOL_NAME': 'DGA03_C2'},\n",
       " {'EQP_NAME': 'SCT06', 'OPER_NO': '2U.WSC50', 'TOOL_NAME': 'SCT06_5-3'},\n",
       " {'EQP_NAME': 'PBT01', 'OPER_NO': '1V.PPB10', 'TOOL_NAME': 'PBT01_CGHA_4-14'},\n",
       " {'EQP_NAME': 'MTN01', 'OPER_NO': '2U.CMT20', 'TOOL_NAME': 'MTN01_6'},\n",
       " {'EQP_NAME': 'DBA71', 'OPER_NO': '2U.CDB10', 'TOOL_NAME': 'DBA71_B1'},\n",
       " {'EQP_NAME': 'SCT07', 'OPER_NO': '2U.WSC40', 'TOOL_NAME': 'SCT07_4-4'}]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict_list_ss = get_data_list(common_res, grpby_list, big_or_small='small')\n",
    "data_dict_list_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ae81e8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_run_ss = get_train_data(df_run, data_dict_list_ss)\n",
    "df_run_ss.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4b3f6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#########################获取传入的整个数据中的所有bad_wafer个数############\n",
    "############################################################################\n",
    "def get_all_bad_wafer_num(df: pyspark.sql.dataframe) -> int:\n",
    "    \"\"\"\n",
    "    Get the number of distinct bad WAFER in the DataFrame.\n",
    "    \"\"\"\n",
    "    return df.filter(\"label == 1\").select('WAFER_ID').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f7b0b5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_wafer_num_small_sample = get_all_bad_wafer_num(df_run_ss)\n",
    "bad_wafer_num_small_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c62bf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3ebb3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_run_ss_pandas = df_run_ss.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a402daa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAFER_ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NBX265-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NBX293-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    WAFER_ID  label\n",
       "0  NBX265-06      1\n",
       "1  NBX293-06      1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 6\n",
    "oper, tool = data_dict_list_ss[i]['OPER_NO'], data_dict_list_ss[i]['TOOL_NAME']\n",
    "df_run_ss_pandas1 = df_run_ss_pandas.query(F\"OPER_NO == '{oper}' & TOOL_NAME == '{tool}'\")\n",
    "df_pivot = get_pivot_table(df=df_run_ss_pandas1, grpby_list=grpby_list)\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "298fcdd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_pca_pandas(df_run):\n",
    "    df_pivot = get_pivot_table(df=df_run, grpby_list=grpby_list)\n",
    "    # 由于是小样本，再重新copy一份制造多一点数据传给PCA模型\n",
    "    df_pivot_copy = df_pivot.copy()\n",
    "    df_pivot_all = pd.concat([df_pivot, df_pivot_copy], axis=0)\n",
    "\n",
    "    # 定义自变量\n",
    "    x_train = df_pivot_all[df_pivot_all.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "    print(x_train.shape)\n",
    "    \n",
    "    if min(x_train.shape) <= 0:\n",
    "        return None\n",
    "\n",
    "    n_components = min(min(x_train.shape) - 2, 20)\n",
    "    model = pca(n_components=n_components, verbose=None)\n",
    "    results = model.fit_transform(x_train)\n",
    "    res_top = results['topfeat']\n",
    "    res_top_select = res_top[res_top['type'] == 'best'][['feature', 'loading']]\n",
    "    res_top_select['importance'] = abs(res_top_select['loading'])\n",
    "    res_top_select = res_top_select.rename(columns={'feature': 'features'}).drop(\"loading\", axis=1).drop_duplicates()\n",
    "\n",
    "    # 增加一些字段信息\n",
    "    res_top_select['bad_wafer'] = sum(df_pivot['label'])\n",
    "    for col_ in grpby_list:\n",
    "        res_top_select[col_] = df_run[col_].values[0]\n",
    "    return res_top_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c901ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 6)\n",
      "(8, 18)\n",
      "(4, 19)\n",
      "(4, 30)\n",
      "(4, 31)\n",
      "(4, 17)\n",
      "(4, 0)\n",
      "(4, 5)\n",
      "(4, 11)\n",
      "(4, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yang.wenjun\\AppData\\Roaming\\Python\\Python39\\site-packages\\pca\\pca.py:1151: RuntimeWarning: divide by zero encountered in divide\n",
      "  rad_cc = (xct**2 / (width / 2.)**2) + (yct**2 / (height / 2.)**2)\n",
      "C:\\Users\\yang.wenjun\\AppData\\Roaming\\Python\\Python39\\site-packages\\pca\\pca.py:1142: RuntimeWarning: invalid value encountered in sqrt\n",
      "  width, height = 2 * n_std * np.sqrt(vals)\n",
      "C:\\Users\\yang.wenjun\\AppData\\Roaming\\Python\\Python39\\site-packages\\pca\\pca.py:1142: RuntimeWarning: invalid value encountered in sqrt\n",
      "  width, height = 2 * n_std * np.sqrt(vals)\n",
      "C:\\Users\\yang.wenjun\\AppData\\Roaming\\Python\\Python39\\site-packages\\pca\\pca.py:1142: RuntimeWarning: invalid value encountered in sqrt\n",
      "  width, height = 2 * n_std * np.sqrt(vals)\n",
      "C:\\Users\\yang.wenjun\\AppData\\Roaming\\Python\\Python39\\site-packages\\pca\\pca.py:1142: RuntimeWarning: invalid value encountered in sqrt\n",
      "  width, height = 2 * n_std * np.sqrt(vals)\n"
     ]
    }
   ],
   "source": [
    "columns = ['features', 'importance', 'bad_wafer', 'EQP_NAME', 'OPER_NO', 'TOOL_NAME']\n",
    "empty_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for i in range(len(data_dict_list_ss)):\n",
    "    oper, tool = data_dict_list_ss[i]['OPER_NO'], data_dict_list_ss[i]['TOOL_NAME']\n",
    "    df_run_ss_pandas1 = df_run_ss_pandas.query(F\"OPER_NO == '{oper}' & TOOL_NAME == '{tool}'\")\n",
    "    resss = get_pca_pandas(df_run_ss_pandas1)\n",
    "    empty_df = pd.concat([empty_df, resss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "edf49db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "      <th>bad_wafer</th>\n",
       "      <th>EQP_NAME</th>\n",
       "      <th>OPER_NO</th>\n",
       "      <th>TOOL_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS1...</td>\n",
       "      <td>0.862031</td>\n",
       "      <td>5</td>\n",
       "      <td>EUT0B</td>\n",
       "      <td>1V.EEU10</td>\n",
       "      <td>EUT0B_PM6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS1...</td>\n",
       "      <td>0.862479</td>\n",
       "      <td>5</td>\n",
       "      <td>EUT0B</td>\n",
       "      <td>1V.EEU10</td>\n",
       "      <td>EUT0B_PM6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS9...</td>\n",
       "      <td>0.771763</td>\n",
       "      <td>5</td>\n",
       "      <td>EUT0B</td>\n",
       "      <td>1V.EEU10</td>\n",
       "      <td>EUT0B_PM6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS1...</td>\n",
       "      <td>0.755538</td>\n",
       "      <td>5</td>\n",
       "      <td>EUT0B</td>\n",
       "      <td>1V.EEU10</td>\n",
       "      <td>EUT0B_PM6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#KITS_L...</td>\n",
       "      <td>0.707122</td>\n",
       "      <td>4</td>\n",
       "      <td>MTA03</td>\n",
       "      <td>2U.CMT10</td>\n",
       "      <td>MTA03_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#DC_VOL...</td>\n",
       "      <td>0.753177</td>\n",
       "      <td>4</td>\n",
       "      <td>MTA03</td>\n",
       "      <td>2U.CMT10</td>\n",
       "      <td>MTA03_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#DC_VOL...</td>\n",
       "      <td>0.759910</td>\n",
       "      <td>4</td>\n",
       "      <td>MTA03</td>\n",
       "      <td>2U.CMT10</td>\n",
       "      <td>MTA03_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#TARGET...</td>\n",
       "      <td>0.706517</td>\n",
       "      <td>4</td>\n",
       "      <td>MTA03</td>\n",
       "      <td>2U.CMT10</td>\n",
       "      <td>MTA03_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#CHAMBE...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>MTA03</td>\n",
       "      <td>2U.CMT10</td>\n",
       "      <td>MTA03_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#CHAMBE...</td>\n",
       "      <td>0.999002</td>\n",
       "      <td>4</td>\n",
       "      <td>MTA03</td>\n",
       "      <td>2U.CMT10</td>\n",
       "      <td>MTA03_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STATISTIC_RESULT#WTL0A#1V.WWK20#WTL0A_L1#FPM_L...</td>\n",
       "      <td>0.923938</td>\n",
       "      <td>2</td>\n",
       "      <td>WTL0A</td>\n",
       "      <td>1V.WWK20</td>\n",
       "      <td>WTL0A_L1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STATISTIC_RESULT#WTL0A#1V.WWK20#WTL0A_L1#FPM_P...</td>\n",
       "      <td>0.871927</td>\n",
       "      <td>2</td>\n",
       "      <td>WTL0A</td>\n",
       "      <td>1V.WWK20</td>\n",
       "      <td>WTL0A_L1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C1#TOTAL...</td>\n",
       "      <td>0.853963</td>\n",
       "      <td>2</td>\n",
       "      <td>DGA03</td>\n",
       "      <td>2U.CDG10</td>\n",
       "      <td>DGA03_C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C1#TOTAL...</td>\n",
       "      <td>0.853971</td>\n",
       "      <td>2</td>\n",
       "      <td>DGA03</td>\n",
       "      <td>2U.CDG10</td>\n",
       "      <td>DGA03_C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C2#TOTAL...</td>\n",
       "      <td>0.853962</td>\n",
       "      <td>2</td>\n",
       "      <td>DGA03</td>\n",
       "      <td>2U.CDG10</td>\n",
       "      <td>DGA03_C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C2#TOTAL...</td>\n",
       "      <td>0.853971</td>\n",
       "      <td>2</td>\n",
       "      <td>DGA03</td>\n",
       "      <td>2U.CDG10</td>\n",
       "      <td>DGA03_C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STATISTIC_RESULT#SCT06#2U.WSC50#SCT06_5-3#CHUC...</td>\n",
       "      <td>0.787959</td>\n",
       "      <td>2</td>\n",
       "      <td>SCT06</td>\n",
       "      <td>2U.WSC50</td>\n",
       "      <td>SCT06_5-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STATISTIC_RESULT#SCT06#2U.WSC50#SCT06_5-3#CHUC...</td>\n",
       "      <td>0.912790</td>\n",
       "      <td>2</td>\n",
       "      <td>SCT06</td>\n",
       "      <td>2U.WSC50</td>\n",
       "      <td>SCT06_5-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STATISTIC_RESULT#MTN01#2U.CMT20#MTN01_6#HEATER...</td>\n",
       "      <td>0.717768</td>\n",
       "      <td>2</td>\n",
       "      <td>MTN01</td>\n",
       "      <td>2U.CMT20</td>\n",
       "      <td>MTN01_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STATISTIC_RESULT#MTN01#2U.CMT20#MTN01_6#HEATER...</td>\n",
       "      <td>0.822787</td>\n",
       "      <td>2</td>\n",
       "      <td>MTN01</td>\n",
       "      <td>2U.CMT20</td>\n",
       "      <td>MTN01_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STATISTIC_RESULT#DBA71#2U.CDB10#DBA71_B1#UV_LA...</td>\n",
       "      <td>0.772767</td>\n",
       "      <td>2</td>\n",
       "      <td>DBA71</td>\n",
       "      <td>2U.CDB10</td>\n",
       "      <td>DBA71_B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STATISTIC_RESULT#DBA71#2U.CDB10#DBA71_B1#HEATE...</td>\n",
       "      <td>0.984344</td>\n",
       "      <td>2</td>\n",
       "      <td>DBA71</td>\n",
       "      <td>2U.CDB10</td>\n",
       "      <td>DBA71_B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STATISTIC_RESULT#SCT07#2U.WSC40#SCT07_4-4#CHUC...</td>\n",
       "      <td>0.987616</td>\n",
       "      <td>2</td>\n",
       "      <td>SCT07</td>\n",
       "      <td>2U.WSC40</td>\n",
       "      <td>SCT07_4-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STATISTIC_RESULT#SCT07#2U.WSC40#SCT07_4-4#CHUC...</td>\n",
       "      <td>0.996731</td>\n",
       "      <td>2</td>\n",
       "      <td>SCT07</td>\n",
       "      <td>2U.WSC40</td>\n",
       "      <td>SCT07_4-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  importance bad_wafer  \\\n",
       "0  STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS1...    0.862031         5   \n",
       "1  STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS1...    0.862479         5   \n",
       "2  STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS9...    0.771763         5   \n",
       "3  STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS1...    0.755538         5   \n",
       "0  STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#KITS_L...    0.707122         4   \n",
       "1  STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#DC_VOL...    0.753177         4   \n",
       "2  STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#DC_VOL...    0.759910         4   \n",
       "3  STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#TARGET...    0.706517         4   \n",
       "4  STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#CHAMBE...    1.000000         4   \n",
       "5  STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#CHAMBE...    0.999002         4   \n",
       "0  STATISTIC_RESULT#WTL0A#1V.WWK20#WTL0A_L1#FPM_L...    0.923938         2   \n",
       "1  STATISTIC_RESULT#WTL0A#1V.WWK20#WTL0A_L1#FPM_P...    0.871927         2   \n",
       "0  STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C1#TOTAL...    0.853963         2   \n",
       "1  STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C1#TOTAL...    0.853971         2   \n",
       "0  STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C2#TOTAL...    0.853962         2   \n",
       "1  STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C2#TOTAL...    0.853971         2   \n",
       "0  STATISTIC_RESULT#SCT06#2U.WSC50#SCT06_5-3#CHUC...    0.787959         2   \n",
       "1  STATISTIC_RESULT#SCT06#2U.WSC50#SCT06_5-3#CHUC...    0.912790         2   \n",
       "0  STATISTIC_RESULT#MTN01#2U.CMT20#MTN01_6#HEATER...    0.717768         2   \n",
       "1  STATISTIC_RESULT#MTN01#2U.CMT20#MTN01_6#HEATER...    0.822787         2   \n",
       "0  STATISTIC_RESULT#DBA71#2U.CDB10#DBA71_B1#UV_LA...    0.772767         2   \n",
       "1  STATISTIC_RESULT#DBA71#2U.CDB10#DBA71_B1#HEATE...    0.984344         2   \n",
       "0  STATISTIC_RESULT#SCT07#2U.WSC40#SCT07_4-4#CHUC...    0.987616         2   \n",
       "1  STATISTIC_RESULT#SCT07#2U.WSC40#SCT07_4-4#CHUC...    0.996731         2   \n",
       "\n",
       "  EQP_NAME   OPER_NO  TOOL_NAME  \n",
       "0    EUT0B  1V.EEU10  EUT0B_PM6  \n",
       "1    EUT0B  1V.EEU10  EUT0B_PM6  \n",
       "2    EUT0B  1V.EEU10  EUT0B_PM6  \n",
       "3    EUT0B  1V.EEU10  EUT0B_PM6  \n",
       "0    MTA03  2U.CMT10    MTA03_4  \n",
       "1    MTA03  2U.CMT10    MTA03_4  \n",
       "2    MTA03  2U.CMT10    MTA03_4  \n",
       "3    MTA03  2U.CMT10    MTA03_4  \n",
       "4    MTA03  2U.CMT10    MTA03_4  \n",
       "5    MTA03  2U.CMT10    MTA03_4  \n",
       "0    WTL0A  1V.WWK20   WTL0A_L1  \n",
       "1    WTL0A  1V.WWK20   WTL0A_L1  \n",
       "0    DGA03  2U.CDG10   DGA03_C1  \n",
       "1    DGA03  2U.CDG10   DGA03_C1  \n",
       "0    DGA03  2U.CDG10   DGA03_C2  \n",
       "1    DGA03  2U.CDG10   DGA03_C2  \n",
       "0    SCT06  2U.WSC50  SCT06_5-3  \n",
       "1    SCT06  2U.WSC50  SCT06_5-3  \n",
       "0    MTN01  2U.CMT20    MTN01_6  \n",
       "1    MTN01  2U.CMT20    MTN01_6  \n",
       "0    DBA71  2U.CDB10   DBA71_B1  \n",
       "1    DBA71  2U.CDB10   DBA71_B1  \n",
       "0    SCT07  2U.WSC40  SCT07_4-4  \n",
       "1    SCT07  2U.WSC40  SCT07_4-4  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f6a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3539c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "#######################################对bad>=1的数据，用pca建模##############################\n",
    "##########################################################################################\n",
    "def get_pivot_table(df: pd.DataFrame, grpby_list: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pivot the DataFrame based on specified grouping columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Data for modeling.\n",
    "    - grpby_list: List of grouping columns.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Result of pivoting the table.\n",
    "    \"\"\"\n",
    "    index_cols = ['WAFER_ID', 'label']\n",
    "    columns_cols = grpby_list + ['parametric_name']\n",
    "    df_pivot = df.dropna(axis=0).pivot_table(index=index_cols,\n",
    "                                             columns=columns_cols,\n",
    "                                             values=['STATISTIC_RESULT'])\n",
    "    df_pivot.columns = df_pivot.columns.map('#'.join)\n",
    "    df_pivot = df_pivot.fillna(df_pivot.mean()).reset_index(drop=False)\n",
    "    \n",
    "    # Remove completely identical columns\n",
    "    for column in df_pivot.columns.difference(index_cols):\n",
    "        if df_pivot[column].nunique() == 1:\n",
    "            df_pivot = df_pivot.drop(column, axis=1)\n",
    "    return df_pivot\n",
    "\n",
    "\n",
    "def fit_pca_small_sample(df: pyspark.sql.dataframe, grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "    \"\"\"\n",
    "    Fit a PCA model on the train data. It is for small sample method(bad_wafer_num >= 1 AND wafer_count >= 2)\n",
    "\n",
    "    Parameters:\n",
    "    - df: Data for modeling.\n",
    "    - grpby_list: List of grouping columns.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Combined dataframe of every feature and its importance in each combination of grpby_list after PCA modeling.\n",
    "    \"\"\"\n",
    "    # Dynamically build schema according to the grpby_list\n",
    "    struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "    struct_fields.extend([StructField(\"features\", StringType(), True),\n",
    "                          StructField(\"importance\", FloatType(), True),\n",
    "                          StructField(\"bad_wafer\", IntegerType(), True)])\n",
    "    schema_all = StructType(struct_fields)\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_model_result(df_run):\n",
    "        df_pivot = get_pivot_table(df=df_run, grpby_list=grpby_list)\n",
    "        # 由于是小样本，再重新copy一份制造多一点数据传给PCA模型\n",
    "        df_pivot_copy = df_pivot.copy()\n",
    "        df_pivot_all = pd.concat([df_pivot, df_pivot_copy], axis=0)\n",
    "\n",
    "        # 定义自变量\n",
    "        x_train = df_pivot_all[df_pivot_all.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "        if min(x_train.shape) <= 0:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        n_components = min(min(x_train.shape) - 2, 20)\n",
    "        model = pca(n_components=n_components, verbose=None)\n",
    "        results = model.fit_transform(x_train)\n",
    "        res_top = results['topfeat']\n",
    "        res_top_select = res_top[res_top['type'] == 'best'][['feature', 'loading']]\n",
    "        res_top_select['importance'] = abs(res_top_select['loading'])\n",
    "        res_top_select = res_top_select.rename(columns={'feature': 'features'}).drop(\"loading\", axis=1).drop_duplicates()\n",
    "\n",
    "        # 增加一些字段信息\n",
    "        res_top_select['bad_wafer'] = sum(df_pivot['label'])\n",
    "        for col_ in grpby_list:\n",
    "            res_top_select[col_] = df_run[col_].values[0]\n",
    "        return res_top_select\n",
    "    return df.groupby(grpby_list).apply(get_model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fb20081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\sql\\pandas\\group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+--------------------+----------+---------+\n",
      "|EQP_NAME| OPER_NO|TOOL_NAME|            features|importance|bad_wafer|\n",
      "+--------+--------+---------+--------------------+----------+---------+\n",
      "|   DBA71|2U.CDB10| DBA71_B1|STATISTIC_RESULT#...|0.77276725|        2|\n",
      "|   DBA71|2U.CDB10| DBA71_B1|STATISTIC_RESULT#...| 0.9843439|        2|\n",
      "|   DGA03|2U.CDG10| DGA03_C1|STATISTIC_RESULT#...|0.85396254|        2|\n",
      "|   DGA03|2U.CDG10| DGA03_C1|STATISTIC_RESULT#...| 0.8539707|        2|\n",
      "|   DGA03|2U.CDG10| DGA03_C2|STATISTIC_RESULT#...| 0.8539624|        2|\n",
      "|   DGA03|2U.CDG10| DGA03_C2|STATISTIC_RESULT#...|0.85397077|        2|\n",
      "|   EUT0B|1V.EEU10|EUT0B_PM6|STATISTIC_RESULT#...|0.86203057|        5|\n",
      "|   EUT0B|1V.EEU10|EUT0B_PM6|STATISTIC_RESULT#...|0.86247903|        5|\n",
      "|   EUT0B|1V.EEU10|EUT0B_PM6|STATISTIC_RESULT#...| 0.7717628|        5|\n",
      "|   EUT0B|1V.EEU10|EUT0B_PM6|STATISTIC_RESULT#...| 0.7555381|        5|\n",
      "|   MTA03|2U.CMT10|  MTA03_4|STATISTIC_RESULT#...| 0.7071218|        4|\n",
      "|   MTA03|2U.CMT10|  MTA03_4|STATISTIC_RESULT#...| 0.7531774|        4|\n",
      "|   MTA03|2U.CMT10|  MTA03_4|STATISTIC_RESULT#...|0.75990975|        4|\n",
      "|   MTA03|2U.CMT10|  MTA03_4|STATISTIC_RESULT#...| 0.7065169|        4|\n",
      "|   MTA03|2U.CMT10|  MTA03_4|STATISTIC_RESULT#...|0.99999154|        4|\n",
      "|   MTA03|2U.CMT10|  MTA03_4|STATISTIC_RESULT#...| 0.9848854|        4|\n",
      "|   MTN01|2U.CMT20|  MTN01_6|STATISTIC_RESULT#...| 0.7177681|        2|\n",
      "|   MTN01|2U.CMT20|  MTN01_6|STATISTIC_RESULT#...| 0.8227868|        2|\n",
      "|   SCT06|2U.WSC50|SCT06_5-3|STATISTIC_RESULT#...| 0.7879586|        2|\n",
      "|   SCT06|2U.WSC50|SCT06_5-3|STATISTIC_RESULT#...|0.91286933|        2|\n",
      "+--------+--------+---------+--------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = fit_pca_small_sample(df=df_run_ss, grpby_list=grpby_list)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7ed1add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resppp = res.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "506446a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EQP_NAME</th>\n",
       "      <th>OPER_NO</th>\n",
       "      <th>TOOL_NAME</th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "      <th>bad_wafer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DBA71</td>\n",
       "      <td>2U.CDB10</td>\n",
       "      <td>DBA71_B1</td>\n",
       "      <td>STATISTIC_RESULT#DBA71#2U.CDB10#DBA71_B1#UV_LA...</td>\n",
       "      <td>0.772767</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DBA71</td>\n",
       "      <td>2U.CDB10</td>\n",
       "      <td>DBA71_B1</td>\n",
       "      <td>STATISTIC_RESULT#DBA71#2U.CDB10#DBA71_B1#HEATE...</td>\n",
       "      <td>0.984344</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DGA03</td>\n",
       "      <td>2U.CDG10</td>\n",
       "      <td>DGA03_C1</td>\n",
       "      <td>STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C1#TOTAL...</td>\n",
       "      <td>0.853963</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DGA03</td>\n",
       "      <td>2U.CDG10</td>\n",
       "      <td>DGA03_C1</td>\n",
       "      <td>STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C1#TOTAL...</td>\n",
       "      <td>0.853971</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DGA03</td>\n",
       "      <td>2U.CDG10</td>\n",
       "      <td>DGA03_C2</td>\n",
       "      <td>STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C2#TOTAL...</td>\n",
       "      <td>0.853962</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DGA03</td>\n",
       "      <td>2U.CDG10</td>\n",
       "      <td>DGA03_C2</td>\n",
       "      <td>STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C2#TOTAL...</td>\n",
       "      <td>0.853971</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EUT0B</td>\n",
       "      <td>1V.EEU10</td>\n",
       "      <td>EUT0B_PM6</td>\n",
       "      <td>STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS1...</td>\n",
       "      <td>0.862031</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EUT0B</td>\n",
       "      <td>1V.EEU10</td>\n",
       "      <td>EUT0B_PM6</td>\n",
       "      <td>STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS1...</td>\n",
       "      <td>0.862479</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EUT0B</td>\n",
       "      <td>1V.EEU10</td>\n",
       "      <td>EUT0B_PM6</td>\n",
       "      <td>STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS9...</td>\n",
       "      <td>0.771763</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EUT0B</td>\n",
       "      <td>1V.EEU10</td>\n",
       "      <td>EUT0B_PM6</td>\n",
       "      <td>STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS1...</td>\n",
       "      <td>0.755538</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MTA03</td>\n",
       "      <td>2U.CMT10</td>\n",
       "      <td>MTA03_4</td>\n",
       "      <td>STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#KITS_L...</td>\n",
       "      <td>0.707122</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MTA03</td>\n",
       "      <td>2U.CMT10</td>\n",
       "      <td>MTA03_4</td>\n",
       "      <td>STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#DC_VOL...</td>\n",
       "      <td>0.753177</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MTA03</td>\n",
       "      <td>2U.CMT10</td>\n",
       "      <td>MTA03_4</td>\n",
       "      <td>STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#DC_VOL...</td>\n",
       "      <td>0.759910</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MTA03</td>\n",
       "      <td>2U.CMT10</td>\n",
       "      <td>MTA03_4</td>\n",
       "      <td>STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#TARGET...</td>\n",
       "      <td>0.706517</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MTA03</td>\n",
       "      <td>2U.CMT10</td>\n",
       "      <td>MTA03_4</td>\n",
       "      <td>STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#CHAMBE...</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MTA03</td>\n",
       "      <td>2U.CMT10</td>\n",
       "      <td>MTA03_4</td>\n",
       "      <td>STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#CHAMBE...</td>\n",
       "      <td>0.984885</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MTN01</td>\n",
       "      <td>2U.CMT20</td>\n",
       "      <td>MTN01_6</td>\n",
       "      <td>STATISTIC_RESULT#MTN01#2U.CMT20#MTN01_6#HEATER...</td>\n",
       "      <td>0.717768</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MTN01</td>\n",
       "      <td>2U.CMT20</td>\n",
       "      <td>MTN01_6</td>\n",
       "      <td>STATISTIC_RESULT#MTN01#2U.CMT20#MTN01_6#HEATER...</td>\n",
       "      <td>0.822787</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SCT06</td>\n",
       "      <td>2U.WSC50</td>\n",
       "      <td>SCT06_5-3</td>\n",
       "      <td>STATISTIC_RESULT#SCT06#2U.WSC50#SCT06_5-3#CHUC...</td>\n",
       "      <td>0.787959</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SCT06</td>\n",
       "      <td>2U.WSC50</td>\n",
       "      <td>SCT06_5-3</td>\n",
       "      <td>STATISTIC_RESULT#SCT06#2U.WSC50#SCT06_5-3#CHUC...</td>\n",
       "      <td>0.912869</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SCT07</td>\n",
       "      <td>2U.WSC40</td>\n",
       "      <td>SCT07_4-4</td>\n",
       "      <td>STATISTIC_RESULT#SCT07#2U.WSC40#SCT07_4-4#CHUC...</td>\n",
       "      <td>0.987616</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>SCT07</td>\n",
       "      <td>2U.WSC40</td>\n",
       "      <td>SCT07_4-4</td>\n",
       "      <td>STATISTIC_RESULT#SCT07#2U.WSC40#SCT07_4-4#CHUC...</td>\n",
       "      <td>0.996731</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>WTL0A</td>\n",
       "      <td>1V.WWK20</td>\n",
       "      <td>WTL0A_L1</td>\n",
       "      <td>STATISTIC_RESULT#WTL0A#1V.WWK20#WTL0A_L1#FPM_L...</td>\n",
       "      <td>0.923938</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>WTL0A</td>\n",
       "      <td>1V.WWK20</td>\n",
       "      <td>WTL0A_L1</td>\n",
       "      <td>STATISTIC_RESULT#WTL0A#1V.WWK20#WTL0A_L1#FPM_P...</td>\n",
       "      <td>0.871927</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EQP_NAME   OPER_NO  TOOL_NAME  \\\n",
       "0     DBA71  2U.CDB10   DBA71_B1   \n",
       "1     DBA71  2U.CDB10   DBA71_B1   \n",
       "2     DGA03  2U.CDG10   DGA03_C1   \n",
       "3     DGA03  2U.CDG10   DGA03_C1   \n",
       "4     DGA03  2U.CDG10   DGA03_C2   \n",
       "5     DGA03  2U.CDG10   DGA03_C2   \n",
       "6     EUT0B  1V.EEU10  EUT0B_PM6   \n",
       "7     EUT0B  1V.EEU10  EUT0B_PM6   \n",
       "8     EUT0B  1V.EEU10  EUT0B_PM6   \n",
       "9     EUT0B  1V.EEU10  EUT0B_PM6   \n",
       "10    MTA03  2U.CMT10    MTA03_4   \n",
       "11    MTA03  2U.CMT10    MTA03_4   \n",
       "12    MTA03  2U.CMT10    MTA03_4   \n",
       "13    MTA03  2U.CMT10    MTA03_4   \n",
       "14    MTA03  2U.CMT10    MTA03_4   \n",
       "15    MTA03  2U.CMT10    MTA03_4   \n",
       "16    MTN01  2U.CMT20    MTN01_6   \n",
       "17    MTN01  2U.CMT20    MTN01_6   \n",
       "18    SCT06  2U.WSC50  SCT06_5-3   \n",
       "19    SCT06  2U.WSC50  SCT06_5-3   \n",
       "20    SCT07  2U.WSC40  SCT07_4-4   \n",
       "21    SCT07  2U.WSC40  SCT07_4-4   \n",
       "22    WTL0A  1V.WWK20   WTL0A_L1   \n",
       "23    WTL0A  1V.WWK20   WTL0A_L1   \n",
       "\n",
       "                                             features  importance  bad_wafer  \n",
       "0   STATISTIC_RESULT#DBA71#2U.CDB10#DBA71_B1#UV_LA...    0.772767          2  \n",
       "1   STATISTIC_RESULT#DBA71#2U.CDB10#DBA71_B1#HEATE...    0.984344          2  \n",
       "2   STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C1#TOTAL...    0.853963          2  \n",
       "3   STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C1#TOTAL...    0.853971          2  \n",
       "4   STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C2#TOTAL...    0.853962          2  \n",
       "5   STATISTIC_RESULT#DGA03#2U.CDG10#DGA03_C2#TOTAL...    0.853971          2  \n",
       "6   STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS1...    0.862031          5  \n",
       "7   STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS1...    0.862479          5  \n",
       "8   STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS9...    0.771763          5  \n",
       "9   STATISTIC_RESULT#EUT0B#1V.EEU10#EUT0B_PM6#GAS1...    0.755538          5  \n",
       "10  STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#KITS_L...    0.707122          4  \n",
       "11  STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#DC_VOL...    0.753177          4  \n",
       "12  STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#DC_VOL...    0.759910          4  \n",
       "13  STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#TARGET...    0.706517          4  \n",
       "14  STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#CHAMBE...    0.999992          4  \n",
       "15  STATISTIC_RESULT#MTA03#2U.CMT10#MTA03_4#CHAMBE...    0.984885          4  \n",
       "16  STATISTIC_RESULT#MTN01#2U.CMT20#MTN01_6#HEATER...    0.717768          2  \n",
       "17  STATISTIC_RESULT#MTN01#2U.CMT20#MTN01_6#HEATER...    0.822787          2  \n",
       "18  STATISTIC_RESULT#SCT06#2U.WSC50#SCT06_5-3#CHUC...    0.787959          2  \n",
       "19  STATISTIC_RESULT#SCT06#2U.WSC50#SCT06_5-3#CHUC...    0.912869          2  \n",
       "20  STATISTIC_RESULT#SCT07#2U.WSC40#SCT07_4-4#CHUC...    0.987616          2  \n",
       "21  STATISTIC_RESULT#SCT07#2U.WSC40#SCT07_4-4#CHUC...    0.996731          2  \n",
       "22  STATISTIC_RESULT#WTL0A#1V.WWK20#WTL0A_L1#FPM_L...    0.923938          2  \n",
       "23  STATISTIC_RESULT#WTL0A#1V.WWK20#WTL0A_L1#FPM_P...    0.871927          2  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85025fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d300e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8f01fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "##################################对bad>=1建模后的结果进行整合############################\n",
    "#####################################################################################\n",
    "def split_features(df: pd.DataFrame, index: int) -> str:\n",
    "    \"\"\"\n",
    "    Split the 'features' column based on the specified index.\n",
    "\n",
    "    Parameters:\n",
    "    - df: RandomForest modeling results with 'features' column.\n",
    "    - index: Order value.\n",
    "\n",
    "    Returns:\n",
    "    - str: Field attribute value.\n",
    "    \"\"\"\n",
    "    return df['features'].apply(lambda x: x.split('#')[index])\n",
    "\n",
    "\n",
    "def get_split_feature_importance_table(df: pd.DataFrame, grpby_list: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the table after splitting the 'features' column based on the specified grouping columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df: RandomForest modeling results with 'features' column.\n",
    "    - grpby_list: List of grouping columns.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Table after splitting features.\n",
    "    \"\"\"\n",
    "    n_feats = len(grpby_list)\n",
    "    for i in range(n_feats):\n",
    "        df[grpby_list[i]] = split_features(df, i + 1)\n",
    "\n",
    "    df['parametric_name'] = split_features(df, n_feats + 1)\n",
    "    df['step'] = split_features(df, n_feats + 2)\n",
    "    df['stats'] = split_features(df, n_feats + 3)\n",
    "    df = df.drop(['features'], axis=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_feature_stats(df: pd.DataFrame, grpby_list: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a column with all statistical features of parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Feature importance table after processing.\n",
    "    - grpby_list: List of grouping columns.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: New column containing all statistical features: 'feature_stats'.\n",
    "    \"\"\"\n",
    "    grpby_list_extend = grpby_list + ['parametric_name', 'step']\n",
    "    feature_stats = df.groupby(grpby_list_extend)['stats'].unique().reset_index()\n",
    "    feature_stats['stats'] = [feature_stats['stats'].iloc[i].tolist() for i in range(len(feature_stats))]\n",
    "    feature_stats['stats'] = feature_stats['stats'].apply(lambda x: \"#\".join(x))\n",
    "    feature_stats = feature_stats.assign(\n",
    "        parametric_name=lambda x: x['parametric_name'] + str('#') + x['step']).drop(\n",
    "        'step', axis=1)\n",
    "    return feature_stats\n",
    "\n",
    "\n",
    "def split_calculate_features_small_sample(df: pyspark.sql.dataframe, grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "    \"\"\"\n",
    "    param df: PCA建模后的结果\n",
    "    param by: 分组字段\n",
    "    return: features和importance结果\n",
    "    \"\"\"\n",
    "    # Dynamically build schema\n",
    "    struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "    struct_fields.extend([StructField(\"parametric_name\", StringType(), True),\n",
    "                          StructField(\"importance\", FloatType(), True),\n",
    "                          StructField(\"bad_wafer\", IntegerType(), True),\n",
    "                          StructField(\"stats\", StringType(), True),])\n",
    "    schema_all = StructType(struct_fields)\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_result(model_results):        \n",
    "        feature_importance_table = model_results[['features',  'importance', 'bad_wafer']].dropna(axis=0)\n",
    "        # 分裂features\n",
    "        feature_importance_res_split = get_split_feature_importance_table(df=feature_importance_table, grpby_list=grpby_list)\n",
    "\n",
    "        # 新增一列，含有参数的所有统计特征:feature_stats\n",
    "        feature_stats = add_feature_stats(df=feature_importance_res_split, grpby_list=grpby_list)\n",
    "\n",
    "        #对同一种组合里的同一个参数进行求和:feature_importance_groupby\n",
    "        feature_importance_groupby = (feature_importance_res_split.groupby(grpby_list + \n",
    "                                                    ['bad_wafer', 'parametric_name', 'step'])['importance'].sum().reset_index())\n",
    "        feature_importance_groupby = feature_importance_groupby.assign(parametric_name=lambda x: x['parametric_name']+str('#')+x['step']).drop('step', axis=1)\n",
    "\n",
    "        # feature_stats和feature_importance_groupby连接\n",
    "        grpby_stats = pd.merge(feature_stats, feature_importance_groupby, on=grpby_list + ['parametric_name']).dropna().reset_index(drop=True)\n",
    "        return grpby_stats\n",
    "    return df.groupby(grpby_list).apply(get_result)\n",
    "\n",
    "\n",
    "def get_finall_results_small_sample(f_res: pyspark.sql.dataframe, bad_wafer_num: int) -> pyspark.sql.dataframe:\n",
    "    \"\"\"\n",
    "    param s_res: roc_auc分数结果\n",
    "    param f_res: features和importance结果\n",
    "    param bad_wafer_num: 数据中所有bad_wafer的数量\n",
    "    return: 最后的建模结果\n",
    "    \"\"\"\n",
    "    f_res = f_res.withColumn(\"bad_ratio\", col(\"bad_wafer\") / bad_wafer_num)\n",
    "    df_merge = f_res.withColumn('weight_original', col('importance') * col('bad_ratio'))\n",
    "\n",
    "    # 最后再次进行一次归一化\n",
    "    weight_all = df_merge.agg({\"weight_original\": \"sum\"}).collect()[0][0]\n",
    "    df_merge = df_merge.withColumn(\"weight\", col(\"weight_original\") / weight_all)\n",
    "\n",
    "    df_merge = df_merge.select(grpby_list + ['parametric_name', 'weight', 'stats']).orderBy('weight', ascending=False)\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "19a7e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#############################将建模后的结果增加特定的列####################################\n",
    "#####################################################################################\n",
    "def add_certain_column(df: pyspark.sql.dataframe, by: str, request_id: str,\n",
    "                       grpby_list: List[str]) -> pyspark.sql.dataframe:\n",
    "    \"\"\"\n",
    "    Add specific columns to the final modeling results.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Final modeling result.\n",
    "    - by: Grouping column, manually add a column 'add'.\n",
    "    - request_id: Request ID passed in.\n",
    "    - grpby_list: List of grouping columns.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Final modeling result with specific columns added.\n",
    "    \"\"\"\n",
    "    # Dynamically build schema_all\n",
    "    struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "    struct_fields.extend([StructField(\"stats\", StringType(), True),\n",
    "                          StructField(\"parametric_name\", StringType(), True),\n",
    "                          StructField(\"weight\", FloatType(), True),\n",
    "                          StructField(\"request_id\", StringType(), True),\n",
    "                          StructField(\"weight_percent\", FloatType(), True),\n",
    "                          StructField(\"index_no\", IntegerType(), True)])\n",
    "    schema_all = StructType(struct_fields)\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_result(final_res: pd.DataFrame) -> pd.DataFrame:\n",
    "        final_res['weight'] = final_res['weight'].astype(float)\n",
    "        final_res = final_res.query(\"weight > 0\")\n",
    "        final_res['request_id'] = request_id\n",
    "        final_res['weight_percent'] = final_res['weight'] * 100\n",
    "        final_res = final_res.sort_values('weight', ascending=False)\n",
    "        final_res['index_no'] = [i + 1 for i in range(len(final_res))]\n",
    "        final_res = final_res.drop('add', axis=1)\n",
    "        return final_res\n",
    "    return df.groupby(grpby_list).apply(get_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "314c4e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------+---------------+--------------------+----------+---------+-----+\n",
      "|    PRODUCT_ID|EQP_NAME| OPER_NO|  PRODG1|      TOOL_NAME|     parametric_name|importance|bad_wafer|stats|\n",
      "+--------------+--------+--------+--------+---------------+--------------------+----------+---------+-----+\n",
      "|AFPNR901N.0B0J|   PBT01|1V.PPB10|L2800Z2N|PBT01_CGHA_4-24|PLATE_TEMP#HDB205...|       1.0|        1| MEAN|\n",
      "|AFPNR901N.0B0J|   PBT01|1V.PPB10|L2800Z2N|PBT01_CLHA_4-21|PLATE_TEMP#DHP150...|       1.0|        1| MEAN|\n",
      "+--------------+--------+--------+--------+---------------+--------------------+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_res = split_calculate_features_small_sample(df=res, grpby_list=grpby_list)\n",
    "f_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fa42749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------+---------------+--------------------+------+-----+\n",
      "|    PRODUCT_ID|EQP_NAME| OPER_NO|  PRODG1|      TOOL_NAME|     parametric_name|weight|stats|\n",
      "+--------------+--------+--------+--------+---------------+--------------------+------+-----+\n",
      "|AFPNR901N.0B0J|   PBT01|1V.PPB10|L2800Z2N|PBT01_CGHA_4-24|PLATE_TEMP#HDB205...|   0.5| MEAN|\n",
      "|AFPNR901N.0B0J|   PBT01|1V.PPB10|L2800Z2N|PBT01_CLHA_4-21|PLATE_TEMP#DHP150...|   0.5| MEAN|\n",
      "+--------------+--------+--------+--------+---------------+--------------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_res_ss = get_finall_results_small_sample(f_res=f_res, bad_wafer_num=bad_wafer_num_small_sample)\n",
    "model_res_ss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c32a4a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------+---------------+-----+--------------------+------+----------+--------------+--------+\n",
      "|    PRODUCT_ID|EQP_NAME| OPER_NO|  PRODG1|      TOOL_NAME|stats|     parametric_name|weight|request_id|weight_percent|index_no|\n",
      "+--------------+--------+--------+--------+---------------+-----+--------------------+------+----------+--------------+--------+\n",
      "|AFPNR901N.0B0J|   PBT01|1V.PPB10|L2800Z2N|PBT01_CGHA_4-24| MEAN|PLATE_TEMP#HDB205...|   0.5|     small|          50.0|       1|\n",
      "|AFPNR901N.0B0J|   PBT01|1V.PPB10|L2800Z2N|PBT01_CLHA_4-21| MEAN|PLATE_TEMP#DHP150...|   0.5|     small|          50.0|       1|\n",
      "+--------------+--------+--------+--------+---------------+-----+--------------------+------+----------+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_res_ss = model_res_ss.withColumn('add', lit(0))\n",
    "final_res_add_columns = add_certain_column(df=final_res_ss, by='add', request_id=request_id, grpby_list=grpby_list)\n",
    "final_res_add_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83597a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b478fd72",
   "metadata": {},
   "source": [
    "#### 利用CASE1制作一个小样本的CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b3d7838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(736, 16)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_case1 = pd.read_csv(\"../DWD_POC_CASE_FD_UVA_DATA_CASE1_PROCESSED1.csv\")\n",
    "df_case1_small_sample_pandas = df_case1[df_case1['WAFER_ID'].isin(['NGE186-06', 'NGE186-12', 'NGE186-24', 'NGG239-19', 'NGE197-02', 'NGE197-15', 'NGE197-21', 'NGF482-01', 'NGF482-14'])]\n",
    "df_case1_small_sample_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f453580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/local/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "736"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = ps.from_pandas(df_case1_small_sample_pandas).to_spark()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f5c06f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRODG1', 'OPER_NO', 'TOOL_NAME']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grpby_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d275b608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736\n"
     ]
    }
   ],
   "source": [
    "df_run = _pre_process(df1)\n",
    "print(df_run.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5911c89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+-----------+--------+-------+\n",
      "|  PRODG1| OPER_NO|TOOL_NAME|wafer_count|good_num|bad_num|\n",
      "+--------+--------+---------+-----------+--------+-------+\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|          6|       3|      3|\n",
      "|L15DV07A|1F.EEK10|EKT72_PM1|          2|       0|      2|\n",
      "+--------+--------+---------+-----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "common_res = commonality_analysis(df_run, grpby_list)\n",
    "common_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4605617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'PRODG1': 'L11CD02A', 'OPER_NO': '1F.EEK10', 'TOOL_NAME': 'EKT72_PM1'},\n",
       " {'PRODG1': 'L15DV07A', 'OPER_NO': '1F.EEK10', 'TOOL_NAME': 'EKT72_PM1'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict_list_ss = get_data_list(common_res, grpby_list, big_or_small='small')\n",
    "data_dict_list_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21783a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "674"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_run_ss = get_train_data(df_run, data_dict_list_ss)\n",
    "df_run_ss.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "306524df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_wafer_num_small_sample = get_all_bad_wafer_num(df_run_ss)\n",
    "bad_wafer_num_small_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5b8bebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+--------------------+----------+---------+\n",
      "|  PRODG1| OPER_NO|TOOL_NAME|            features|importance|bad_wafer|\n",
      "+--------+--------+---------+--------------------+----------+---------+\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|STATISTIC_RESULT#...| 0.5198736|        3|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|STATISTIC_RESULT#...| 0.9020127|        3|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|STATISTIC_RESULT#...|  0.940183|        3|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|STATISTIC_RESULT#...|0.76628584|        3|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|STATISTIC_RESULT#...| 0.6545386|        3|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|STATISTIC_RESULT#...|0.92193615|        3|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|STATISTIC_RESULT#...| 0.9986149|        3|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|STATISTIC_RESULT#...| 0.9862689|        3|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|STATISTIC_RESULT#...| 0.9045629|        3|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|STATISTIC_RESULT#...|0.98242646|        3|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|STATISTIC_RESULT#...| 0.7724064|        3|\n",
      "|L15DV07A|1F.EEK10|EKT72_PM1|STATISTIC_RESULT#...|0.99998707|        2|\n",
      "|L15DV07A|1F.EEK10|EKT72_PM1|STATISTIC_RESULT#...| 0.7442202|        2|\n",
      "|L15DV07A|1F.EEK10|EKT72_PM1|STATISTIC_RESULT#...| 0.6679336|        2|\n",
      "+--------+--------+---------+--------------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = fit_pca_small_sample(df=df_run_ss, by=grpby_list)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c1d9090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+--------------------+----------+---------+----------+\n",
      "|  PRODG1| OPER_NO|TOOL_NAME|     parametric_name|importance|bad_wafer|     stats|\n",
      "+--------+--------+---------+--------------------+----------+---------+----------+\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|APC_POSITION#AOTU...| 1.9848838|      3.0|MEAN#RANGE|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|BOTTOMFLOWRATE#AO...| 0.9045629|      3.0|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|CENTER_GAS_PRESSU...|0.98242646|      3.0|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|CENTER_HE_PRESSUR...| 0.7724064|      3.0|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|ESC_CURRENT#AOTU_...| 0.6545386|      3.0|       MAX|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_C1_VAR_CAPACIT...|  0.940183|      3.0|     RANGE|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_RF_POWER#AOTU_...|0.92193615|      3.0|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_RF_VPP#AOTU_ST...| 0.5198736|      3.0|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|PROCESS_GAS_5_CHF...| 0.9020127|      3.0|       SUM|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|UPPER_TEMPERATURE...|0.76628584|      3.0|      MEAN|\n",
      "|L15DV07A|1F.EEK10|EKT72_PM1|APC_POSITION#AOTU...| 1.4121537|      2.0|      MEAN|\n",
      "|L15DV07A|1F.EEK10|EKT72_PM1|PROCESS_GAS_6_CF4...|0.99998707|      2.0|       SUM|\n",
      "+--------+--------+---------+--------------------+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_res = split_calculate_features_small_sample(df=res, by=grpby_list)\n",
    "f_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc609073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+--------------------+--------------------+----------+\n",
      "|  PRODG1| OPER_NO|TOOL_NAME|     parametric_name|              weight|     stats|\n",
      "+--------+--------+---------+--------------------+--------------------+----------+\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|APC_POSITION#AOTU...|  0.1811487599999665|MEAN#RANGE|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|CENTER_GAS_PRESSU...| 0.08966033032687143|      MEAN|\n",
      "|L15DV07A|1F.EEK10|EKT72_PM1|APC_POSITION#AOTU...| 0.08591935284863195|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_C1_VAR_CAPACIT...| 0.08580501436300049|     RANGE|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_RF_POWER#AOTU_...|   0.084139732715689|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|BOTTOMFLOWRATE#AO...| 0.08255417630960758|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|PROCESS_GAS_5_CHF...| 0.08232143583157621|       SUM|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|CENTER_HE_PRESSUR...| 0.07049302456904988|      MEAN|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|UPPER_TEMPERATURE...|  0.0699344365422384|      MEAN|\n",
      "|L15DV07A|1F.EEK10|EKT72_PM1|PROCESS_GAS_6_CF4...| 0.06084198928272122|       SUM|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|ESC_CURRENT#AOTU_...| 0.05973591579578428|       MAX|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|LO_RF_VPP#AOTU_ST...|0.047445831414863124|      MEAN|\n",
      "+--------+--------+---------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_res_ss = get_finall_results_small_sample(f_res=f_res, bad_wafer_num=bad_wafer_num_small_sample)\n",
    "model_res_ss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "96c909b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "|  PRODG1| OPER_NO|TOOL_NAME|     stats|     parametric_name|     weight|request_id|weight_percent|index_no|\n",
      "+--------+--------+---------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|MEAN#RANGE|APC_POSITION#AOTU...| 0.18114875|       fff|     18.114876|       1|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|CENTER_GAS_PRESSU...| 0.08966033|       fff|      8.966033|       2|\n",
      "|L15DV07A|1F.EEK10|EKT72_PM1|      MEAN|APC_POSITION#AOTU...| 0.08591935|       fff|      8.591935|       3|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|     RANGE|LO_C1_VAR_CAPACIT...|0.085805014|       fff|      8.580502|       4|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|LO_RF_POWER#AOTU_...|0.084139735|       fff|      8.413973|       5|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|BOTTOMFLOWRATE#AO...| 0.08255418|       fff|      8.255418|       6|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|       SUM|PROCESS_GAS_5_CHF...|0.082321435|       fff|      8.232143|       7|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|CENTER_HE_PRESSUR...| 0.07049303|       fff|     7.0493026|       8|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|UPPER_TEMPERATURE...|0.069934435|       fff|     6.9934435|       9|\n",
      "|L15DV07A|1F.EEK10|EKT72_PM1|       SUM|PROCESS_GAS_6_CF4...| 0.06084199|       fff|      6.084199|      10|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|       MAX|ESC_CURRENT#AOTU_...|0.059735917|       fff|      5.973592|      11|\n",
      "|L11CD02A|1F.EEK10|EKT72_PM1|      MEAN|LO_RF_VPP#AOTU_ST...| 0.04744583|       fff|      4.744583|      12|\n",
      "+--------+--------+---------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_res_ss = model_res_ss.withColumn('add', lit(0))\n",
    "final_res_add_columns = add_certain_column(df=final_res_ss, by='add', request_id=request_id)\n",
    "final_res_add_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ea0164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_run_ss_pandas = df_run_ss.toPandas()\n",
    "# df_run_ss_pandas\n",
    "\n",
    "# df_run_bs_pandas[df_run_bs_pandas['label'] == 0]['parametric_name'].nunique()\n",
    "\n",
    "# df_run_bs_pandas[df_run_bs_pandas['label'] == 1]['parametric_name'].nunique()\n",
    "\n",
    "# df_pivot = df_run_ss_pandas.dropna(axis=0).pivot_table(index=['WAFER_ID', 'label'], \n",
    "#                                                  columns=['OPER_NO', 'TOOL_NAME', 'parametric_name'],\n",
    "#                                                  values=['STATISTIC_RESULT'])\n",
    "\n",
    "# df_pivot.columns = df_pivot.columns.map('#'.join)\n",
    "# df_pivot = df_pivot.fillna(df_pivot.mean()).reset_index(drop=False)\n",
    "# df_pivot\n",
    "\n",
    "# df_pivot1 = df_pivot.copy()\n",
    "# df_pivot_all = pd.concat([df_pivot, df_pivot1], axis=0)\n",
    "\n",
    "# df_pivot_all\n",
    "\n",
    "# # 定义自变量\n",
    "# x_train = df_pivot_all[df_pivot_all.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "\n",
    "# # 建立模型\n",
    "# model = pca(n_components=min(x_train.shape[0], x_train.shape[1])-1, verbose=None)\n",
    "# results = model.fit_transform(x_train)\n",
    "# res_top = results['topfeat']\n",
    "# res_top_select = res_top[res_top['type'] == 'best'][['feature', 'loading']]\n",
    "# res_top_select = res_top_select.drop_duplicates()\n",
    "\n",
    "# res.toPandas().sort_values('importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806ad8d",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a73ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b7772899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "####################################小样本算法整合###################################\n",
    "#####################################################################################\n",
    "\n",
    "def fit_small_data_model(df_run, common_res, grpby_list, request_id):\n",
    "    \n",
    "    df1 = None\n",
    "    df2 = None\n",
    "\n",
    "    data_dict_list_ss = get_data_list(common_res=common_res, grpby_list=grpby_list, big_or_small='small')\n",
    "    print(\"data_dict_list_ss:\", data_dict_list_ss)\n",
    "    if len(data_dict_list_ss) == 0:\n",
    "        msg = '该查询条件下数据库中实际BAD_WAFER数量为0, 无法分析'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    df_run_ss = get_train_data(df_run=df_run, data_dict_list=data_dict_list_ss)\n",
    "    if df_run_ss.count() == 0:\n",
    "        msg = '数据库中暂无此类数据!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    bad_wafer_num_small_sample = get_all_bad_wafer_num(df_run_ss)\n",
    "    if bad_wafer_num_small_sample < 1:\n",
    "        msg = '该查询条件下数据库中实际BAD_WAFER数量小于1片, 请提供更多的BAD_WAFER数量!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    res = fit_pca_small_sample(df=df_run_ss, by=grpby_list)\n",
    "    if res.count() == 0:\n",
    "        msg = '算法内部暂时异常!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    f_res = split_calculate_features_small_sample(df=res, by=grpby_list)\n",
    "    if f_res.count() == 0:\n",
    "        msg = '算法结果求和暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    model_res_ss = get_finall_results_small_sample(f_res=f_res, bad_wafer_num=bad_wafer_num_small_sample)\n",
    "    if model_res_ss.count() == 0:\n",
    "        msg = '算法结果拼接暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    final_res_ss = model_res_ss.withColumn('add', lit(0))\n",
    "    final_res_add_columns = add_certain_column(df=final_res_ss, by='add', request_id=request_id)\n",
    "    if final_res_add_columns.count() == 0:\n",
    "        msg = '算法结果增加列暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "    else:\n",
    "        return df1, final_res_add_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d78ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "################################将最后的结果写回数据库###############################\n",
    "#####################################################################################\n",
    "def doris_stream_load_from_df(df, engine, table, is_json=True, chunksize=100000, partitions=None):\n",
    "    engine_url = engine.url\n",
    "    url = 'http://%s:18030/api/%s/%s/_stream_load' % (engine_url.host, engine_url.database, table)\n",
    "\n",
    "    format_str = 'csv' if not is_json else 'json'\n",
    "    headers = {\n",
    "        'Content-Type': 'text/plain; charset=UTF-8',\n",
    "        'format': format_str,\n",
    "        'Expect': '100-continue'\n",
    "    }\n",
    "    if is_json:\n",
    "        headers['strip_outer_array'] = 'true'\n",
    "        headers['read_json_by_line'] = 'true'\n",
    "    else:\n",
    "        headers['column_separator'] = '@'\n",
    "    \n",
    "    if partitions:\n",
    "        headers['partitions'] = partitions\n",
    "    \n",
    "    auth = requests.auth.HTTPBasicAuth(engine_url.username, engine_url.password)\n",
    "    session = requests.sessions.Session()\n",
    "    session.should_strip_auth = lambda old_url, new_url: False\n",
    "    \n",
    "    l = len(df)\n",
    "    if l > 0:\n",
    "        if chunksize and chunksize < l:\n",
    "            batches = l // chunksize\n",
    "            if l % chunksize > 0:\n",
    "                batches += 1\n",
    "            for i in range(batches):\n",
    "                si = i * chunksize\n",
    "                ei = min(si + chunksize, l)\n",
    "                sub = df[si:ei]\n",
    "                do_doris_stream_load_from_df(sub, session, url, headers, auth, is_json)\n",
    "        else:\n",
    "            do_doris_stream_load_from_df(df, session, url, headers, auth, is_json)\n",
    "\n",
    "\n",
    "def do_doris_stream_load_from_df(df, session, url, headers, auth, is_json=False):\n",
    "    data = df.to_csv(header=False, index=False, sep='@') if not is_json else df.to_json(orient='records', date_format='iso')\n",
    "    #print(data)\n",
    "    \n",
    "    resp = session.request(\n",
    "        'PUT',\n",
    "        url = url,\n",
    "        data=data.encode('utf-8'),\n",
    "        headers=headers,\n",
    "        auth=auth\n",
    "    )\n",
    "    print(resp.reason, resp.text)\n",
    "    check_stream_load_response(resp.text)\n",
    "\n",
    "\n",
    "def check_stream_load_response(resp_text):\n",
    "    resp = json.loads(resp_text)\n",
    "    if resp['Status'] not in [\"Success\", \"Publish Timeout\"]:\n",
    "        raise Exception(resp['Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6dbc5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "#######################################正式调用以上函数#######################################\n",
    "##########################################################################################\n",
    "# request_id = 'sdd'\n",
    "# grpby_list = ['OPER_NO', 'TOOL_NAME']\n",
    "\n",
    "# # 1. 解析json 为字典， df1为kafka输入的结果数据，获取到parse_dict, request_id, grpby_list\n",
    "# df2 = df1.toPandas() \n",
    "# parse_dict, request_id, grpby_list = get_some_info(df2)\n",
    "# print(type(parse_dict))\n",
    "# print(grpby_list)\n",
    "\n",
    "# # 2. 从kafka 关键字映射都具体数据源中的字段,没有的可以删除\n",
    "# keyword_map_from_json_to_table: dict = {\n",
    "#     \"prodg1\": \"PRODG1\",\n",
    "#     \"waferId\": \"WAFER_ID\",\n",
    "#     \"dateRange\": \"START_TIME\",\n",
    "#     \"productId\": \"PRODUCT_ID\",\n",
    "#     \"operNo\": \"OPER_NO\",\n",
    "#     \"eqp\": \"EQP_NAME\",\n",
    "#     \"tool\": \"TOOL_NAME\",\n",
    "#     \"lot\": \"LOT_ID\",\n",
    "#     \"recipeName\": \"RECIPE_NAME\"}\n",
    "\n",
    "# # 3. 获取查询条件list\n",
    "# select_condition_list = parse_dict\n",
    "\n",
    "# # 4. 指定查询表名, 根据实际情况需要修改\n",
    "# table_name = \"etl.DWD_POC_CASE_FD_UVA_DATA_TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa450022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# spark = (SparkSession.builder\n",
    "#             .master(\"local[*]\")\n",
    "#             .config(\"spark.jars.packages\", \"ai.catboost:catboost-spark_3.3_2.12:1.2\")\n",
    "#             .appName(\"RF\")\n",
    "#             .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef057a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fb12995",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "77f019e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/usr/local/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "df_pandas = pd.read_csv(\"small4.csv\")\n",
    "\n",
    "df1 = ps.from_pandas(df_pandas).to_spark()\n",
    "print(df1.count())\n",
    "\n",
    "good = ['NBX392-15', 'NBX392-20', 'NBX392-24', 'NBX391-24', 'NBX391-25', 'NBX548-09', 'NBX391-01', 'NBX391-02', 'NBX391-13', 'NBX391-17']\n",
    "bad  = ['NBX500-10', 'NBX500-01', 'NBX500-09']\n",
    "\n",
    "if 'label' in df1.columns:\n",
    "    df1 = df1\n",
    "else:\n",
    "    df1 = get_label_double(df1, bad, good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f121fc00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+------+--------+----------+--------------+--------+--------+----------+---------+--------------------+----------+-----------+----------+--------------------+-------------------+------------+-------------+--------------+-------+---------------+-------------------+-------------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+-------------+----------+-------------+------+------------+---------+----------------+-------+-----+\n",
      "|TOOL_ID|      TOOL_NAME|RUN_ID|EQP_NAME| CASE_INFO|    PRODUCT_ID|  PRODG1| OPER_NO|    LOT_ID| WAFER_ID|         RECIPE_NAME|     INPUT|     WINDOW|STATISTICS|     parametric_name|         START_TIME|PARTITION_ID|STATISTIC_KEY|COLLECTION_KEY| SEQ_ID|PROCESS_TYPE_ID|         TIME_STAMP|    CALC_TIME_STAMP|          TARGET|   LOWER_WARNING|   UPPER_WARNING|  LOWER_CRITICAL|  UPPER_CRITICAL|LOWER_OUTLIER|UPPER_OUTLIER|RULES_ENABLED|ALARM_RULE|       RESULT|STATUS|      REGION|ERROR_MSG|STATISTIC_RESULT|VERSION|label|\n",
      "+-------+---------------+------+--------+----------+--------------+--------+--------+----------+---------+--------------------+----------+-----------+----------+--------------------+-------------------+------------+-------------+--------------+-------+---------------+-------------------+-------------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+-------------+----------+-------------+------+------------+---------+----------------+-------+-----+\n",
      "|   9279|PBT01_CLHA_4-12|365189|   PBT01|2023-12-25|AFPNR901N.0C0C|L2800Z2N|1G.PPB10|NBX500.000|NBX500-01|WaferFlow/PRD/PRO...|PLATE_TEMP|DHP150_MEAN|      MEAN|PLATE_TEMP#DHP150...|2023-12-25 18:41:35|         279|         null|         16101|1241416|           null|2023-12-25 18:41:35|2023-12-25 18:43:01|150.015922804888|149.902903086151|150.128942523624|149.846393226782|150.185452382993|         null|         null|            1|      null|149.953478261|NORMAL|LOWER_NORMAL|     null|   149.953478261|      4|    1|\n",
      "|   9280|PBT01_CLHA_4-21|359818|   PBT01|2023-12-25|AFPNR901N.0C0C|L2800Z2N|1G.PPB10|NBX500.000|NBX500-10|WaferFlow/PRD/PRO...|PLATE_TEMP|DHP150_MEAN|      MEAN|PLATE_TEMP#DHP150...|2023-12-25 18:44:46|         280|         null|         16101|1241416|           null|2023-12-25 18:44:46|2023-12-25 18:46:12|150.045654774697|149.935835347171|150.155474202224|149.880925633408|150.210383915987|         null|         null|            1|      null|      150.015|NORMAL|LOWER_NORMAL|     null|         150.015|      4|    1|\n",
      "|   9279|PBT01_CLHA_4-12|366799|   PBT01|2023-12-28|AFPNR901N.0B01|L2800Z2N|1G.PPB10|NBX548.110|NBX548-09|WaferFlow/PRD/PRO...|PLATE_TEMP|DHP150_MEAN|      MEAN|PLATE_TEMP#DHP150...|2023-12-28 19:30:03|         279|         null|         16101|1241416|           null|2023-12-28 19:30:03|2023-12-28 19:31:30|150.015922804888|149.902903086151|150.128942523624|149.846393226782|150.185452382993|         null|         null|            1|      null|     149.9875|NORMAL|LOWER_NORMAL|     null|        149.9875|      4|    0|\n",
      "|   9279|PBT01_CLHA_4-12|362253|   PBT01|2023-12-20|AFPNR901N.0C0C|L2800Z2N|1G.PPB10|NBX500.000|NBX500-01|WaferFlow/PRD/PRO...|PLATE_TEMP|DHP150_MEAN|      MEAN|PLATE_TEMP#DHP150...|2023-12-20 16:22:35|         279|         null|         16101|1241416|           null|2023-12-20 16:22:35|2023-12-20 16:24:01|150.015922804888|149.902903086151|150.128942523624|149.846393226782|150.185452382993|         null|         null|            1|      null|149.946666667|NORMAL|LOWER_NORMAL|     null|   149.946666667|      4|    1|\n",
      "|   9280|PBT01_CLHA_4-21|356904|   PBT01|2023-12-20|AFPNR901N.0C0C|L2800Z2N|1G.PPB10|NBX500.000|NBX500-10|WaferFlow/PRD/PRO...|PLATE_TEMP|DHP150_MEAN|      MEAN|PLATE_TEMP#DHP150...|2023-12-20 16:29:43|         280|         null|         16101|1241416|           null|2023-12-20 16:29:43|2023-12-20 16:31:09|150.045654774697|149.935835347171|150.155474202224|149.880925633408|150.210383915987|         null|         null|            1|      null|     150.0075|NORMAL|LOWER_NORMAL|     null|        150.0075|      4|    1|\n",
      "|   9279|PBT01_CLHA_4-12|365191|   PBT01|2023-12-25|AFPNR901N.0C0C|L2800Z2N|1G.PPB10|NBX500.000|NBX500-09|WaferFlow/PRD/PRO...|PLATE_TEMP|DHP150_MEAN|      MEAN|PLATE_TEMP#DHP150...|2023-12-25 18:44:27|         279|         null|         16101|1241416|           null|2023-12-25 18:44:27|2023-12-25 18:45:53|150.015922804888|149.902903086151|150.128942523624|149.846393226782|150.185452382993|         null|         null|            1|      null|    149.97125|NORMAL|LOWER_NORMAL|     null|       149.97125|      4|    1|\n",
      "|   9279|PBT01_CLHA_4-12|362255|   PBT01|2023-12-20|AFPNR901N.0C0C|L2800Z2N|1G.PPB10|NBX500.000|NBX500-09|WaferFlow/PRD/PRO...|PLATE_TEMP|DHP150_MEAN|      MEAN|PLATE_TEMP#DHP150...|2023-12-20 16:29:17|         279|         null|         16101|1241416|           null|2023-12-20 16:29:17|2023-12-20 16:30:44|150.015922804888|149.902903086151|150.128942523624|149.846393226782|150.185452382993|         null|         null|            1|      null|149.989166667|NORMAL|LOWER_NORMAL|     null|   149.989166667|      4|    1|\n",
      "|   9279|PBT01_CLHA_4-12|373798|   PBT01|2024-01-09|AFPNR901N.0D0D|L2800Z2N|1G.PPB10|NBX392.010|NBX392-24|WaferFlow/PRD/PRO...|PLATE_TEMP|DHP150_MEAN|      MEAN|PLATE_TEMP#DHP150...|2024-01-09 23:07:49|         279|         null|         16101|1241416|           null|2024-01-09 23:07:49|2024-01-09 23:09:16|150.015922804888|149.902903086151|150.128942523624|149.846393226782|150.185452382993|         null|         null|            1|      null|149.967083333|NORMAL|LOWER_NORMAL|     null|   149.967083333|      4|    0|\n",
      "|   9279|PBT01_CLHA_4-12|373797|   PBT01|2024-01-09|AFPNR901N.0D0D|L2800Z2N|1G.PPB10|NBX392.010|NBX392-20|WaferFlow/PRD/PRO...|PLATE_TEMP|DHP150_MEAN|      MEAN|PLATE_TEMP#DHP150...|2024-01-09 23:06:05|         279|         null|         16101|1241416|           null|2024-01-09 23:06:05|2024-01-09 23:07:32|150.015922804888|149.902903086151|150.128942523624|149.846393226782|150.185452382993|         null|         null|            1|      null|149.995833333|NORMAL|LOWER_NORMAL|     null|   149.995833333|      4|    0|\n",
      "|   9278|PBT01_CLHA_4-11|385956|   PBT01|2024-01-09|AFPNR901N.0D0D|L2800Z2N|1G.PPB10|NBX392.010|NBX392-15|WaferFlow/PRD/PRO...|PLATE_TEMP|DHP150_MEAN|      MEAN|PLATE_TEMP#DHP150...|2024-01-09 23:00:39|         278|         null|         16101|1241416|           null|2024-01-09 23:00:39|2024-01-09 23:02:05|150.015431805042|149.912491217033|150.118372393051|149.861020923028|150.169842687055|         null|         null|            1|      null|149.995217391|NORMAL|LOWER_NORMAL|     null|   149.995217391|      4|    0|\n",
      "+-------+---------------+------+--------+----------+--------------+--------+--------+----------+---------+--------------------+----------+-----------+----------+--------------------+-------------------+------------+-------------+--------------+-------+---------------+-------------------+-------------------+----------------+----------------+----------------+----------------+----------------+-------------+-------------+-------------+----------+-------------+------+------------+---------+----------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add943ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0dd3fca0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [96], line 14\u001b[0m\n\u001b[1;32m     13\u001b[0m df1 \u001b[38;5;241m=\u001b[39m integrate_operno(df\u001b[38;5;241m=\u001b[39mdf1, merge_operno_list\u001b[38;5;241m=\u001b[39mmerge_operno)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m())\n\u001b[1;32m     15\u001b[0m df_run \u001b[38;5;241m=\u001b[39m _pre_process(df1)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'count'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [96], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     69\u001b[0m     df_kafka \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m主程序发生异常: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestId\u001b[39m\u001b[38;5;124m\"\u001b[39m: request_id}, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 70\u001b[0m     df1 \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mcreateDataFrame(df_kafka)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 主程序\n",
    "try:\n",
    "    # 从数据库中获取数据\n",
    "#     df1 = get_data_from_doris(select_condition_list=select_condition_list, table_name=table_name)\n",
    "#     print(df1.count())\n",
    "#     if df1.count() == 0:\n",
    "#         msg = '解析SQL获取数据异常: 数据库中可能没有数据!'\n",
    "#         df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "#         df1 = spark.createDataFrame(df_kafka)\n",
    "#         raise ValueError\n",
    "\n",
    "    # 1. 站点融合和数据预处理\n",
    "    df1 = integrate_operno(df=df1, merge_operno_list=merge_operno)\n",
    "    print(df1.count())\n",
    "    df_run = _pre_process(df1)\n",
    "    print(df_run.count())\n",
    "    if df_run.count() == 0:\n",
    "        msg = '该条件下数据库中暂无数据，请检查！'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "\n",
    "    # 2. 进行共性分析\n",
    "    common_res = commonality_analysis(df_run, grpby_list)\n",
    "    common_res.show()\n",
    "    if common_res.count() == 0:\n",
    "        msg = '共性分析结果异常!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "\n",
    "    # 3. 挑选出数据：bad和good要同时大于3\n",
    "    data_dict_list_bs = get_data_list(common_res, grpby_list, big_or_small='big')\n",
    "    print(\"data_dict_list_bs:\", data_dict_list_bs)\n",
    "    if len(data_dict_list_bs) != 0:\n",
    "        print(\"****************大样本算法调用****************\")\n",
    "        df1, final_res_add_columns = fit_big_data_model(df_run, data_dict_list_bs, grpby_list, request_id)\n",
    "    else:        \n",
    "        print(\"****************小样本算法调用****************\")\n",
    "        df1, final_res_add_columns = fit_small_data_model(df_run, common_res, grpby_list, request_id)\n",
    "    \n",
    "\n",
    "    if df1 is not None:\n",
    "        raise ValueError\n",
    "    else:\n",
    "        # final_res_add_columns 是最后的结果，要写回数据库\n",
    "        # ddd = final_res_add_columns.toPandas()\n",
    "        # user =\"root\"\n",
    "        # host = \"10.52.199.81\"\n",
    "        # password = \"Nexchip%40123\"\n",
    "        # db = \"etl\"\n",
    "        # port = 9030\n",
    "        # engine = create_engine(\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\".format(user = user,\n",
    "        #                                                                                     password = password,\n",
    "        #                                                                                     host = host,\n",
    "        #                                                                                     port = port,\n",
    "        #                                                                                     db = db))\n",
    "        # doris_stream_load_from_df(ddd, engine, \"results\")\n",
    "\n",
    "        # # 最终成功的话，就会输出下面这条\n",
    "        print(\"运行成功\")\n",
    "        df_kafka = pd.DataFrame({\"code\": 0, \"msg\": \"运行成功\", \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "\n",
    "except ValueError as ve:\n",
    "    pass\n",
    "\n",
    "except Exception as e:\n",
    "    df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f\"主程序发生异常: {str(e)}\", \"requestId\": request_id}, index=[0])\n",
    "    df1 = spark.createDataFrame(df_kafka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9e68ca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终的df1是：\n",
      "<class 'NoneType'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [97], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m最终的df1是：\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(df1))\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m最终的算法结果是：\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(final_res_add_columns))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "print(\"最终的df1是：\")\n",
    "print(type(df1))\n",
    "df1.show()\n",
    "\n",
    "print(\"最终的算法结果是：\")\n",
    "print(type(final_res_add_columns))\n",
    "final_res_add_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "306bb66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+-----+--------------------+------+----------+--------------+--------+\n",
      "|  PRODG1| OPER_NO|      TOOL_NAME|stats|     parametric_name|weight|request_id|weight_percent|index_no|\n",
      "+--------+--------+---------------+-----+--------------------+------+----------+--------------+--------+\n",
      "|L2800Z2N|1G.PPB10|PBT01_CLHA_4-12| MEAN|PLATE_TEMP#DHP150...|   1.0|       fff|         100.0|       1|\n",
      "+--------+--------+---------------+-----+--------------------+------+----------+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_res_add_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb63e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268564f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0782d987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
