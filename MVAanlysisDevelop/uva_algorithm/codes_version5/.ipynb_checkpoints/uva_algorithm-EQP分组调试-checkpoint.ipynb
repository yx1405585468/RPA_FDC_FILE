{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b567a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from pca import pca\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, max, col, countDistinct, when, rank, lit\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "# from backend_spark.doris_common.doris_client import DorisClient\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ec5b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/python-3.9.13/bin/python3'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"pandas_udf\") \\\n",
    "    .config('spark.sql.session.timeZone', 'Asia/Shanghai') \\\n",
    "    .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "    .config('spark.driver.memory', '1024m') \\\n",
    "    .config('spark.driver.cores', '3') \\\n",
    "    .config('spark.executor.memory', '1024m') \\\n",
    "    .config('spark.executor.cores', '1') \\\n",
    "    .config('spark.cores.max', '2') \\\n",
    "    .config('spark.driver.host','192.168.22.28') \\\n",
    "    .master(\"spark://192.168.12.47:7077,192.168.12.48:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdafa465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1098609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "##########################融合OPER_NO字段##########################\n",
    "###################################################################\n",
    "def integrate_operno(df, merge_operno_list):\n",
    "    if merge_operno_list is not None and len(merge_operno_list) > 0:\n",
    "        # 将mergeOperno中每个字典的values提取出来，组成一个列表\n",
    "        values_to_replace = [list(rule.values())[0] for rule in merge_operno_list]\n",
    "\n",
    "        # 将每一个字典中的values拼接起来\n",
    "        merged_values = [\"_\".join(list(rule.values())[0]) for rule in merge_operno_list]\n",
    "\n",
    "        for values, replacement_value in zip(values_to_replace, merged_values):\n",
    "            df = df.withColumn(\"OPER_NO\",\n",
    "                               when(col(\"OPER_NO\").isin(values), replacement_value).otherwise(col(\"OPER_NO\")))\n",
    "        return df\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "\n",
    "############################################################################\n",
    "##################################FDC数据预处理###############################\n",
    "############################################################################\n",
    "def _pre_process(df):\n",
    "    \"\"\"\n",
    "    param df: 从数据库中读取出来的某个CASE数据\n",
    "    return: 数据预处理，后面要根据实际情况统一添加\n",
    "    \"\"\"\n",
    "    # 只选出会用到的列\n",
    "    df = df.select('WAFER_ID', 'TOOL_ID', 'RUN_ID', 'EQP_NAME', 'PRODUCT_ID', 'PRODG1', 'TOOL_NAME',\n",
    "                   'OPER_NO', 'parametric_name', 'STATISTIC_RESULT', 'label')\n",
    "    # 剔除NA值\n",
    "    df = df.filter(col('STATISTIC_RESULT').isNotNull())\n",
    "    # 按照所有的行进行去重\n",
    "    df1 = df.dropDuplicates()\n",
    "    # 选最新的RUN\n",
    "    df2 = df1.groupBy('WAFER_ID', 'OPER_NO', 'TOOL_ID').agg(max('RUN_ID').alias('RUN_ID'))\n",
    "    df_run = df1.join(df2.dropDuplicates(subset=['WAFER_ID', 'OPER_NO', 'TOOL_ID', 'RUN_ID']),\n",
    "                      on=['WAFER_ID', 'OPER_NO', 'TOOL_ID', 'RUN_ID'], how='inner')\n",
    "    return df_run\n",
    "\n",
    "\n",
    "def commonality_analysis(df_run, grpby_list):\n",
    "    \"\"\"\n",
    "    param df_run: 数据预处理后的数据\n",
    "    return: 共性分析后的结果， 返回bad wafer前十的组合\n",
    "    \"\"\"\n",
    "    grps = (df_run.groupBy(grpby_list)\n",
    "            .agg(countDistinct('WAFER_ID').alias('wafer_count'),\n",
    "                 countDistinct('WAFER_ID', when(df_run['label'] == 0, 1)).alias('good_num'),\n",
    "                 countDistinct('WAFER_ID', when(df_run['label'] == 1, 1)).alias('bad_num'))\n",
    "            .orderBy('bad_num', ascending=False))\n",
    "\n",
    "    # 单站点+单腔室的情况\n",
    "    if grps.count() == 1:\n",
    "        return grps\n",
    "    else:\n",
    "        grps = grps.filter(grps['bad_num'] > 0)\n",
    "        window_sep = Window().orderBy(col(\"bad_num\").desc())\n",
    "        ranked_df = grps.withColumn(\"rank\", rank().over(window_sep))\n",
    "        grpss = ranked_df.filter(col(\"rank\") <= 10).drop(\"rank\")\n",
    "        return grpss\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "#################################获取样本数据#########################\n",
    "############################################################################\n",
    "def get_data_list(common_res, grpby_list, big_or_small='big'):\n",
    "    \"\"\"\n",
    "    param common_res: 共性分析后的结果, 按照大样本或者小样本条件筛选出组合\n",
    "    param grpby_list: 按照PRODG1+OPER_NO+TOOL_NAME分组或OPER_NO+TOOL_NAME分组\n",
    "    param big_or_small: big或者small\n",
    "    return: 对应组合的字典形式, 包在一个大列表中\n",
    "    \"\"\"\n",
    "    assert big_or_small in ['big', 'small'], \"只能选择big或者small, 请检查拼写\"\n",
    "    if big_or_small == 'big':\n",
    "        good_bad_grps = common_res.filter(\"good_num >= 3 AND bad_num >= 3\")\n",
    "    else:\n",
    "        good_bad_grps = common_res.filter(\"bad_num >= 1 AND wafer_count >=2\")\n",
    "    good_bad_grps = good_bad_grps.orderBy(col(\"bad_num\").desc(), col(\"wafer_count\").desc(),\n",
    "                                          col(\"good_num\").desc()).limit(5)\n",
    "\n",
    "    if 'PRODG1' in grpby_list:\n",
    "        data_list = good_bad_grps['PRODG1', 'OPER_NO', 'EQP_NAME'].collect()\n",
    "    else:\n",
    "        data_list = good_bad_grps['OPER_NO', 'EQP_NAME'].collect()\n",
    "\n",
    "    data_dict_list = [row.asDict() for row in data_list]\n",
    "    return data_dict_list\n",
    "\n",
    "\n",
    "def get_train_data(df_run, data_dict_list):\n",
    "    \"\"\"\n",
    "    param df_run: 数据预处理后的数据\n",
    "    param data_dict: 筛选后的字典结果\n",
    "    return: 从原始数据中过滤出真正用来建模的组合数据\n",
    "    \"\"\"\n",
    "    if len(data_dict_list[0]) == 3:\n",
    "        prod, oper, tool = data_dict_list[0]['PRODG1'], data_dict_list[0]['OPER_NO'], data_dict_list[0]['EQP_NAME']\n",
    "        df_s = df_run.filter(\"PRODG1 == '{}' AND OPER_NO == '{}' AND EQP_NAME == '{}'\".format(prod, oper, tool))\n",
    "        for i in range(1, len(data_dict_list)):\n",
    "            prod, oper, tool = data_dict_list[i]['PRODG1'], data_dict_list[i]['OPER_NO'], data_dict_list[i]['EQP_NAME']\n",
    "            df_m = df_run.filter(\"PRODG1 == '{}' AND OPER_NO == '{}' and EQP_NAME == '{}'\".format(prod, oper, tool))\n",
    "            df_s = df_s.union(df_m)\n",
    "    else:\n",
    "        oper, tool = data_dict_list[0]['OPER_NO'], data_dict_list[0]['EQP_NAME']\n",
    "        df_s = df_run.filter(\"OPER_NO == '{}' AND EQP_NAME == '{}'\".format(oper, tool))\n",
    "        for i in range(1, len(data_dict_list)):\n",
    "            oper, tool = data_dict_list[i]['OPER_NO'], data_dict_list[i]['EQP_NAME']\n",
    "            df_m = df_run.filter(\"OPER_NO == '{}' and EQP_NAME == '{}'\".format(oper, tool))\n",
    "            df_s = df_s.union(df_m)\n",
    "    return df_s\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#########################获取传入的整个数据中的所有bad_wafer个数############\n",
    "############################################################################\n",
    "def get_all_bad_wafer_num(df):\n",
    "    \"\"\"\n",
    "    param df: 筛选后的数据\n",
    "    return: 数据中所有bad_wafer的数量\n",
    "    \"\"\"\n",
    "    return df.filter(\"label == 1\").select('WAFER_ID').distinct().count()\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#####################对good>=3和bad>=3的数据，用rf建模######################\n",
    "############################################################################\n",
    "def get_pivot_table(df, by):\n",
    "    \"\"\"\n",
    "    param df: 大样本组合的数据\n",
    "    param by: 分组字段\n",
    "    return: 表格透视后的结果\n",
    "    \"\"\"\n",
    "    if len(by) == 3:\n",
    "        df_pivot = df.dropna(axis=0).pivot_table(index=['WAFER_ID', 'label'],\n",
    "                                                 columns=['OPER_NO', 'EQP_NAME', 'parametric_name', 'PRODG1'],\n",
    "                                                 values=['STATISTIC_RESULT'])\n",
    "    else:\n",
    "        df_pivot = df.dropna(axis=0).pivot_table(index=['WAFER_ID', 'label'],\n",
    "                                                 columns=['OPER_NO', 'EQP_NAME', 'parametric_name'],\n",
    "                                                 values=['STATISTIC_RESULT'])\n",
    "    df_pivot.columns = df_pivot.columns.map('#'.join)\n",
    "    df_pivot = df_pivot.fillna(df_pivot.mean()).reset_index(drop=False)\n",
    "    return df_pivot\n",
    "\n",
    "\n",
    "def fit_rf_big_sample(df, by):\n",
    "    \"\"\"\n",
    "    param df: 大样本组合的数据\n",
    "    param by: 分组字段\n",
    "    return: RandomForest建模后的结果\n",
    "    \"\"\"\n",
    "    schema_all = StructType([\n",
    "        StructField(\"PRODG1\", StringType(), True),\n",
    "        StructField(\"OPER_NO\", StringType(), True),\n",
    "        StructField(\"EQP_NAME\", StringType(), True),\n",
    "        StructField(\"bad_wafer\", IntegerType(), True),\n",
    "        StructField(\"roc_auc_score\", FloatType(), True),\n",
    "        StructField(\"features\", StringType(), True),\n",
    "        StructField(\"importance\", FloatType(), True)])\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_model_result(df_run):\n",
    "        # 表格透视\n",
    "        df_pivot = get_pivot_table(df=df_run, by=by)\n",
    "\n",
    "        # 定义自变量和因变量\n",
    "        X_train = df_pivot[df_pivot.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "        y_train = df_pivot[['label']]\n",
    "\n",
    "        z_ratio = y_train.value_counts(normalize=True)\n",
    "        good_ratio = z_ratio[0]\n",
    "        bad_ratio = z_ratio[1]\n",
    "        if abs(good_ratio - bad_ratio) > 0.7:\n",
    "            undersampler = ClusterCentroids(random_state=101)\n",
    "            X_train, y_train = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        # 网格搜索\n",
    "        pipe = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value=-999)),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', RandomForestClassifier())])\n",
    "        param_grid = {'model__n_estimators': [*range(50, 100, 10)],\n",
    "                      'model__max_depth': [*range(10, 50, 10)]}\n",
    "        grid = GridSearchCV(estimator=pipe, scoring='roc_auc', param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "        grid.fit(X_train.values, y_train.values.ravel())\n",
    "        roc_auc_score_ = grid.best_score_\n",
    "\n",
    "        # 特征重要度、结果汇总\n",
    "        small_importance_res = pd.DataFrame({\n",
    "            'features': X_train.columns,\n",
    "            'importance': grid.best_estimator_.steps[2][1].feature_importances_}).sort_values(by='importance',\n",
    "                                                                                              ascending=False)\n",
    "        if len(by) == 3:\n",
    "            small_sample_res = pd.DataFrame({\n",
    "                'PRODG1': df_run['PRODG1'].unique(),\n",
    "                'OPER_NO': df_run['OPER_NO'].unique(),\n",
    "                'EQP_NAME': df_run['EQP_NAME'].unique(),\n",
    "                'bad_wafer': sum(df_pivot['label']),\n",
    "                'roc_auc_score': roc_auc_score_})\n",
    "        else:\n",
    "            PRODG1 = 'grplen2'\n",
    "            small_sample_res = pd.DataFrame({\n",
    "                'PRODG1': PRODG1,\n",
    "                'OPER_NO': df_run['OPER_NO'].unique(),\n",
    "                'EQP_NAME': df_run['EQP_NAME'].unique(),\n",
    "                'bad_wafer': sum(df_pivot['label']),\n",
    "                'roc_auc_score': roc_auc_score_})\n",
    "        return pd.concat([small_importance_res, small_sample_res])\n",
    "\n",
    "    return df.groupby(by).apply(get_model_result)\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "#########################对good>=3和bad>=3建模后的结果进行整合############################\n",
    "#####################################################################################\n",
    "def split_score_big_sample(df, by):\n",
    "    \"\"\"\n",
    "    param df: RandomForest建模后的结果\n",
    "    param by: 分组字段\n",
    "    return: roc_auc分数结果\n",
    "    \"\"\"\n",
    "    schema_all = StructType([StructField(\"PRODG1\", StringType(), True),\n",
    "                             StructField(\"OPER_NO\", StringType(), True),\n",
    "                             StructField(\"EQP_NAME\", StringType(), True),\n",
    "                             StructField(\"bad_wafer\", IntegerType(), True),\n",
    "                             StructField(\"roc_auc_score\", FloatType(), True)])\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_result(model_results):\n",
    "        sample_res = model_results[['PRODG1', 'OPER_NO', 'EQP_NAME', 'bad_wafer', 'roc_auc_score']].dropna(axis=0)\n",
    "        sample_res = sample_res[sample_res['roc_auc_score'] > 0.6]\n",
    "        return sample_res\n",
    "\n",
    "    return df.groupby(by).apply(get_result)\n",
    "\n",
    "\n",
    "def split_features(df, index) -> str:\n",
    "    \"\"\"\n",
    "    param df: RandomForest建模后的feature_importance_table\n",
    "    param index: 顺序值\n",
    "    return: 字段属性值\n",
    "    \"\"\"\n",
    "    return df['features'].apply(lambda x: x.split('#')[index])\n",
    "\n",
    "\n",
    "def get_split_feature_importance_table(df, by):\n",
    "    \"\"\"\n",
    "    param df: RandomForest建模后的feature_importance_table\n",
    "    param by: OPER_NO+TOOL_NAME+PRODG1或者OPER_NO+TOOL_NAME\n",
    "    return: 分裂features后的表\n",
    "    \"\"\"\n",
    "    df['STATISTIC_RESULT'] = split_features(df, 0)\n",
    "    df['OPER_NO'] = split_features(df, 1)\n",
    "    df['EQP_NAME'] = split_features(df, 2)\n",
    "    df['parametric_name'] = split_features(df, 3)\n",
    "    df['step'] = split_features(df, 4)\n",
    "    df['stats'] = split_features(df, 5)\n",
    "\n",
    "    if 'PRODG1' in by:\n",
    "        df['PRODG1'] = split_features(df, 6)\n",
    "    else:\n",
    "        df = df.assign(PRODG1='grplen2')\n",
    "\n",
    "    df = df.drop(['features', 'STATISTIC_RESULT'], axis=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_feature_stats(df):\n",
    "    \"\"\"\n",
    "    param df: 经过处理后的feature_importance_table\n",
    "    return: 新增一列，含有参数的所有统计特征:feature_stats\n",
    "    \"\"\"\n",
    "    feature_stats = df.groupby(['PRODG1', 'OPER_NO', 'EQP_NAME', 'parametric_name', 'step'])[\n",
    "        'stats'].unique().reset_index()\n",
    "    feature_stats['stats'] = [feature_stats['stats'].iloc[i].tolist() for i in range(len(feature_stats))]\n",
    "    feature_stats['stats'] = feature_stats['stats'].apply(lambda x: \"#\".join(x))\n",
    "    feature_stats = feature_stats.assign(parametric_name=lambda x: x['parametric_name'] + str('#') + x['step']).drop(\n",
    "        'step', axis=1)\n",
    "    return feature_stats\n",
    "\n",
    "\n",
    "def split_calculate_features_big_sample(df, by):\n",
    "    \"\"\"\n",
    "    param df: RandomForest建模后的结果\n",
    "    param by: 分组字段\n",
    "    return: features和importance结果\n",
    "    \"\"\"\n",
    "    schema_all = StructType([\n",
    "        StructField(\"PRODG1\", StringType(), True),\n",
    "        StructField(\"OPER_NO\", StringType(), True),\n",
    "        StructField(\"EQP_NAME\", StringType(), True),\n",
    "        StructField(\"parametric_name\", StringType(), True),\n",
    "        StructField(\"importance\", FloatType(), True),\n",
    "        StructField(\"stats\", StringType(), True)])\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_result(model_results):\n",
    "        # 先从随机森林的模型结果中取出包含features和importance的dataframe\n",
    "        feature_importance_table = model_results[['features', 'importance']].dropna(axis=0)\n",
    "\n",
    "        # 分裂features\n",
    "        feature_importance_res_split = get_split_feature_importance_table(feature_importance_table, by)\n",
    "\n",
    "        # 去除importance为0的组合\n",
    "        feature_importance_res_split_drop = feature_importance_res_split.query(\"importance > 0\").reset_index(drop=True)\n",
    "\n",
    "        # 取每一种组合结果的前60%或者100%\n",
    "        feature_importance_res_split_nlargest = (\n",
    "            feature_importance_res_split_drop.groupby(by=['PRODG1', 'OPER_NO', 'EQP_NAME'])\n",
    "            .apply(lambda x: x.nlargest(int(x.shape[0] * 0.6), 'importance') if x.shape[0] > 1 else x.nlargest(\n",
    "                int(x.shape[0] * 1), 'importance'))\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "        # 新增一列，含有参数的所有统计特征:feature_stats\n",
    "        feature_stats = add_feature_stats(feature_importance_res_split_drop)\n",
    "\n",
    "        # 对同一种组合里的同一个参数进行求和:feature_importance_groupby\n",
    "        feature_importance_groupby = (feature_importance_res_split_nlargest.groupby(['PRODG1', 'OPER_NO', 'EQP_NAME',\n",
    "                                                                                     'parametric_name', 'step'])[\n",
    "                                          'importance'].sum().reset_index())\n",
    "        feature_importance_groupby = feature_importance_groupby.assign(\n",
    "            parametric_name=lambda x: x['parametric_name'] + str('#') + x['step']).drop('step', axis=1)\n",
    "\n",
    "        # feature_stats和feature_importance_groupby连接\n",
    "        grpby_stats = pd.merge(feature_stats, feature_importance_groupby,\n",
    "                               on=['PRODG1', 'OPER_NO', 'EQP_NAME', 'parametric_name']).dropna().reset_index(drop=True)\n",
    "        return grpby_stats\n",
    "\n",
    "    return df.groupby(by).apply(get_result)\n",
    "\n",
    "\n",
    "def get_finall_results_big_sample(s_res, f_res, bad_wafer_num):\n",
    "    \"\"\"\n",
    "    param s_res: roc_auc分数结果\n",
    "    param f_res: features和importance结果\n",
    "    param bad_wafer_num: 数据中所有bad_wafer的数量\n",
    "    return: 最后的建模结果\n",
    "    \"\"\"\n",
    "    # feature_importance_groupby和sample_res连接\n",
    "    roc_auc_score_all = s_res.agg({\"roc_auc_score\": \"sum\"}).collect()[0][0]\n",
    "    s_res = s_res.withColumn(\"roc_auc_score_ratio\", col(\"roc_auc_score\") / roc_auc_score_all)\n",
    "    s_res = s_res.withColumn(\"bad_ratio\", col(\"bad_wafer\") / bad_wafer_num)\n",
    "\n",
    "    df_merge = s_res.join(f_res, on=['PRODG1', 'OPER_NO', 'EQP_NAME'], how='left')\n",
    "    df_merge = df_merge.withColumn('weight_original', col('roc_auc_score_ratio') * col('bad_ratio') * col('importance'))\n",
    "\n",
    "    # 最后再次进行一次归一化\n",
    "    weight_all = df_merge.agg({\"weight_original\": \"sum\"}).collect()[0][0]\n",
    "    df_merge = df_merge.withColumn(\"weight\", col(\"weight_original\") / weight_all)\n",
    "\n",
    "    df_merge = df_merge.select(['PRODG1', 'OPER_NO', 'EQP_NAME',\n",
    "                                'parametric_name', 'weight', 'stats']).orderBy('weight', ascending=False)\n",
    "    return df_merge\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "#############################将建模后的结果增加特定的列####################################\n",
    "#####################################################################################\n",
    "def add_certain_column(df, by, request_id):\n",
    "    \"\"\"\n",
    "    param df: 最后的建模结果\n",
    "    param by: 分组字段, 手动增加一列add\n",
    "    param request_id: 传入的request_id\n",
    "    return: 最后的建模结果增加特定的列\n",
    "    \"\"\"\n",
    "    schema_all = StructType([\n",
    "        StructField(\"PRODG1\", StringType(), True),\n",
    "        StructField(\"OPER_NO\", StringType(), True),\n",
    "        StructField(\"EQP_NAME\", StringType(), True),\n",
    "        StructField(\"stats\", StringType(), True),\n",
    "        StructField(\"parametric_name\", StringType(), True),\n",
    "        StructField(\"weight\", FloatType(), True),\n",
    "        StructField(\"request_id\", StringType(), True),\n",
    "        StructField(\"weight_percent\", FloatType(), True),\n",
    "        StructField(\"index_no\", IntegerType(), True)])\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_result(final_res):\n",
    "        final_res['weight'] = final_res['weight'].astype(float)\n",
    "        final_res = final_res.query(\"weight > 0\")\n",
    "        final_res['request_id'] = request_id\n",
    "        final_res['weight_percent'] = final_res['weight'] * 100\n",
    "        final_res = final_res.sort_values('weight', ascending=False)\n",
    "        final_res['index_no'] = [i + 1 for i in range(len(final_res))]\n",
    "        final_res = final_res.drop('add', axis=1)\n",
    "        # final_res['parametric_name'] = final_res['parametric_name'].str.replace(\"_\", \"+\")\n",
    "        final_res['PRODG1'] = final_res['PRODG1'].apply(lambda x: None if x == 'grplen2' else x)\n",
    "        return final_res\n",
    "    return df.groupby(by).apply(get_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88954844",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "#######################################对bad>=1的数据，用pca建模##############################\n",
    "##########################################################################################\n",
    "def fit_pca_small_sample(df, by):\n",
    "    \"\"\"\n",
    "    param df: 小样本组合的数据\n",
    "    param by: 分组字段\n",
    "    return: PCA建模后的结果\n",
    "    \"\"\"\n",
    "    schema_all = StructType([StructField(\"PRODG1\", StringType(), True),\n",
    "                             StructField(\"OPER_NO\", StringType(), True),\n",
    "                             StructField(\"EQP_NAME\", StringType(), True),\n",
    "                             StructField(\"features\", StringType(), True),\n",
    "                             StructField(\"importance\", FloatType(), True),\n",
    "                             StructField(\"bad_wafer\", IntegerType(), True)])\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_model_result(df_run):\n",
    "        df_pivot = get_pivot_table(df=df_run, by=by)\n",
    "        # 由于是小样本，再重新copy一份制造多一点数据传给PCA模型\n",
    "        df_pivot_copy = df_pivot.copy()\n",
    "        df_pivot_all = pd.concat([df_pivot, df_pivot_copy], axis=0)\n",
    "\n",
    "        # 定义自变量\n",
    "        x_train = df_pivot_all[df_pivot_all.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "\n",
    "        # 建立模型，传入给PCA的n_components选择x_train.shape中的最小值-1；\n",
    "        # 选择是70%或者80%，出来的特征很有可能只是一两个\n",
    "        model = pca(n_components=min(x_train.shape[0], x_train.shape[1]) - 1, verbose=None)\n",
    "        results = model.fit_transform(x_train)\n",
    "        res_top = results['topfeat']\n",
    "        res_top_select = res_top[res_top['type'] == 'best'][['feature', 'loading']]\n",
    "        res_top_select = res_top_select.drop_duplicates()\n",
    "        res_top_select['importance'] = abs(res_top_select['loading'])\n",
    "        res_top_select = res_top_select.rename(columns={'feature': 'features'})\n",
    "        res_top_select = res_top_select.drop(\"loading\", axis=1)\n",
    "\n",
    "        # 增加一些字段信息\n",
    "        res_top_select['bad_wafer'] = sum(df_pivot['label'])\n",
    "        res_top_select['OPER_NO'] = df_run['OPER_NO'].values[0]\n",
    "        res_top_select['EQP_NAME'] = df_run['EQP_NAME'].values[0]\n",
    "        if len(by) == 3:\n",
    "            res_top_select['PRODG1'] = df_run['PRODG1'].values[0]\n",
    "        else:\n",
    "            res_top_select['PRODG1'] = 'grplen2'\n",
    "\n",
    "        return res_top_select\n",
    "\n",
    "    return df.groupby(by).apply(get_model_result)\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "##################################对bad>=1建模后的结果进行整合############################\n",
    "#####################################################################################\n",
    "def split_calculate_features_small_sample(df, by):\n",
    "    \"\"\"\n",
    "    param df: PCA建模后的结果\n",
    "    param by: 分组字段\n",
    "    return: features和importance结果\n",
    "    \"\"\"\n",
    "    schema_all = StructType([StructField(\"PRODG1\", StringType(), True),\n",
    "                             StructField(\"OPER_NO\", StringType(), True),\n",
    "                             StructField(\"EQP_NAME\", StringType(), True),\n",
    "                             StructField(\"parametric_name\", StringType(), True),\n",
    "                             StructField(\"importance\", FloatType(), True),\n",
    "                             StructField(\"bad_wafer\", FloatType(), True),\n",
    "                             StructField(\"stats\", StringType(), True)])\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_result(model_results):\n",
    "        feature_importance_table = model_results[['features', 'importance', 'bad_wafer']].dropna(axis=0)\n",
    "        # 分裂features\n",
    "        feature_importance_res_split = get_split_feature_importance_table(feature_importance_table, by)\n",
    "\n",
    "        # 新增一列，含有参数的所有统计特征:feature_stats\n",
    "        feature_stats = add_feature_stats(feature_importance_res_split)\n",
    "\n",
    "        # 对同一种组合里的同一个参数进行求和:feature_importance_groupby\n",
    "        feature_importance_groupby = (\n",
    "            feature_importance_res_split.groupby(['PRODG1', 'OPER_NO', 'EQP_NAME', 'bad_wafer',\n",
    "                                                  'parametric_name', 'step'])['importance'].sum().reset_index())\n",
    "        feature_importance_groupby = feature_importance_groupby.assign(\n",
    "            parametric_name=lambda x: x['parametric_name'] + str('#') + x['step']).drop('step', axis=1)\n",
    "\n",
    "        # feature_stats和feature_importance_groupby连接\n",
    "        grpby_stats = pd.merge(feature_stats, feature_importance_groupby,\n",
    "                               on=['PRODG1', 'OPER_NO', 'EQP_NAME', 'parametric_name']).dropna().reset_index(drop=True)\n",
    "        return grpby_stats\n",
    "\n",
    "    return df.groupby(by).apply(get_result)\n",
    "\n",
    "\n",
    "def get_finall_results_small_sample(f_res, bad_wafer_num):\n",
    "    \"\"\"\n",
    "    param s_res: roc_auc分数结果\n",
    "    param f_res: features和importance结果\n",
    "    param bad_wafer_num: 数据中所有bad_wafer的数量\n",
    "    return: 最后的建模结果\n",
    "    \"\"\"\n",
    "    f_res = f_res.withColumn(\"bad_ratio\", col(\"bad_wafer\") / bad_wafer_num)\n",
    "    df_merge = f_res.withColumn('weight_original', col('importance') * col('bad_ratio'))\n",
    "\n",
    "    # 最后再次进行一次归一化\n",
    "    weight_all = df_merge.agg({\"weight_original\": \"sum\"}).collect()[0][0]\n",
    "    df_merge = df_merge.withColumn(\"weight\", col(\"weight_original\") / weight_all)\n",
    "\n",
    "    df_merge = df_merge.select(['PRODG1', 'OPER_NO', 'EQP_NAME',\n",
    "                                'parametric_name', 'weight', 'stats']).orderBy('weight', ascending=False)\n",
    "    return df_merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "918cd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大样本数据模型整合\n",
    "def fit_big_data_model(df_run, data_dict_list_bs, grpby_list, request_id):\n",
    "    df1 = None\n",
    "    df2 = None\n",
    "\n",
    "    # 1. 获取用于建模的大样本数据\n",
    "    df_run_bs = get_train_data(df_run, data_dict_list_bs)\n",
    "    if df_run_bs.count() == 0:\n",
    "        msg = '数据库中暂无此类数据!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    # 2. 获取所有bad wafer数量\n",
    "    bad_wafer_num_big_sample = get_all_bad_wafer_num(df_run_bs)\n",
    "    if bad_wafer_num_big_sample < 3:\n",
    "        msg = '数据库中实际BAD_WAFER数量小于3片, 请提供更多的BAD_WAFER数量!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    # 3. 对挑选出的大样本数据进行建模\n",
    "    res = fit_rf_big_sample(df=df_run_bs, by=grpby_list)\n",
    "    if res.count() == 0:\n",
    "        msg = '算法内部暂时异常!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    # 4. 将建模结果进行整合\n",
    "    s_res = split_score_big_sample(df=res, by=['PRODG1', 'OPER_NO', 'EQP_NAME'])\n",
    "    if s_res.count() == 0:\n",
    "        msg = '算法运行评分结果较低, 暂无输出, 建议增加BAD_WAFER数量'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    f_res = split_calculate_features_big_sample(df=res, by=grpby_list)\n",
    "    if f_res.count() == 0:\n",
    "        msg = '算法结果求和暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    model_res_bs = get_finall_results_big_sample(s_res=s_res, f_res=f_res, bad_wafer_num=bad_wafer_num_big_sample)\n",
    "    if model_res_bs.count() == 0:\n",
    "        msg = '算法结果拼接暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    # 7. 增加特定的列\n",
    "    final_res_bs = model_res_bs.withColumn('add', lit(0))\n",
    "    final_res_add_columns = add_certain_column(df=final_res_bs, by='add', request_id=request_id)\n",
    "    if final_res_add_columns.count() == 0:\n",
    "        msg = '算法结果增加列暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "    else:\n",
    "        return df1, final_res_add_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f37f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小样本数据模型整合\n",
    "def fit_small_data_model(df_run, common_res, grpby_list, request_id):\n",
    "    df1 = None\n",
    "    df2 = None\n",
    "\n",
    "    data_dict_list_ss = get_data_list(common_res=common_res, grpby_list=grpby_list, big_or_small='small')\n",
    "    print(\"data_dict_list_ss:\", data_dict_list_ss)\n",
    "    if len(data_dict_list_ss) == 0:\n",
    "        msg = '该查询条件下数据库中实际BAD_WAFER数量为0, 无法分析'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    df_run_ss = get_train_data(df_run=df_run, data_dict_list=data_dict_list_ss)\n",
    "    if df_run_ss.count() == 0:\n",
    "        msg = '数据库中暂无此类数据!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    bad_wafer_num_small_sample = get_all_bad_wafer_num(df_run_ss)\n",
    "    if bad_wafer_num_small_sample < 1:\n",
    "        msg = '该查询条件下数据库中实际BAD_WAFER数量小于1片, 请提供更多的BAD_WAFER数量!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    res = fit_pca_small_sample(df=df_run_ss, by=grpby_list)\n",
    "    if res.count() == 0:\n",
    "        msg = '算法内部暂时异常!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    f_res = split_calculate_features_small_sample(df=res, by=grpby_list)\n",
    "    if f_res.count() == 0:\n",
    "        msg = '算法结果求和暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    model_res_ss = get_finall_results_small_sample(f_res=f_res, bad_wafer_num=bad_wafer_num_small_sample)\n",
    "    if model_res_ss.count() == 0:\n",
    "        msg = '算法结果拼接暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "\n",
    "    final_res_ss = model_res_ss.withColumn('add', lit(0))\n",
    "    final_res_add_columns = add_certain_column(df=final_res_ss, by='add', request_id=request_id)\n",
    "    if final_res_add_columns.count() == 0:\n",
    "        msg = '算法结果增加列暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        return df1, df2\n",
    "    else:\n",
    "        return df1, final_res_add_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162a341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de97e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grpby_list = ['OPER_NO', 'EQP_NAME']\n",
    "grpby_list = ['PRODG1', 'OPER_NO', 'EQP_NAME']\n",
    "request_id = 'sad'\n",
    "merge_operno = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff70b17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取数据pandas类型： (4600, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\pandas\\internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\pandas\\utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas类型转为spark： 4600\n",
      "站点融合后： 4600\n",
      "数据预处理后： 4600\n",
      "+--------+--------+--------+-----------+--------+-------+\n",
      "|  PRODG1| OPER_NO|EQP_NAME|wafer_count|good_num|bad_num|\n",
      "+--------+--------+--------+-----------+--------+-------+\n",
      "|L11CD02A|1F.EEK10|   EKT72|         47|      11|     36|\n",
      "|L11MW20A|1F.EEK10|   EKT72|          3|       0|      3|\n",
      "|L11PP03A|1F.EEK10|   EKT72|          1|       0|      1|\n",
      "+--------+--------+--------+-----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pandas = pd.read_csv(\"D:/Jupyterfiles/晶合MVAFDC_general开发/MVAanlysisDevelop/uva_algorithm/CASE1_DATA/DWD_POC_CASE_FD_UVA_DATA_CASE1_PROCESSED1.csv\")\n",
    "df_pandas = df_pandas.iloc[47000:51600, :]\n",
    "print(\"读取数据pandas类型：\", df_pandas.shape)\n",
    "\n",
    "\n",
    "df1 = ps.from_pandas(df_pandas).to_spark()\n",
    "print(\"pandas类型转为spark：\", df1.count())\n",
    "\n",
    "# 1. 站点融合和数据预处理\n",
    "df1 = integrate_operno(df=df1, merge_operno_list=merge_operno)\n",
    "print(\"站点融合后：\", df1.count())\n",
    "\n",
    "df_run = _pre_process(df1)\n",
    "print(\"数据预处理后：\", df_run.count())\n",
    "\n",
    "\n",
    "# 2. 进行共性分析\n",
    "common_res = commonality_analysis(df_run, grpby_list)\n",
    "common_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f359099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dict_list_bs: [{'PRODG1': 'L11CD02A', 'OPER_NO': '1F.EEK10', 'EQP_NAME': 'EKT72'}]\n",
      "****************大样本算法调用****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\sql\\pandas\\group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\yang.wenjun\\AppData\\Local\\Temp\\ipykernel_38932\\2876604146.py\", line 186, in get_model_result\n  File \"/usr/local/python-3.9.13/lib/python3.9/site-packages/sklearn/model_selection/_search.py\", line 885, in fit\n    refit_metric = self.refit\n  File \"/usr/local/python-3.9.13/lib/python3.9/site-packages/sklearn/model_selection/_search.py\", line 1379, in _run_search\n    evaluate_candidates(ParameterGrid(self.param_grid))\n  File \"/usr/local/python-3.9.13/lib/python3.9/site-packages/sklearn/model_selection/_search.py\", line 822, in evaluate_candidates\n    out = parallel(\n  File \"/usr/local/python-3.9.13/lib/python3.9/site-packages/joblib/parallel.py\", line 1073, in __call__\n    self._pickle_cache = None\n  File \"/usr/local/python-3.9.13/lib/python3.9/site-packages/joblib/parallel.py\", line 961, in retrieve\n    raise\n  File \"/usr/local/python-3.9.13/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 544, in wrap_future_result\n    raise TimeoutError from e\n  File \"/usr/local/python-3.9.13/lib/python3.9/concurrent/futures/_base.py\", line 451, in result\n    self = None\n  File \"/usr/local/python-3.9.13/lib/python3.9/concurrent/futures/_base.py\", line 394, in __get_result\n    self = None\njoblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {EXIT(1)}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_38932\\4150995406.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dict_list_bs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"****************大样本算法调用****************\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_res_add_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_big_data_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_run\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dict_list_bs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrpby_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"****************小样本算法调用****************\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_38932\\1763823861.py\u001b[0m in \u001b[0;36mfit_big_data_model\u001b[1;34m(df_run, data_dict_list_bs, grpby_list, request_id)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mf_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_calculate_features_big_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrpby_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mf_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'算法结果求和暂时异常'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mdf_kafka\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"code\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"msg\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mf'{msg}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"requestId\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    802\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m         \"\"\"\n\u001b[1;32m--> 804\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\developer\\Anaconda\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\yang.wenjun\\AppData\\Local\\Temp\\ipykernel_38932\\2876604146.py\", line 186, in get_model_result\n  File \"/usr/local/python-3.9.13/lib/python3.9/site-packages/sklearn/model_selection/_search.py\", line 885, in fit\n    refit_metric = self.refit\n  File \"/usr/local/python-3.9.13/lib/python3.9/site-packages/sklearn/model_selection/_search.py\", line 1379, in _run_search\n    evaluate_candidates(ParameterGrid(self.param_grid))\n  File \"/usr/local/python-3.9.13/lib/python3.9/site-packages/sklearn/model_selection/_search.py\", line 822, in evaluate_candidates\n    out = parallel(\n  File \"/usr/local/python-3.9.13/lib/python3.9/site-packages/joblib/parallel.py\", line 1073, in __call__\n    self._pickle_cache = None\n  File \"/usr/local/python-3.9.13/lib/python3.9/site-packages/joblib/parallel.py\", line 961, in retrieve\n    raise\n  File \"/usr/local/python-3.9.13/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 544, in wrap_future_result\n    raise TimeoutError from e\n  File \"/usr/local/python-3.9.13/lib/python3.9/concurrent/futures/_base.py\", line 451, in result\n    self = None\n  File \"/usr/local/python-3.9.13/lib/python3.9/concurrent/futures/_base.py\", line 394, in __get_result\n    self = None\njoblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {EXIT(1)}\n"
     ]
    }
   ],
   "source": [
    "# 3. 挑选出数据：bad和good要同时大于3\n",
    "data_dict_list_bs = get_data_list(common_res, grpby_list, big_or_small='big')\n",
    "print(\"data_dict_list_bs:\", data_dict_list_bs)\n",
    "\n",
    "if len(data_dict_list_bs) != 0:\n",
    "    print(\"****************大样本算法调用****************\")\n",
    "    df1, final_res_add_columns = fit_big_data_model(df_run, data_dict_list_bs, grpby_list, request_id)\n",
    "else:\n",
    "    print(\"****************小样本算法调用****************\")\n",
    "    df1, final_res_add_columns = fit_small_data_model(df_run, common_res, grpby_list, request_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82574307",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'toPandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_38932\\3005641797.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'toPandas'"
     ]
    }
   ],
   "source": [
    "df1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92c9e503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全部数据建模结果：\n",
      "+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "|  PRODG1| OPER_NO|EQP_NAME|     stats|     parametric_name|     weight|request_id|weight_percent|index_no|\n",
      "+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "|L11CD02A|1F.EEK10|   EKT72|MEAN#SLOPE|LO_RF_VPP#AOTU_ST...| 0.09998581|       sad|      9.998581|       1|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|LO_RF_VPP#STEP2_MINI|  0.0992132|       sad|       9.92132|       2|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|LO_RF_VPP#STEP2_M...|  0.0895008|       sad|       8.95008|       3|\n",
      "|L11TG05A|1F.EEK10|   EKT72|      MEAN|CENTER_GAS_PRESSU...|0.052282117|       sad|      5.228212|       4|\n",
      "|L11CD02A|1F.EEK10|   EKT72|  MEAN#MAX|LO_RF_REF_POWER#A...|0.043431465|       sad|      4.343147|       5|\n",
      "|L11TG05A|1F.EEK10|   EKT72|      MEAN|FLOWSPLITEDGE#AOT...| 0.04166428|       sad|      4.166428|       6|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|LOWER_TEMPERATURE...|0.038942326|       sad|     3.8942325|       7|\n",
      "|L11TG05A|1F.EEK10|   EKT72|      MEAN|LOWER_TEMPERATURE...|0.037830725|       sad|     3.7830727|       8|\n",
      "|L11CD02A|1F.EEK10|   EKT72|     SLOPE|LO_C1_VAR_CAPACIT...| 0.03043551|       sad|      3.043551|       9|\n",
      "|L11TG05A|1F.EEK10|   EKT72|MEAN#RANGE|APC_POSITION#AOTU...|0.027999746|       sad|     2.7999747|      10|\n",
      "|L11TG05A|1F.EEK10|   EKT72|      MEAN|LO_RF_POWER#AOTU_...|0.024460148|       sad|      2.446015|      11|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|PROCESS_GAS_8_O2#...|0.023416305|       sad|     2.3416305|      12|\n",
      "|L11CD02A|1F.EEK10|   EKT72|RANGE#MEAN|CHAMBER_PRESSURE#...| 0.02257451|       sad|      2.257451|      13|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|CENTER_GAS_PRESSU...|0.020319164|       sad|     2.0319164|      14|\n",
      "|L11TG05A|1F.EEK10|   EKT72|      MEAN|ESC_VOLTAGE#AOTU_...|0.019605795|       sad|     1.9605794|      15|\n",
      "|L11TG05A|1F.EEK10|   EKT72|      MEAN|EDGE_HE_FLOW#AOTU...|0.019605795|       sad|     1.9605794|      16|\n",
      "|L11TG05A|1F.EEK10|   EKT72|      MEAN|EDGE_GAS_PRESSURE...|0.019605795|       sad|     1.9605794|      17|\n",
      "|L11TG05A|1F.EEK10|   EKT72|  MEAN#MAX|ESC_CURRENT#AOTU_...|0.019483633|       sad|     1.9483633|      18|\n",
      "|L11CD02A|1F.EEK10|   EKT72|     RANGE|LO_C2_VAR_CAPACIT...|  0.0192361|       sad|     1.9236101|      19|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|FLOWSPLITEDGE#AOT...| 0.01922603|       sad|     1.9226029|      20|\n",
      "+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"全部数据建模结果：\")\n",
    "final_res_add_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ce6d720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:500建模结果：\n",
      "+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "|  PRODG1| OPER_NO|EQP_NAME|     stats|     parametric_name|     weight|request_id|weight_percent|index_no|\n",
      "+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|PROCESS_GAS_8_O2#...| 0.16348487|       sad|     16.348486|       1|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|LO_RF_VPP#STEP2_M...| 0.16033822|       sad|     16.033821|       2|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|LO_RF_VPP#STEP2_MINI| 0.10415568|       sad|     10.415568|       3|\n",
      "|L11CD02A|1F.EEK10|   EKT72|MEAN#SLOPE|LO_RF_VPP#AOTU_ST...| 0.10382607|       sad|    10.3826065|       4|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|EDGE_GAS_PRESSURE...| 0.09637696|       sad|      9.637696|       5|\n",
      "|L11CD02A|1F.EEK10|   EKT72|       SUM|PROCESS_GAS_5_CHF...| 0.07881552|       sad|      7.881552|       6|\n",
      "|L11CD02A|1F.EEK10|   EKT72|     RANGE|LO_C2_VAR_CAPACIT...| 0.05245579|       sad|      5.245579|       7|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|CENTER_GAS_PRESSU...| 0.05082973|       sad|      5.082973|       8|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|PROCESS_GAS_7_AR#...|0.049221247|       sad|      4.922125|       9|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|BOTTOMFLOWRATE#AO...|0.048605986|       sad|     4.8605986|      10|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|LOWER_TEMPERATURE...|0.047269978|       sad|      4.726998|      11|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|LOWERBRINETEMP#AO...|0.045195654|       sad|     4.5195656|      12|\n",
      "+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"1:500建模结果：\")\n",
    "final_res_add_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8962418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000:2000建模结果：\n",
      "+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "|  PRODG1| OPER_NO|EQP_NAME|     stats|     parametric_name|     weight|request_id|weight_percent|index_no|\n",
      "+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "|L11CD02A|1F.EEK10|   EKT72|MEAN#SLOPE|LO_RF_VPP#AOTU_ST...| 0.13653621|       sad|     13.653622|       1|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|LOWER_TEMPERATURE...|  0.1221142|       sad|     12.211419|       2|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|EDGE_HE_FLOW#AOTU...| 0.11701586|       sad|     11.701586|       3|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|LO_RF_VPP#STEP2_MINI| 0.10394778|       sad|     10.394778|       4|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|WALL_TEMPERATURE#...| 0.07280642|       sad|      7.280642|       5|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|LO_RF_VPP#STEP2_M...| 0.07248003|       sad|      7.248003|       6|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|LOWERBRINETEMP#AO...|  0.0660626|       sad|       6.60626|       7|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|LO_RF_REF_POWER#A...|0.055441815|       sad|      5.544182|       8|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|FLOWSPLITCENTER#A...|0.044495825|       sad|     4.4495826|       9|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|FLOWSPLITEDGE#AOT...| 0.04049444|       sad|     4.0494437|      10|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|EDGE_GAS_PRESSURE...|0.037163578|       sad|     3.7163577|      11|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|UPPER_TEMPERATURE...|0.032295037|       sad|     3.2295036|      12|\n",
      "|L11CD02A|1F.EEK10|   EKT72|       SUM|PROCESS_GAS_5_CHF...|0.030745244|       sad|     3.0745244|      13|\n",
      "|L11CD02A|1F.EEK10|   EKT72|RANGE#MEAN|CHAMBER_PRESSURE#...|0.027883021|       sad|     2.7883022|      14|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|BOTTOMFLOWRATE#AO...|0.024271388|       sad|     2.4271388|      15|\n",
      "+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"1000:2000建模结果：\")\n",
    "final_res_add_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "785f509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全部数据建模结果：\n",
      "+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "|  PRODG1| OPER_NO|EQP_NAME|     stats|     parametric_name|     weight|request_id|weight_percent|index_no|\n",
      "+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "|L11CD02A|1F.EEK10|   EKT72|RANGE#MEAN|APC_POSITION#AOTU...| 0.11607814|       sad|     11.607814|       1|\n",
      "|L11CD02A|1F.EEK10|   EKT72|RANGE#MEAN|CHAMBER_PRESSURE#...| 0.10681782|       sad|     10.681782|       2|\n",
      "|L11CD02A|1F.EEK10|   EKT72|  MAX#MEAN|LO_RF_REF_POWER#A...|  0.0910814|       sad|      9.108141|       3|\n",
      "|L11CD02A|1F.EEK10|   EKT72|       MAX|ESC_CURRENT#AOTU_...|0.079885475|       sad|     7.9885473|       4|\n",
      "|L11CD02A|1F.EEK10|   EKT72|       SUM|PROCESS_GAS_5_CHF...| 0.05983414|       sad|      5.983414|       5|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|BOTTOMFLOWRATE#AO...|0.059666812|       sad|      5.966681|       6|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|CENTER_GAS_PRESSU...|0.058282234|       sad|      5.828223|       7|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|EDGE_GAS_PRESSURE...|   0.058228|       sad|        5.8228|       8|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|CENTER_HE_PRESSUR...|0.054515883|       sad|      5.451588|       9|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|LO_RF_POWER#AOTU_...|0.051303037|       sad|      5.130304|      10|\n",
      "|L11CD02A|1F.EEK10|   EKT72|     RANGE|LO_C1_VAR_CAPACIT...| 0.05082088|       sad|      5.082088|      11|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|UPPER_TEMPERATURE...|0.040256962|       sad|     4.0256963|      12|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|EDGE_HE_FLOW#AOTU...|0.038564254|       sad|     3.8564253|      13|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|WALL_TEMPERATURE#...|0.038115613|       sad|     3.8115613|      14|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|PROCESS_GAS_8_O2#...|0.033500805|       sad|     3.3500807|      15|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|PROCESS_GAS_7_AR#...|0.032328453|       sad|     3.2328453|      16|\n",
      "|L11CD02A|1F.EEK10|   EKT72|      MEAN|LO_RF_VPP#AOTU_ST...|0.030720083|       sad|     3.0720084|      17|\n",
      "+--------+--------+--------+----------+--------------------+-----------+----------+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"47000:51600建模结果：\")\n",
    "final_res_add_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b7dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d4c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308d850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
