{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e31e9e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "from pca import pca\n",
    "from typing import Union, List, Dict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, lit, col, when\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1838b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/python-3.9.13/bin/python3'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"pandas_udf\") \\\n",
    "    .config('spark.sql.session.timeZone', 'Asia/Shanghai') \\\n",
    "    .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "    .config('spark.driver.memory', '8g') \\\n",
    "    .config('spark.driver.cores', '12') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .config('spark.executor.cores', '12') \\\n",
    "    .config('spark.cores.max', '12') \\\n",
    "    .config('spark.driver.host', '192.168.22.28') \\\n",
    "    .master(\"spark://192.168.12.47:7077,192.168.12.48:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bcb4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAFER_ID</th>\n",
       "      <th>OPE_NO</th>\n",
       "      <th>INLINE_PARAMETER_ID</th>\n",
       "      <th>MEASURE_TIME</th>\n",
       "      <th>RANGE_INDEX</th>\n",
       "      <th>FAB_ID</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>LOT_ID</th>\n",
       "      <th>AVERAGE</th>\n",
       "      <th>MAX_VAL</th>\n",
       "      <th>...</th>\n",
       "      <th>ACT_CODE</th>\n",
       "      <th>ETL_INSERT_TIME</th>\n",
       "      <th>ETL_ARC_FLAG</th>\n",
       "      <th>ETL_BATCH_SYNC_TS</th>\n",
       "      <th>ETL_DEL_FLAG</th>\n",
       "      <th>ETL_DS_JOB_NM</th>\n",
       "      <th>ETL_SRC_DB</th>\n",
       "      <th>ETL_SRC_TBL</th>\n",
       "      <th>ETL_TBL_OPER_TS</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IKAS02.082-15</td>\n",
       "      <td>IKAS.OPEN.1</td>\n",
       "      <td>01W0</td>\n",
       "      <td>2023-07-08 16:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-08 16:44:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IKAS02.082-15</td>\n",
       "      <td>IKAS.OPEN.3</td>\n",
       "      <td>YTW0</td>\n",
       "      <td>2023-07-18 13:31:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-18 13:41:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IKAS02.082-15</td>\n",
       "      <td>IKAS.OPEN.3</td>\n",
       "      <td>YMW0</td>\n",
       "      <td>2023-07-18 13:31:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-18 13:41:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IKAS02.082-15</td>\n",
       "      <td>IKAS.OPEN.3</td>\n",
       "      <td>YSIG</td>\n",
       "      <td>2023-07-16 21:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-16 21:19:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IKAS02.082-15</td>\n",
       "      <td>IKAS.OPEN.3</td>\n",
       "      <td>YSIG</td>\n",
       "      <td>2023-07-17 04:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-17 04:41:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32273</th>\n",
       "      <td>IKAS02.082-16</td>\n",
       "      <td>IKAS.OPEN.1</td>\n",
       "      <td>04W0</td>\n",
       "      <td>2023-07-08 16:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-08 16:44:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32274</th>\n",
       "      <td>IKAS02.082-16</td>\n",
       "      <td>IKAS.OPEN.1</td>\n",
       "      <td>05W0</td>\n",
       "      <td>2023-07-08 16:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-08 16:44:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32275</th>\n",
       "      <td>IKAS02.082-16</td>\n",
       "      <td>IKAS.OPEN.1</td>\n",
       "      <td>06W0</td>\n",
       "      <td>2023-07-08 16:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-08 16:44:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32276</th>\n",
       "      <td>IKAS02.082-16</td>\n",
       "      <td>IKAS.OPEN.1</td>\n",
       "      <td>07W0</td>\n",
       "      <td>2023-07-08 16:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-08 16:44:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32277</th>\n",
       "      <td>IKAS02.082-16</td>\n",
       "      <td>IKAS.OPEN.1</td>\n",
       "      <td>08W0</td>\n",
       "      <td>2023-07-08 16:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-08 16:44:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32278 rows × 144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            WAFER_ID       OPE_NO INLINE_PARAMETER_ID         MEASURE_TIME  \\\n",
       "0      IKAS02.082-15  IKAS.OPEN.1                01W0  2023-07-08 16:30:00   \n",
       "1      IKAS02.082-15  IKAS.OPEN.3                YTW0  2023-07-18 13:31:00   \n",
       "2      IKAS02.082-15  IKAS.OPEN.3                YMW0  2023-07-18 13:31:00   \n",
       "3      IKAS02.082-15  IKAS.OPEN.3                YSIG  2023-07-16 21:09:00   \n",
       "4      IKAS02.082-15  IKAS.OPEN.3                YSIG  2023-07-17 04:30:00   \n",
       "...              ...          ...                 ...                  ...   \n",
       "32273  IKAS02.082-16  IKAS.OPEN.1                04W0  2023-07-08 16:30:00   \n",
       "32274  IKAS02.082-16  IKAS.OPEN.1                05W0  2023-07-08 16:30:00   \n",
       "32275  IKAS02.082-16  IKAS.OPEN.1                06W0  2023-07-08 16:30:00   \n",
       "32276  IKAS02.082-16  IKAS.OPEN.1                07W0  2023-07-08 16:30:00   \n",
       "32277  IKAS02.082-16  IKAS.OPEN.1                08W0  2023-07-08 16:30:00   \n",
       "\n",
       "       RANGE_INDEX FAB_ID         PRODUCT_ID         LOT_ID   AVERAGE  \\\n",
       "0                0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.064000   \n",
       "1                0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.000393   \n",
       "2                0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.002098   \n",
       "3                0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.006500   \n",
       "4                0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.017300   \n",
       "...            ...    ...                ...            ...       ...   \n",
       "32273            0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.064500   \n",
       "32274            0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.064100   \n",
       "32275            0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.064400   \n",
       "32276            0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.064300   \n",
       "32277            0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.064200   \n",
       "\n",
       "       MAX_VAL  ...  ACT_CODE      ETL_INSERT_TIME  ETL_ARC_FLAG  \\\n",
       "0          NaN  ...       NaN  2023-07-08 16:44:00             0   \n",
       "1          NaN  ...       NaN  2023-07-18 13:41:00             0   \n",
       "2          NaN  ...       NaN  2023-07-18 13:41:00             0   \n",
       "3          NaN  ...       NaN  2023-07-16 21:19:00             0   \n",
       "4          NaN  ...       NaN  2023-07-17 04:41:00             0   \n",
       "...        ...  ...       ...                  ...           ...   \n",
       "32273      NaN  ...       NaN  2023-07-08 16:44:00             0   \n",
       "32274      NaN  ...       NaN  2023-07-08 16:44:00             0   \n",
       "32275      NaN  ...       NaN  2023-07-08 16:44:00             0   \n",
       "32276      NaN  ...       NaN  2023-07-08 16:44:00             0   \n",
       "32277      NaN  ...       NaN  2023-07-08 16:44:00             0   \n",
       "\n",
       "         ETL_BATCH_SYNC_TS  ETL_DEL_FLAG  ETL_DS_JOB_NM  ETL_SRC_DB  \\\n",
       "0      1970-01-01 00:00:00             0            NaN         NaN   \n",
       "1      1970-01-01 00:00:00             0            NaN         NaN   \n",
       "2      1970-01-01 00:00:00             0            NaN         NaN   \n",
       "3      1970-01-01 00:00:00             0            NaN         NaN   \n",
       "4      1970-01-01 00:00:00             0            NaN         NaN   \n",
       "...                    ...           ...            ...         ...   \n",
       "32273  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32274  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32275  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32276  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32277  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "\n",
       "       ETL_SRC_TBL      ETL_TBL_OPER_TS  label  \n",
       "0              NaN  1970-01-01 00:00:00      1  \n",
       "1              NaN  1970-01-01 00:00:00      1  \n",
       "2              NaN  1970-01-01 00:00:00      1  \n",
       "3              NaN  1970-01-01 00:00:00      1  \n",
       "4              NaN  1970-01-01 00:00:00      1  \n",
       "...            ...                  ...    ...  \n",
       "32273          NaN  1970-01-01 00:00:00      1  \n",
       "32274          NaN  1970-01-01 00:00:00      1  \n",
       "32275          NaN  1970-01-01 00:00:00      1  \n",
       "32276          NaN  1970-01-01 00:00:00      1  \n",
       "32277          NaN  1970-01-01 00:00:00      1  \n",
       "\n",
       "[32278 rows x 144 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = pd.read_csv(\"D:/Jupyterfiles/晶合MVAFDC_general开发/MVAanlysisDevelop/inline_algorithm/codes_version7/inline_test_data3_bysite.csv\")\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e39a956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32278"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = ps.from_pandas(df_pandas).to_spark()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb66b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b6d236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_JSON_config(df: pd.DataFrame):\n",
    "    request_id = df[\"requestId\"].values[0]\n",
    "    request_params = df[\"requestParam\"].values[0]\n",
    "    parse_dict = json.loads(request_params)\n",
    "\n",
    "    # PRODUCT_ID, PROG1, EQP, CHAMBER, OPER_NO存在部分合并的情况\n",
    "    try:\n",
    "        # OPER_NO的部分合并结果\n",
    "        merge_operno = list(parse_dict.get('mergeOperno')) if parse_dict.get('mergeOperno') else None\n",
    "    except KeyError:\n",
    "        merge_operno = None\n",
    "\n",
    "    try:\n",
    "        # PROG1的部分合并结果\n",
    "        merge_prodg1 = list(parse_dict.get('mergeProdg1')) if parse_dict.get('mergeProdg1') else None\n",
    "    except KeyError:\n",
    "        merge_prodg1 = None\n",
    "\n",
    "    try:\n",
    "        # PRODUCT_ID的部分合并结果\n",
    "        merge_product = list(parse_dict.get('mergeProductId')) if parse_dict.get('mergeProductId') else None\n",
    "    except KeyError:\n",
    "        merge_product = None\n",
    "\n",
    "    try:\n",
    "        # EQP的部分合并结果\n",
    "        merge_eqp = list(parse_dict.get('mergeEqp')) if parse_dict.get('mergeEqp') else None\n",
    "    except KeyError:\n",
    "        merge_eqp = None\n",
    "\n",
    "    try:\n",
    "        # CHAMBER的部分合并结果\n",
    "        merge_chamber = list(parse_dict.get('mergeChamber')) if parse_dict.get('mergeChamber') else None\n",
    "    except KeyError:\n",
    "        merge_chamber = None\n",
    "\n",
    "    # 获取good_site和bad_site\n",
    "    try:\n",
    "        good_site = list(parse_dict.get('goodSite')) if parse_dict.get('goodSite') else None\n",
    "    except KeyError:\n",
    "        good_site = None\n",
    "\n",
    "    try:\n",
    "        bad_site = list(parse_dict.get('badSite')) if parse_dict.get('badSite') else None\n",
    "    except KeyError:\n",
    "        bad_site = None\n",
    "\n",
    "    # group by 子句中的字段\n",
    "    group_by_list = parse_dict.get(\"groupByList\")\n",
    "    if group_by_list is None or len(group_by_list) == 0:\n",
    "        group_by_list = [\"PRODG1\", \"PRODUCT_ID\", \"OPER_NO\", \"EQP_NAME\", \"TOOL_NAME\"]\n",
    "        # PRODUCT_ID, PROG1, CHAMBER 这3个存在一键合并的切换开关\n",
    "        # 且一键合并PROG1时会自动一键合并PRODUCT_ID\n",
    "        flag_merge_prodg1 = parse_dict.get('flagMergeAllProdg1')\n",
    "        flag_merge_product_id = parse_dict.get('flagMergeAllProductId')\n",
    "        flag_merge_chamber = parse_dict.get('flagMergeAllChamber')\n",
    "\n",
    "        if flag_merge_prodg1 == '1':\n",
    "            # 一键合并PROG1时，部分合并PROG1和PRODUCT_ID的情况都会被忽略\n",
    "            merge_prodg1 = None\n",
    "            merge_product = None\n",
    "            group_by_list = ['OPER_NO', \"EQP_NAME\", 'TOOL_NAME']\n",
    "            if flag_merge_chamber == '1':\n",
    "                group_by_list = ['OPER_NO', \"EQP_NAME\"]\n",
    "        elif flag_merge_product_id == '1':\n",
    "            # 一键合并PRODUCT_ID时，部分合并PRODUCT_ID的情况会被忽略\n",
    "            merge_product = None\n",
    "            group_by_list = [\"PRODG1\", \"OPER_NO\", \"EQP_NAME\", \"TOOL_NAME\"]\n",
    "            if flag_merge_chamber == '1':\n",
    "                # 一键合并CHAMBER时，部分合并CHAMBER的情况会被忽略\n",
    "                group_by_list = [\"PRODG1\", 'OPER_NO', \"EQP_NAME\"]\n",
    "        elif flag_merge_chamber == '1':\n",
    "            merge_chamber = None\n",
    "            group_by_list = [\"PRODG1\", \"PRODUCT_ID\", \"OPER_NO\", \"EQP_NAME\"]\n",
    "\n",
    "    return parse_dict, request_id, group_by_list, merge_operno, merge_prodg1, merge_product, merge_eqp, merge_chamber, good_site, bad_site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131ee2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_config_ = {\"requestId\": \"715\", \"algorithm\": \"inline_by_site\",\n",
    "                    \"requestParam\": {\"dateRange\": {\"start\": \"2021-12-14 16:22:40\", \"end\": \"2024-03-14 16:22:40\"},\n",
    "                                     \"uploadId\": \"e6eefec4ba2e4456bdde5efbcf47c438\",\n",
    "                                     \"goodSite\": [\"SITE16_VAL\", \"SITE7_VAL\", \"SITE11_VAL\", \"SITE14_VAL\", \"SITE17_VAL\",\n",
    "                                                  \"SITE20_VAL\",\n",
    "                                                  \"SITE9_VAL\", \"SITE12_VAL\", \"SITE13_VAL\", \"SITE15_VAL\", \"SITE18_VAL\",\n",
    "                                                  \"SITE19_VAL\",\n",
    "                                                  \"SITE10_VAL\", \"SITE8_VAL\"],\n",
    "                                     \"badSite\": [\"SITE6_VAL\", \"SITE2_VAL\", \"SITE3_VAL\", \"SITE5_VAL\", \"SITE1_VAL\",\n",
    "                                                 \"SITE4_VAL\"],\n",
    "                                     \"flagMergeAllProdg1\": \"0\", \"flagMergeAllProductId\": \"0\",\n",
    "                                     \"flagMergeAllChamber\": \"0\",\n",
    "                                     \"mergeProdg1\": [], \"mergeProductId\": [], \"mergeEqp\": [], \"mergeChamber\": [],\n",
    "                                     \"mergeOperno\": []}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc8d2188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse_dict:\n",
      "{'dateRange': {'start': '2021-12-14 16:22:40', 'end': '2024-03-14 16:22:40'}, 'uploadId': 'e6eefec4ba2e4456bdde5efbcf47c438', 'goodSite': ['SITE16_VAL', 'SITE7_VAL', 'SITE11_VAL', 'SITE14_VAL', 'SITE17_VAL', 'SITE20_VAL', 'SITE9_VAL', 'SITE12_VAL', 'SITE13_VAL', 'SITE15_VAL', 'SITE18_VAL', 'SITE19_VAL', 'SITE10_VAL', 'SITE8_VAL'], 'badSite': ['SITE6_VAL', 'SITE2_VAL', 'SITE3_VAL', 'SITE5_VAL', 'SITE1_VAL', 'SITE4_VAL'], 'flagMergeAllProdg1': '0', 'flagMergeAllProductId': '0', 'flagMergeAllChamber': '0', 'mergeProdg1': [], 'mergeProductId': [], 'mergeEqp': [], 'mergeChamber': [], 'mergeOperno': []}\n",
      "request_id:\n",
      "715\n",
      "grpby_list:\n",
      "['PRODG1', 'PRODUCT_ID', 'OPER_NO', 'EQP_NAME', 'TOOL_NAME']\n",
      "merge_operno:\n",
      "None\n",
      "merge_prodg1:\n",
      "None\n",
      "merge_product:\n",
      "None\n",
      "merge_eqp:\n",
      "None\n",
      "merge_chamber:\n",
      "None\n",
      "good_site:\n",
      "['SITE16_VAL', 'SITE7_VAL', 'SITE11_VAL', 'SITE14_VAL', 'SITE17_VAL', 'SITE20_VAL', 'SITE9_VAL', 'SITE12_VAL', 'SITE13_VAL', 'SITE15_VAL', 'SITE18_VAL', 'SITE19_VAL', 'SITE10_VAL', 'SITE8_VAL']\n",
      "bad_site:\n",
      "['SITE6_VAL', 'SITE2_VAL', 'SITE3_VAL', 'SITE5_VAL', 'SITE1_VAL', 'SITE4_VAL']\n"
     ]
    }
   ],
   "source": [
    "df_info_ = pd.DataFrame({\"requestId\": [json_config_[\"requestId\"]],\n",
    "                         \"requestParam\": [json.dumps(json_config_[\"requestParam\"])]})\n",
    "\n",
    "# 解析JSON并且读取数据\n",
    "parse_dict, request_id, grpby_list, merge_operno, merge_prodg1, merge_product, merge_eqp, merge_chamber, good_site, bad_site = parse_JSON_config(\n",
    "    df_info_)\n",
    "print(\"parse_dict:\")\n",
    "print(parse_dict)\n",
    "print(\"request_id:\")\n",
    "print(request_id)\n",
    "print(\"grpby_list:\")\n",
    "print(grpby_list)\n",
    "print(\"merge_operno:\")\n",
    "print(merge_operno)\n",
    "print(\"merge_prodg1:\")\n",
    "print(merge_prodg1)\n",
    "print(\"merge_product:\")\n",
    "print(merge_product)\n",
    "print(\"merge_eqp:\")\n",
    "print(merge_eqp)\n",
    "print(\"merge_chamber:\")\n",
    "print(merge_chamber)\n",
    "print(\"good_site:\")\n",
    "print(good_site)\n",
    "print(\"bad_site:\")\n",
    "print(bad_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70daae75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4540c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessorForInline:\n",
    "    def __init__(self,\n",
    "                 df: pyspark.sql.dataframe,\n",
    "                 columns_list: list[str],\n",
    "                 certain_column: str,\n",
    "                 key_words: list[str],\n",
    "                 convert_to_numeric_list: list[str],\n",
    "                 merge_operno_list: List[Dict[str, List[str]]]):\n",
    "        self.df = df\n",
    "        self.columns_list = columns_list\n",
    "        self.certain_column = certain_column\n",
    "        self.key_words = key_words\n",
    "        self.convert_to_numeric_list = convert_to_numeric_list\n",
    "        self.merge_operno_list = merge_operno_list\n",
    "\n",
    "    @staticmethod\n",
    "    def select_columns(df: pyspark.sql.dataframe, columns_list: list[str]) -> pyspark.sql.dataframe:\n",
    "        return df.select(columns_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def exclude_some_data(df: pyspark.sql.dataframe, key_words: list[str],\n",
    "                          certain_column: str) -> pyspark.sql.dataframe:\n",
    "        key_words_str = '|'.join(key_words)\n",
    "        df_filtered = df.filter(~col(certain_column).rlike(key_words_str))\n",
    "        return df_filtered\n",
    "\n",
    "    @staticmethod\n",
    "    def pre_process(df: pyspark.sql.dataframe, convert_to_numeric_list: list[str]) -> pyspark.sql.dataframe:\n",
    "        for column in convert_to_numeric_list:\n",
    "            df = df.withColumn(column, col(column).cast('double'))\n",
    "        if 'SITE_COUNT' in convert_to_numeric_list:\n",
    "            convert_to_numeric_list.remove('SITE_COUNT')\n",
    "        df = df.dropna(subset=convert_to_numeric_list, how='all')\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def integrate_columns(df: pyspark.sql.dataframe,\n",
    "                          merge_operno_list: List[Dict[str, List[str]]]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Integrate columns in the DataFrame based on the provided list.\n",
    "\n",
    "        :param df: The input DataFrame.\n",
    "        :param merge_operno_list: A list of dictionaries where each dictionary contains values to be merged.\n",
    "               Example: [{'2F.CDS10_XX.TDS01': ['2F.CDS10', 'XX.TDS01']},\n",
    "                         {'2F.CDS20_XX.CDS20': ['2F.CDS20', 'XX.CDS20']}]\n",
    "        :return: DataFrame with 'OPER_NO' and other specified columns integrated according to the merge rules.\n",
    "        \"\"\"\n",
    "        # split using comma\n",
    "        splitter_comma = \",\"\n",
    "        if merge_operno_list is not None and len(merge_operno_list) > 0:\n",
    "            values_to_replace = [list(rule.values())[0] for rule in merge_operno_list]\n",
    "            merged_values = [splitter_comma.join(list(rule.values())[0]) for rule in merge_operno_list]\n",
    "\n",
    "            for values, replacement_value in zip(values_to_replace, merged_values):\n",
    "                df = df.withColumn(\"OPE_NO\",\n",
    "                                   when(col(\"OPE_NO\").isin(values), replacement_value).otherwise(col(\"OPE_NO\")))\n",
    "        return df\n",
    "\n",
    "    def run(self) -> pyspark.sql.dataframe:\n",
    "        df_select = self.select_columns(df=self.df, columns_list=self.columns_list)\n",
    "        df_esd = self.exclude_some_data(df=df_select, key_words=self.key_words, certain_column=self.certain_column)\n",
    "        df_pp = self.pre_process(df=df_esd, convert_to_numeric_list=self.convert_to_numeric_list)\n",
    "        df_integrate = self.integrate_columns(df=df_pp, merge_operno_list=self.merge_operno_list)\n",
    "        return df_integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "decdd240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractFeaturesBySite:\n",
    "    @staticmethod\n",
    "    def process_missing_values_for_site(df: pd.DataFrame,\n",
    "                                        good_site_columns: list[str],\n",
    "                                        bad_site_columns: list[str],\n",
    "                                        missing_value_threshold: Union[int, float] = 0.6,\n",
    "                                        process_miss_site_mode: str = 'drop') -> pd.DataFrame:\n",
    "        assert process_miss_site_mode in ['drop', 'fill']\n",
    "        site_columns = good_site_columns + bad_site_columns\n",
    "        if process_miss_site_mode == 'drop':\n",
    "            # drop rows based on the missing value threshold\n",
    "            df = df.dropna(subset=site_columns, thresh=missing_value_threshold)\n",
    "        else:\n",
    "            # fill missing values in the corresponding site rows using the AVERAGE of that row\n",
    "            df[site_columns] = df[site_columns].apply(lambda column: column.fillna(df['AVERAGE']))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_statistics(row):\n",
    "        return pd.Series({\n",
    "            'MAX_VAL': row.max(),\n",
    "            'MIN_VAL': row.min(),\n",
    "            'MEDIAN': row.median(),\n",
    "            'AVERAGE': row.mean(),\n",
    "            'STD_DEV': row.std(),\n",
    "            'PERCENTILE_25': row.quantile(0.25),\n",
    "            'PERCENTILE_75': row.quantile(0.75)})\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_site_stats(df: pd.DataFrame, grpby_list: list[str], site_columns: list[str],\n",
    "                             good_or_bad: str) -> pd.DataFrame:\n",
    "        assert good_or_bad in ['good', 'bad'], \"Label could only be 'good' or 'bad'\"\n",
    "        selected_df = df[grpby_list + ['WAFER_ID', 'INLINE_PARAMETER_ID'] + site_columns].reset_index(drop=True)\n",
    "        # Perform statistical calculations for each row\n",
    "        side_features = selected_df.apply(lambda row: ExtractFeaturesBySite.calculate_statistics(row[site_columns]),\n",
    "                                          axis=1)\n",
    "        side_features = side_features.fillna(0)\n",
    "        df_with_features = pd.concat([selected_df, side_features], axis=1)\n",
    "        if good_or_bad == 'good':\n",
    "            df_with_features['label'] = 0\n",
    "        else:\n",
    "            df_with_features['label'] = 1\n",
    "        return df_with_features\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_features_by_site(df: pd.DataFrame,\n",
    "                                 grpby_list: list[str],\n",
    "                                 good_site_columns: list[str],\n",
    "                                 bad_site_columns: list[str],\n",
    "                                 missing_value_threshold: Union[int, float] = 0.6,\n",
    "                                 process_miss_site_mode: str = 'drop') -> Union[pd.DataFrame, None]:\n",
    "        \"\"\"\n",
    "        Extracts features from a DataFrame based on good and bad site columns.\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The input DataFrame.\n",
    "        - grp_list: ['OPE_NO'] for most the case.\n",
    "        - good_site_columns (list): List of columns representing good sites.\n",
    "        - bad_site_columns (list): List of columns representing bad sites.\n",
    "        - missing_value_threshold (Union[int, float]): Threshold for missing values.\n",
    "        - process_miss_site_mode (str): Mode for handling missing values in site columns, e.g. drop or fill\n",
    "        Returns:\n",
    "        - Union[pd.DataFrame, None]: DataFrame with extracted features or None if no data is available.\n",
    "        \"\"\"\n",
    "        df_pandas_specific_ = ExtractFeaturesBySite.process_missing_values_for_site(df=df,\n",
    "                                                                                    good_site_columns=good_site_columns,\n",
    "                                                                                    bad_site_columns=bad_site_columns,\n",
    "                                                                                    missing_value_threshold=missing_value_threshold,\n",
    "                                                                                    process_miss_site_mode=process_miss_site_mode)\n",
    "        if df_pandas_specific_.shape[0] != 0:\n",
    "            side_with_features1 = ExtractFeaturesBySite.calculate_site_stats(df_pandas_specific_, grpby_list,\n",
    "                                                                             good_site_columns,\n",
    "                                                                             good_or_bad='good')\n",
    "            side_with_features2 = ExtractFeaturesBySite.calculate_site_stats(df_pandas_specific_, grpby_list,\n",
    "                                                                             bad_site_columns,\n",
    "                                                                             good_or_bad='bad')\n",
    "            side_with_features1_select = side_with_features1[\n",
    "                grpby_list + ['WAFER_ID', 'INLINE_PARAMETER_ID', 'MAX_VAL', 'MIN_VAL', 'MEDIAN',\n",
    "                              'AVERAGE', 'STD_DEV', 'PERCENTILE_25', 'PERCENTILE_75', 'label']]\n",
    "            side_with_features2_select = side_with_features2[\n",
    "                grpby_list + ['WAFER_ID', 'INLINE_PARAMETER_ID', 'MAX_VAL', 'MIN_VAL', 'MEDIAN',\n",
    "                              'AVERAGE', 'STD_DEV', 'PERCENTILE_25', 'PERCENTILE_75', 'label']]\n",
    "            side_with_features_all = pd.concat([side_with_features1_select, side_with_features2_select], axis=0)\n",
    "            return side_with_features_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb980392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitInlineModelBySite:\n",
    "    def __init__(self,\n",
    "                 df: pyspark.sql.dataframe,\n",
    "                 grpby_list: list[str],\n",
    "                 good_site_columns: list[str],\n",
    "                 bad_site_columns: list[str],\n",
    "                 process_miss_site_mode: str,\n",
    "                 columns_to_process: list[str],\n",
    "                 missing_value_threshold: Union[int, float],\n",
    "                 model: str = 'pca'):\n",
    "        \"\"\"\n",
    "        Initialize the FitInlineModelBySite object.\n",
    "\n",
    "        Parameters:\n",
    "        - df: pyspark.sql.dataframe, the input data\n",
    "        - grpby_list: list[str], the grouping variable, inline data should be [\"OPE_NO\"] mostly\n",
    "        - good_site_columns: List of str, column names for good sites\n",
    "        - bad_site_columns: List of str, column names for bad sites\n",
    "        - process_miss_site_mode: str, mode for handling missing values in site data, e.g. drop or fill\n",
    "        - columns_to_process: List of str, columns to process in missing value functions\n",
    "        - missing_value_threshold: Union[int, float], threshold for missing values\n",
    "        - model: str, default is 'pca', other options include 'rf' for random forest, 'decisionTree' for decision tree,\n",
    "                 svc, logistic and sgd.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.grpby_list = grpby_list\n",
    "        self.good_site_columns = good_site_columns\n",
    "        self.bad_site_columns = bad_site_columns\n",
    "        self.process_miss_site_mode = process_miss_site_mode\n",
    "        self.columns_to_process = columns_to_process\n",
    "        self.missing_value_threshold = missing_value_threshold\n",
    "        self.model = model\n",
    "\n",
    "    @staticmethod\n",
    "    def process_missing_values(df, columns_to_process, missing_value_threshold):\n",
    "        for column in columns_to_process:\n",
    "            missing_percentage = df[column].isnull().mean()\n",
    "            if missing_percentage > missing_value_threshold:\n",
    "                df = df.drop(columns=[column])\n",
    "            else:\n",
    "                df[column] = df[column].fillna(df[column].mean())\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pivot_table(df, grpby_list, columns_to_process, missing_value_threshold):\n",
    "        df = FitInlineModelBySite.process_missing_values(df, columns_to_process, missing_value_threshold)\n",
    "        index_list = ['WAFER_ID', 'label']\n",
    "        columns_list = grpby_list + ['INLINE_PARAMETER_ID']\n",
    "        values_list = df.columns.difference(['WAFER_ID', 'INLINE_PARAMETER_ID', 'label'] + grpby_list)\n",
    "        pivot_result = df.pivot_table(index=index_list,\n",
    "                                      columns=columns_list,\n",
    "                                      values=values_list)\n",
    "        pivot_result.columns = pivot_result.columns.map('#'.join)\n",
    "        pivot_result = FitInlineModelBySite.process_missing_values(pivot_result, pivot_result.columns,\n",
    "                                                                   missing_value_threshold)\n",
    "        pivot_result = pivot_result.reset_index(drop=False)\n",
    "        # Remove completely identical columns\n",
    "        for column in pivot_result.columns.difference(index_list):\n",
    "            if pivot_result[column].nunique() == 1:\n",
    "                pivot_result = pivot_result.drop(column, axis=1)\n",
    "        return pivot_result\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_pca_model(df, grpby_list, good_site_columns, bad_site_columns, columns_to_process, process_miss_site_mode,\n",
    "                      missing_value_threshold):\n",
    "        schema_all = StructType([StructField(\"features\", StringType(), True),\n",
    "                                 StructField(\"importance\", FloatType(), True)])\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_model_result(df_run):\n",
    "            side_with_features_all = ExtractFeaturesBySite.extract_features_by_site(df=df_run,\n",
    "                                                                                    grpby_list=grpby_list,\n",
    "                                                                                    good_site_columns=good_site_columns,\n",
    "                                                                                    bad_site_columns=bad_site_columns,\n",
    "                                                                                    missing_value_threshold=missing_value_threshold,\n",
    "                                                                                    process_miss_site_mode=process_miss_site_mode)\n",
    "            if side_with_features_all is None:\n",
    "                return pd.DataFrame({\"features\": \"STATS#OPE#PARAM\", \"importance\": -100}, index=[0])\n",
    "\n",
    "            pivot_result = FitInlineModelBySite.get_pivot_table(df=side_with_features_all,\n",
    "                                                                grpby_list=grpby_list,\n",
    "                                                                columns_to_process=columns_to_process,\n",
    "                                                                missing_value_threshold=missing_value_threshold)\n",
    "            x_train = pivot_result[pivot_result.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "\n",
    "            if x_train.shape[1] > 1:\n",
    "                n_components = min(min(x_train.shape) - 2, 20)\n",
    "                model = pca(n_components=n_components, verbose=None)\n",
    "                results = model.fit_transform(x_train)\n",
    "                res_top = results['topfeat']\n",
    "                res_top_select = res_top[res_top['type'] == 'best'][['feature', 'loading']]\n",
    "                res_top_select['importance'] = abs(res_top_select['loading'])\n",
    "                res_top_select = res_top_select.rename(columns={'feature': 'features'}).drop(\"loading\",\n",
    "                                                                                             axis=1).drop_duplicates()\n",
    "                return res_top_select\n",
    "            else:\n",
    "                res_top_select = pd.DataFrame({\"features\": \"STATS#OPE#PARAM\", \"importance\": -101}, index=[0])\n",
    "                return res_top_select\n",
    "\n",
    "        return df.groupby(grpby_list).apply(get_model_result)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pipe_params(model):\n",
    "        common_steps = [\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value=-999)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]\n",
    "        models = {\n",
    "            'rf': (RandomForestClassifier(random_state=2024), {\n",
    "                'model__n_estimators': [*range(10, 60, 10)],\n",
    "                'model__max_depth': [*range(5, 50, 10)],\n",
    "                'model__min_samples_split': [2, 5],\n",
    "                'model__min_samples_leaf': [1, 3]\n",
    "            }),\n",
    "\n",
    "            'decisionTree': (DecisionTreeClassifier(random_state=2024), {\n",
    "                'model__max_depth': [None, 5, 10, 15],\n",
    "                'model__min_samples_split': [2, 5, 10],\n",
    "                'model__min_samples_leaf': [1, 2, 4]\n",
    "            }),\n",
    "\n",
    "            'svc': (LinearSVC(random_state=2024, fit_intercept=False), {\n",
    "                'model__loss': ['hinge', 'squared_hinge'],\n",
    "                'model__C': [0.1, 0.5, 1, 10, 50]\n",
    "            }),\n",
    "\n",
    "            'logistic': (LogisticRegression(random_state=2024, fit_intercept=False, solver='liblinear'), {\n",
    "                'model__penalty': ['l1', 'l2'],\n",
    "                'model__C': [0.1, 0.5, 1, 10, 50]\n",
    "            }),\n",
    "\n",
    "            'sgd': (SGDClassifier(random_state=2024, fit_intercept=False), {\n",
    "                'model__loss': ['hinge', 'log_loss', 'perceptron', 'huber'],\n",
    "                'model__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "                'model__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "                'model__max_iter': [100, 500, 1000]\n",
    "            })\n",
    "        }\n",
    "\n",
    "        if model in models:\n",
    "            model_class, param_grid = models[model]\n",
    "            steps = common_steps + [('model', model_class)]\n",
    "            pipe = Pipeline(steps)\n",
    "        else:\n",
    "            raise Exception('Wrong Model Selection. Supported models are: pca, rf, decisionTree, svc, logistic, sgd.')\n",
    "        return pipe, param_grid\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_classification_model(df, grpby_list, good_site_columns, bad_site_columns,\n",
    "                                 columns_to_process, process_miss_site_mode, missing_value_threshold, model):\n",
    "        schema_all = StructType([StructField(\"features\", StringType(), True),\n",
    "                                 StructField(\"importance\", FloatType(), True)])\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_model_result(df_run):\n",
    "            side_with_features_all = ExtractFeaturesBySite.extract_features_by_site(df=df_run,\n",
    "                                                                                    grpby_list=grpby_list,\n",
    "                                                                                    good_site_columns=good_site_columns,\n",
    "                                                                                    bad_site_columns=bad_site_columns,\n",
    "                                                                                    missing_value_threshold=missing_value_threshold,\n",
    "                                                                                    process_miss_site_mode=process_miss_site_mode)\n",
    "            if side_with_features_all is None:\n",
    "                return pd.DataFrame({\"features\": \"STATS#OPE#PARAM\", \"importance\": -100}, index=[0])\n",
    "\n",
    "            pivot_result = FitInlineModelBySite.get_pivot_table(df=side_with_features_all,\n",
    "                                                                grpby_list=grpby_list,\n",
    "                                                                columns_to_process=columns_to_process,\n",
    "                                                                missing_value_threshold=missing_value_threshold)\n",
    "            x_train = pivot_result[pivot_result.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "            y_train = pivot_result[['label']]\n",
    "\n",
    "            if min(x_train.shape) > 4 and y_train['label'].nunique() > 1:\n",
    "                pipe, param_grid = FitInlineModelBySite.get_pipe_params(model=model)\n",
    "                try:\n",
    "                    grid = GridSearchCV(estimator=pipe, scoring='roc_auc', param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "                    grid.fit(x_train.values, y_train.values.ravel())\n",
    "\n",
    "                except ValueError:\n",
    "                    small_importance_res = pd.DataFrame({\"features\": \"STATS#OPE#PARAM\", \"importance\": -101}, index=[0])\n",
    "                    return small_importance_res\n",
    "\n",
    "                best_est = grid.best_estimator_.steps[-1][-1]\n",
    "                if hasattr(best_est, 'feature_importances_'):\n",
    "                    small_importance_res = pd.DataFrame({'features': x_train.columns,\n",
    "                                                         'importance': best_est.feature_importances_})\n",
    "                else:\n",
    "                    small_importance_res = pd.DataFrame({'features': x_train.columns,\n",
    "                                                         'importance': abs(best_est.coef_.ravel())})\n",
    "                return small_importance_res\n",
    "\n",
    "            else:\n",
    "                small_importance_res = pd.DataFrame({\"features\": \"STATS#OPE#PARAM\", \"importance\": -102}, index=[0])\n",
    "                return small_importance_res\n",
    "\n",
    "        return df.groupby(grpby_list).apply(get_model_result)\n",
    "\n",
    "    def run(self):\n",
    "        if self.model == 'pca':\n",
    "            res = self.fit_pca_model(df=self.df, grpby_list=self.grpby_list,\n",
    "                                     good_site_columns=self.good_site_columns,\n",
    "                                     bad_site_columns=self.bad_site_columns,\n",
    "                                     columns_to_process=self.columns_to_process,\n",
    "                                     process_miss_site_mode=self.process_miss_site_mode,\n",
    "                                     missing_value_threshold=self.missing_value_threshold)\n",
    "        else:\n",
    "            res = self.fit_classification_model(df=self.df, grpby_list=self.grpby_list,\n",
    "                                                good_site_columns=self.good_site_columns,\n",
    "                                                bad_site_columns=self.bad_site_columns,\n",
    "                                                columns_to_process=self.columns_to_process,\n",
    "                                                process_miss_site_mode=self.process_miss_site_mode,\n",
    "                                                missing_value_threshold=self.missing_value_threshold,\n",
    "                                                model=self.model)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76c25b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitInlineModelResults:\n",
    "    def __init__(self, df: pyspark.sql.dataframe, grpby_list: List[str], request_id: str):\n",
    "        self.df = df\n",
    "        self.grpby_list = grpby_list\n",
    "        self.request_id = request_id\n",
    "\n",
    "    @staticmethod\n",
    "    def split_features(df: pd.DataFrame, index: int) -> str:\n",
    "        return df['features'].apply(lambda x: x.split('#')[index])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_split_features(df: pd.DataFrame, grpby_list: List[str]) -> pd.DataFrame:\n",
    "        n_feats = len(grpby_list)\n",
    "        for i in range(n_feats):\n",
    "            df[grpby_list[i]] = SplitInlineModelResults.split_features(df, i + 1)\n",
    "\n",
    "        df['INLINE_PARAMETER_ID'] = SplitInlineModelResults.split_features(df, n_feats + 1)\n",
    "        df = df.drop(['features'], axis=1).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def split_calculate_features(df: pyspark.sql.dataframe, grpby_list: List[str], by: str) -> pyspark.sql.dataframe:\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"INLINE_PARAMETER_ID\", StringType(), True),\n",
    "                              StructField(\"importance\", FloatType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_model_result(df_run):\n",
    "            split_table = SplitInlineModelResults.get_split_features(df=df_run, grpby_list=grpby_list)\n",
    "            split_table_grpby = split_table.groupby(grpby_list + ['INLINE_PARAMETER_ID'])[\n",
    "                'importance'].sum().reset_index(drop=False)\n",
    "            return split_table_grpby\n",
    "\n",
    "        return df.groupby(by).apply(get_model_result)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_certain_column(df: pyspark.sql.dataframe, grpby_list: List[str], request_id: str,\n",
    "                           by: str) -> pyspark.sql.dataframe:\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"INLINE_PARAMETER_ID\", StringType(), True),\n",
    "                              StructField(\"AVG_SPEC_CHK_RESULT_COUNT\", FloatType(), True),\n",
    "                              StructField(\"request_id\", StringType(), True),\n",
    "                              StructField(\"weight\", FloatType(), True),\n",
    "                              StructField(\"weight_percent\", FloatType(), True),\n",
    "                              StructField(\"index_no\", IntegerType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_result(final_res):\n",
    "            # Calculate weights and normalize\n",
    "            final_res['importance'] = final_res['importance'].astype(float)\n",
    "            final_res = final_res.query(\"importance > 0\")\n",
    "            final_res['weight'] = final_res['importance'] / final_res['importance'].sum()\n",
    "            final_res['weight_percent'] = final_res['weight'] * 100\n",
    "            final_res = final_res.sort_values('weight', ascending=False)\n",
    "\n",
    "            final_res['index_no'] = [i + 1 for i in range(len(final_res))]\n",
    "            final_res['AVG_SPEC_CHK_RESULT_COUNT'] = 0.0\n",
    "            final_res['request_id'] = request_id\n",
    "            final_res = final_res.drop(['importance', 'temp'], axis=1)\n",
    "            return final_res\n",
    "\n",
    "        return df.groupby(by).apply(get_result)\n",
    "\n",
    "    def run(self):\n",
    "        df = self.df.filter('importance > 0')\n",
    "        print(\"importance > 0 : \", df.count())\n",
    "        df = df.withColumn('temp', lit(0))\n",
    "        res = self.split_calculate_features(df=df, grpby_list=self.grpby_list, by='temp')\n",
    "        res = res.withColumn('temp', lit(1))\n",
    "        final_res = self.add_certain_column(df=res, grpby_list=self.grpby_list, request_id=self.request_id, by='temp')\n",
    "        final_res = final_res.withColumnRenamed('OPE_NO', 'OPER_NO')\n",
    "        return final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c927140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_site_columns = good_site\n",
    "bad_site_columns = bad_site\n",
    "\n",
    "good_site_columns = list(set(good_site_columns))\n",
    "bad_site_columns = list(set(bad_site_columns))\n",
    "site_columns = good_site_columns + bad_site_columns\n",
    "\n",
    "grpby_list = ['OPE_NO']\n",
    "\n",
    "columns_list = grpby_list + ['WAFER_ID', 'INLINE_PARAMETER_ID', 'SITE_COUNT', 'AVERAGE'] + site_columns\n",
    "\n",
    "key_words = ['CXS', 'CYS', 'FDS']\n",
    "\n",
    "convert_to_numeric_list = ['SITE_COUNT', 'AVERAGE'] + site_columns\n",
    "\n",
    "certain_column = 'INLINE_PARAMETER_ID'\n",
    "\n",
    "merge_operno_list = merge_operno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95b46d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31791"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocess = DataPreprocessorForInline(df=df1,\n",
    "                                                  columns_list=columns_list,\n",
    "                                                  certain_column=certain_column,\n",
    "                                                  key_words=key_words,\n",
    "                                                  convert_to_numeric_list=convert_to_numeric_list,\n",
    "                                                  merge_operno_list=merge_operno_list).run()\n",
    "df_preprocess.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1f7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "001eb97b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = FitInlineModelBySite(df=df_preprocess,\n",
    "                           grpby_list=grpby_list,\n",
    "                           good_site_columns=good_site_columns,\n",
    "                           bad_site_columns=bad_site_columns,\n",
    "                           process_miss_site_mode='drop',\n",
    "                           columns_to_process=['AVERAGE', 'MAX_VAL', 'MEDIAN', 'MIN_VAL', 'STD_DEV',\n",
    "                                               'PERCENTILE_25', 'PERCENTILE_75'],\n",
    "                           missing_value_threshold=0.6,\n",
    "                           model='pca').run()\n",
    "# res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea00d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pandas = res.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "018ea4e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# res_pandas.to_csv(\"res_pandas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c0816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af4a3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = pd.read_csv(\"res_pandas.csv\", index_col=0)\n",
    "# res\n",
    "\n",
    "# res = ps.from_pandas(res).to_spark()\n",
    "# res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd86b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_res = SplitInlineModelResults(df=res, grpby_list=grpby_list, request_id=request_id).run()\n",
    "final_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3bc92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c51fa2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d37a947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6b7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe7d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e0f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ca8432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58f69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
