{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e31e9e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "from pca import pca\n",
    "from typing import Union, List, Dict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, lit, col, when\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1838b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/python-3.9.13/bin/python3'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"pandas_udf\") \\\n",
    "    .config('spark.sql.session.timeZone', 'Asia/Shanghai') \\\n",
    "    .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "    .config('spark.driver.memory', '8g') \\\n",
    "    .config('spark.driver.cores', '12') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .config('spark.executor.cores', '12') \\\n",
    "    .config('spark.cores.max', '12') \\\n",
    "    .config('spark.driver.host', '192.168.22.28') \\\n",
    "    .master(\"spark://192.168.12.47:7077,192.168.12.48:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43bcb4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAFER_ID</th>\n",
       "      <th>OPE_NO</th>\n",
       "      <th>INLINE_PARAMETER_ID</th>\n",
       "      <th>MEASURE_TIME</th>\n",
       "      <th>RANGE_INDEX</th>\n",
       "      <th>FAB_ID</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>LOT_ID</th>\n",
       "      <th>AVERAGE</th>\n",
       "      <th>MAX_VAL</th>\n",
       "      <th>...</th>\n",
       "      <th>ACT_CODE</th>\n",
       "      <th>ETL_INSERT_TIME</th>\n",
       "      <th>ETL_ARC_FLAG</th>\n",
       "      <th>ETL_BATCH_SYNC_TS</th>\n",
       "      <th>ETL_DEL_FLAG</th>\n",
       "      <th>ETL_DS_JOB_NM</th>\n",
       "      <th>ETL_SRC_DB</th>\n",
       "      <th>ETL_SRC_TBL</th>\n",
       "      <th>ETL_TBL_OPER_TS</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IKAS02.082-15</td>\n",
       "      <td>IKAS.OPEN.1</td>\n",
       "      <td>01W0</td>\n",
       "      <td>2023-07-08 16:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-08 16:44:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IKAS02.082-15</td>\n",
       "      <td>IKAS.OPEN.3</td>\n",
       "      <td>YTW0</td>\n",
       "      <td>2023-07-18 13:31:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-18 13:41:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IKAS02.082-15</td>\n",
       "      <td>IKAS.OPEN.3</td>\n",
       "      <td>YMW0</td>\n",
       "      <td>2023-07-18 13:31:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-18 13:41:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IKAS02.082-15</td>\n",
       "      <td>IKAS.OPEN.3</td>\n",
       "      <td>YSIG</td>\n",
       "      <td>2023-07-16 21:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-16 21:19:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IKAS02.082-15</td>\n",
       "      <td>IKAS.OPEN.3</td>\n",
       "      <td>YSIG</td>\n",
       "      <td>2023-07-17 04:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-17 04:41:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32273</th>\n",
       "      <td>IKAS02.082-16</td>\n",
       "      <td>IKAS.OPEN.1</td>\n",
       "      <td>04W0</td>\n",
       "      <td>2023-07-08 16:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-08 16:44:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32274</th>\n",
       "      <td>IKAS02.082-16</td>\n",
       "      <td>IKAS.OPEN.1</td>\n",
       "      <td>05W0</td>\n",
       "      <td>2023-07-08 16:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-08 16:44:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32275</th>\n",
       "      <td>IKAS02.082-16</td>\n",
       "      <td>IKAS.OPEN.1</td>\n",
       "      <td>06W0</td>\n",
       "      <td>2023-07-08 16:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-08 16:44:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32276</th>\n",
       "      <td>IKAS02.082-16</td>\n",
       "      <td>IKAS.OPEN.1</td>\n",
       "      <td>07W0</td>\n",
       "      <td>2023-07-08 16:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-08 16:44:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32277</th>\n",
       "      <td>IKAS02.082-16</td>\n",
       "      <td>IKAS.OPEN.1</td>\n",
       "      <td>08W0</td>\n",
       "      <td>2023-07-08 16:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>IKAS.PROJECT.0A02</td>\n",
       "      <td>IKAS02.082030</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-07-08 16:44:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32278 rows × 144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            WAFER_ID       OPE_NO INLINE_PARAMETER_ID         MEASURE_TIME  \\\n",
       "0      IKAS02.082-15  IKAS.OPEN.1                01W0  2023-07-08 16:30:00   \n",
       "1      IKAS02.082-15  IKAS.OPEN.3                YTW0  2023-07-18 13:31:00   \n",
       "2      IKAS02.082-15  IKAS.OPEN.3                YMW0  2023-07-18 13:31:00   \n",
       "3      IKAS02.082-15  IKAS.OPEN.3                YSIG  2023-07-16 21:09:00   \n",
       "4      IKAS02.082-15  IKAS.OPEN.3                YSIG  2023-07-17 04:30:00   \n",
       "...              ...          ...                 ...                  ...   \n",
       "32273  IKAS02.082-16  IKAS.OPEN.1                04W0  2023-07-08 16:30:00   \n",
       "32274  IKAS02.082-16  IKAS.OPEN.1                05W0  2023-07-08 16:30:00   \n",
       "32275  IKAS02.082-16  IKAS.OPEN.1                06W0  2023-07-08 16:30:00   \n",
       "32276  IKAS02.082-16  IKAS.OPEN.1                07W0  2023-07-08 16:30:00   \n",
       "32277  IKAS02.082-16  IKAS.OPEN.1                08W0  2023-07-08 16:30:00   \n",
       "\n",
       "       RANGE_INDEX FAB_ID         PRODUCT_ID         LOT_ID   AVERAGE  \\\n",
       "0                0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.064000   \n",
       "1                0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.000393   \n",
       "2                0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.002098   \n",
       "3                0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.006500   \n",
       "4                0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.017300   \n",
       "...            ...    ...                ...            ...       ...   \n",
       "32273            0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.064500   \n",
       "32274            0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.064100   \n",
       "32275            0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.064400   \n",
       "32276            0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.064300   \n",
       "32277            0     N1  IKAS.PROJECT.0A02  IKAS02.082030  0.064200   \n",
       "\n",
       "       MAX_VAL  ...  ACT_CODE      ETL_INSERT_TIME  ETL_ARC_FLAG  \\\n",
       "0          NaN  ...       NaN  2023-07-08 16:44:00             0   \n",
       "1          NaN  ...       NaN  2023-07-18 13:41:00             0   \n",
       "2          NaN  ...       NaN  2023-07-18 13:41:00             0   \n",
       "3          NaN  ...       NaN  2023-07-16 21:19:00             0   \n",
       "4          NaN  ...       NaN  2023-07-17 04:41:00             0   \n",
       "...        ...  ...       ...                  ...           ...   \n",
       "32273      NaN  ...       NaN  2023-07-08 16:44:00             0   \n",
       "32274      NaN  ...       NaN  2023-07-08 16:44:00             0   \n",
       "32275      NaN  ...       NaN  2023-07-08 16:44:00             0   \n",
       "32276      NaN  ...       NaN  2023-07-08 16:44:00             0   \n",
       "32277      NaN  ...       NaN  2023-07-08 16:44:00             0   \n",
       "\n",
       "         ETL_BATCH_SYNC_TS  ETL_DEL_FLAG  ETL_DS_JOB_NM  ETL_SRC_DB  \\\n",
       "0      1970-01-01 00:00:00             0            NaN         NaN   \n",
       "1      1970-01-01 00:00:00             0            NaN         NaN   \n",
       "2      1970-01-01 00:00:00             0            NaN         NaN   \n",
       "3      1970-01-01 00:00:00             0            NaN         NaN   \n",
       "4      1970-01-01 00:00:00             0            NaN         NaN   \n",
       "...                    ...           ...            ...         ...   \n",
       "32273  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32274  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32275  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32276  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32277  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "\n",
       "       ETL_SRC_TBL      ETL_TBL_OPER_TS  label  \n",
       "0              NaN  1970-01-01 00:00:00      1  \n",
       "1              NaN  1970-01-01 00:00:00      1  \n",
       "2              NaN  1970-01-01 00:00:00      1  \n",
       "3              NaN  1970-01-01 00:00:00      1  \n",
       "4              NaN  1970-01-01 00:00:00      1  \n",
       "...            ...                  ...    ...  \n",
       "32273          NaN  1970-01-01 00:00:00      1  \n",
       "32274          NaN  1970-01-01 00:00:00      1  \n",
       "32275          NaN  1970-01-01 00:00:00      1  \n",
       "32276          NaN  1970-01-01 00:00:00      1  \n",
       "32277          NaN  1970-01-01 00:00:00      1  \n",
       "\n",
       "[32278 rows x 144 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = pd.read_csv(\"D:/Jupyterfiles/晶合MVAFDC_general开发/MVAanlysisDevelop/inline_algorithm/codes_version7/inline_test_data3_bysite.csv\")\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e39a956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32278"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = ps.from_pandas(df_pandas).to_spark()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb66b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6d236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_JSON_config(df: pd.DataFrame):\n",
    "    request_id = df[\"requestId\"].values[0]\n",
    "    request_params = df[\"requestParam\"].values[0]\n",
    "    parse_dict = json.loads(request_params)\n",
    "\n",
    "    # PRODUCT_ID, PROG1, EQP, CHAMBER, OPER_NO存在部分合并的情况\n",
    "    try:\n",
    "        # OPER_NO的部分合并结果\n",
    "        merge_operno = list(parse_dict.get('mergeOperno')) if parse_dict.get('mergeOperno') else None\n",
    "    except KeyError:\n",
    "        merge_operno = None\n",
    "\n",
    "    try:\n",
    "        # PROG1的部分合并结果\n",
    "        merge_prodg1 = list(parse_dict.get('mergeProdg1')) if parse_dict.get('mergeProdg1') else None\n",
    "    except KeyError:\n",
    "        merge_prodg1 = None\n",
    "\n",
    "    try:\n",
    "        # PRODUCT_ID的部分合并结果\n",
    "        merge_product = list(parse_dict.get('mergeProductId')) if parse_dict.get('mergeProductId') else None\n",
    "    except KeyError:\n",
    "        merge_product = None\n",
    "\n",
    "    try:\n",
    "        # EQP的部分合并结果\n",
    "        merge_eqp = list(parse_dict.get('mergeEqp')) if parse_dict.get('mergeEqp') else None\n",
    "    except KeyError:\n",
    "        merge_eqp = None\n",
    "\n",
    "    try:\n",
    "        # CHAMBER的部分合并结果\n",
    "        merge_chamber = list(parse_dict.get('mergeChamber')) if parse_dict.get('mergeChamber') else None\n",
    "    except KeyError:\n",
    "        merge_chamber = None\n",
    "\n",
    "    # 获取good_site和bad_site\n",
    "    try:\n",
    "        good_site = list(parse_dict.get('goodSite')) if parse_dict.get('goodSite') else None\n",
    "    except KeyError:\n",
    "        good_site = None\n",
    "\n",
    "    try:\n",
    "        bad_site = list(parse_dict.get('badSite')) if parse_dict.get('badSite') else None\n",
    "    except KeyError:\n",
    "        bad_site = None\n",
    "\n",
    "    # group by 子句中的字段\n",
    "    group_by_list = parse_dict.get(\"groupByList\")\n",
    "    if group_by_list is None or len(group_by_list) == 0:\n",
    "        group_by_list = [\"PRODG1\", \"PRODUCT_ID\", \"OPER_NO\", \"EQP_NAME\", \"TOOL_NAME\"]\n",
    "        # PRODUCT_ID, PROG1, CHAMBER 这3个存在一键合并的切换开关\n",
    "        # 且一键合并PROG1时会自动一键合并PRODUCT_ID\n",
    "        flag_merge_prodg1 = parse_dict.get('flagMergeAllProdg1')\n",
    "        flag_merge_product_id = parse_dict.get('flagMergeAllProductId')\n",
    "        flag_merge_chamber = parse_dict.get('flagMergeAllChamber')\n",
    "\n",
    "        if flag_merge_prodg1 == '1':\n",
    "            # 一键合并PROG1时，部分合并PROG1和PRODUCT_ID的情况都会被忽略\n",
    "            merge_prodg1 = None\n",
    "            merge_product = None\n",
    "            group_by_list = ['OPER_NO', \"EQP_NAME\", 'TOOL_NAME']\n",
    "            if flag_merge_chamber == '1':\n",
    "                group_by_list = ['OPER_NO', \"EQP_NAME\"]\n",
    "        elif flag_merge_product_id == '1':\n",
    "            # 一键合并PRODUCT_ID时，部分合并PRODUCT_ID的情况会被忽略\n",
    "            merge_product = None\n",
    "            group_by_list = [\"PRODG1\", \"OPER_NO\", \"EQP_NAME\", \"TOOL_NAME\"]\n",
    "            if flag_merge_chamber == '1':\n",
    "                # 一键合并CHAMBER时，部分合并CHAMBER的情况会被忽略\n",
    "                group_by_list = [\"PRODG1\", 'OPER_NO', \"EQP_NAME\"]\n",
    "        elif flag_merge_chamber == '1':\n",
    "            merge_chamber = None\n",
    "            group_by_list = [\"PRODG1\", \"PRODUCT_ID\", \"OPER_NO\", \"EQP_NAME\"]\n",
    "\n",
    "    return parse_dict, request_id, group_by_list, merge_operno, merge_prodg1, merge_product, merge_eqp, merge_chamber, good_site, bad_site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "131ee2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_config_ = {\"requestId\": \"715\", \"algorithm\": \"inline_by_site\",\n",
    "                    \"requestParam\": {\"dateRange\": {\"start\": \"2021-12-14 16:22:40\", \"end\": \"2024-03-14 16:22:40\"},\n",
    "                                     \"uploadId\": \"e6eefec4ba2e4456bdde5efbcf47c438\",\n",
    "                                     \"goodSite\": [\"SITE16_VAL\", \"SITE7_VAL\", \"SITE11_VAL\", \"SITE14_VAL\", \"SITE17_VAL\",\n",
    "                                                  \"SITE20_VAL\",\n",
    "                                                  \"SITE9_VAL\", \"SITE12_VAL\", \"SITE13_VAL\", \"SITE15_VAL\", \"SITE18_VAL\",\n",
    "                                                  \"SITE19_VAL\",\n",
    "                                                  \"SITE10_VAL\", \"SITE8_VAL\"],\n",
    "                                     \"badSite\": [\"SITE6_VAL\", \"SITE2_VAL\", \"SITE3_VAL\", \"SITE5_VAL\", \"SITE1_VAL\",\n",
    "                                                 \"SITE4_VAL\"],\n",
    "                                     \"flagMergeAllProdg1\": \"0\", \"flagMergeAllProductId\": \"0\",\n",
    "                                     \"flagMergeAllChamber\": \"0\",\n",
    "                                     \"mergeProdg1\": [], \"mergeProductId\": [], \"mergeEqp\": [], \"mergeChamber\": [],\n",
    "                                     \"mergeOperno\": []}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc8d2188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse_dict:\n",
      "{'dateRange': {'start': '2021-12-14 16:22:40', 'end': '2024-03-14 16:22:40'}, 'uploadId': 'e6eefec4ba2e4456bdde5efbcf47c438', 'goodSite': ['SITE16_VAL', 'SITE7_VAL', 'SITE11_VAL', 'SITE14_VAL', 'SITE17_VAL', 'SITE20_VAL', 'SITE9_VAL', 'SITE12_VAL', 'SITE13_VAL', 'SITE15_VAL', 'SITE18_VAL', 'SITE19_VAL', 'SITE10_VAL', 'SITE8_VAL'], 'badSite': ['SITE6_VAL', 'SITE2_VAL', 'SITE3_VAL', 'SITE5_VAL', 'SITE1_VAL', 'SITE4_VAL'], 'flagMergeAllProdg1': '0', 'flagMergeAllProductId': '0', 'flagMergeAllChamber': '0', 'mergeProdg1': [], 'mergeProductId': [], 'mergeEqp': [], 'mergeChamber': [], 'mergeOperno': []}\n",
      "request_id:\n",
      "715\n",
      "grpby_list:\n",
      "['PRODG1', 'PRODUCT_ID', 'OPER_NO', 'EQP_NAME', 'TOOL_NAME']\n",
      "merge_operno:\n",
      "None\n",
      "merge_prodg1:\n",
      "None\n",
      "merge_product:\n",
      "None\n",
      "merge_eqp:\n",
      "None\n",
      "merge_chamber:\n",
      "None\n",
      "good_site:\n",
      "['SITE16_VAL', 'SITE7_VAL', 'SITE11_VAL', 'SITE14_VAL', 'SITE17_VAL', 'SITE20_VAL', 'SITE9_VAL', 'SITE12_VAL', 'SITE13_VAL', 'SITE15_VAL', 'SITE18_VAL', 'SITE19_VAL', 'SITE10_VAL', 'SITE8_VAL']\n",
      "bad_site:\n",
      "['SITE6_VAL', 'SITE2_VAL', 'SITE3_VAL', 'SITE5_VAL', 'SITE1_VAL', 'SITE4_VAL']\n"
     ]
    }
   ],
   "source": [
    "df_info_ = pd.DataFrame({\"requestId\": [json_config_[\"requestId\"]],\n",
    "                         \"requestParam\": [json.dumps(json_config_[\"requestParam\"])]})\n",
    "\n",
    "# 解析JSON并且读取数据\n",
    "parse_dict, request_id, grpby_list, merge_operno, merge_prodg1, merge_product, merge_eqp, merge_chamber, good_site, bad_site = parse_JSON_config(\n",
    "    df_info_)\n",
    "print(\"parse_dict:\")\n",
    "print(parse_dict)\n",
    "print(\"request_id:\")\n",
    "print(request_id)\n",
    "print(\"grpby_list:\")\n",
    "print(grpby_list)\n",
    "print(\"merge_operno:\")\n",
    "print(merge_operno)\n",
    "print(\"merge_prodg1:\")\n",
    "print(merge_prodg1)\n",
    "print(\"merge_product:\")\n",
    "print(merge_product)\n",
    "print(\"merge_eqp:\")\n",
    "print(merge_eqp)\n",
    "print(\"merge_chamber:\")\n",
    "print(merge_chamber)\n",
    "print(\"good_site:\")\n",
    "print(good_site)\n",
    "print(\"bad_site:\")\n",
    "print(bad_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70daae75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd88048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessorForInline:\n",
    "    def __init__(self,\n",
    "                 df: pyspark.sql.dataframe,\n",
    "                 columns_list: list[str],\n",
    "                 certain_column: str,\n",
    "                 key_words: list[str],\n",
    "                 convert_to_numeric_list: list[str],\n",
    "                 merge_operno_list: List[Dict[str, List[str]]]):\n",
    "        self.df = df\n",
    "        self.columns_list = columns_list\n",
    "        self.certain_column = certain_column\n",
    "        self.key_words = key_words\n",
    "        self.convert_to_numeric_list = convert_to_numeric_list\n",
    "        self.merge_operno_list = merge_operno_list\n",
    "\n",
    "    def select_columns(self, df):\n",
    "        return df.select(self.columns_list)\n",
    "\n",
    "    def exclude_some_data(self, df):\n",
    "        key_words_str = '|'.join(self.key_words)\n",
    "        df_filtered = df.filter(~col(self.certain_column).rlike(key_words_str))\n",
    "        return df_filtered\n",
    "\n",
    "    def pre_process(self, df):\n",
    "        for column in self.convert_to_numeric_list:\n",
    "            df = df.withColumn(column, col(column).cast('double'))\n",
    "        if 'SITE_COUNT' in self.convert_to_numeric_list:\n",
    "            self.convert_to_numeric_list.remove('SITE_COUNT')\n",
    "        df = df.dropna(subset=self.convert_to_numeric_list, how='all')\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def integrate_columns(df, merge_operno_list: List[Dict[str, List[str]]]) -> pyspark.sql.dataframe:\n",
    "        \"\"\"\n",
    "        Integrate columns in the DataFrame based on the provided list.\n",
    "\n",
    "        :param df: The input DataFrame.\n",
    "        :param merge_operno_list: A list of dictionaries where each dictionary contains values to be merged.\n",
    "               Example: [{'2F.CDS10_XX.TDS01': ['2F.CDS10', 'XX.TDS01']},\n",
    "                         {'2F.CDS20_XX.CDS20': ['2F.CDS20', 'XX.CDS20']}]\n",
    "        :return: DataFrame with 'OPER_NO' and other specified columns integrated according to the merge rules.\n",
    "        \"\"\"\n",
    "        # split using comma\n",
    "        splitter_comma = \",\"\n",
    "        if merge_operno_list is not None and len(merge_operno_list) > 0:\n",
    "            # Extract values from each dictionary in merge_operno_list and create a list\n",
    "            values_to_replace = [list(rule.values())[0] for rule in merge_operno_list]\n",
    "            # Concatenate values from each dictionary\n",
    "            merged_values = [splitter_comma.join(list(rule.values())[0]) for rule in merge_operno_list]\n",
    "\n",
    "            # Replace values in 'OPER_NO' column based on the rules defined in merge_operno_list\n",
    "            for values, replacement_value in zip(values_to_replace, merged_values):\n",
    "                df = df.withColumn(\"OPE_NO\", when(col(\"OPE_NO\").isin(values), replacement_value).otherwise(col(\"OPE_NO\")))\n",
    "        return df\n",
    "\n",
    "    def run(self):\n",
    "        df_select = self.select_columns(df=self.df)\n",
    "        df_esd = self.exclude_some_data(df=df_select)\n",
    "        df_pp = self.pre_process(df=df_esd)\n",
    "        df_integrate = self.integrate_columns(df=df_pp, merge_operno_list=self.merge_operno_list)\n",
    "        return df_integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a42caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractFeaturesBySite:\n",
    "    @staticmethod\n",
    "    def process_missing_values_for_site(df: pd.DataFrame,\n",
    "                                        good_site_columns: list[str],\n",
    "                                        bad_site_columns: list[str],\n",
    "                                        missing_value_threshold: Union[int, float] = 0.6,\n",
    "                                        process_miss_site_mode: str = 'drop') -> pd.DataFrame:\n",
    "        assert process_miss_site_mode in ['drop', 'fill']\n",
    "        site_columns = good_site_columns + bad_site_columns\n",
    "        if process_miss_site_mode == 'drop':\n",
    "            # drop rows based on the missing value threshold\n",
    "            df = df.dropna(subset=site_columns, thresh=missing_value_threshold)\n",
    "        else:\n",
    "            # fill missing values in the corresponding site rows using the AVERAGE of that row\n",
    "            df[site_columns] = df[site_columns].apply(lambda column: column.fillna(df['AVERAGE']))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_statistics(row):\n",
    "        return pd.Series({\n",
    "            'MAX_VAL': row.max(),\n",
    "            'MIN_VAL': row.min(),\n",
    "            'MEDIAN': row.median(),\n",
    "            'AVERAGE': row.mean(),\n",
    "            'STD_DEV': row.std(),\n",
    "            'PERCENTILE_25': row.quantile(0.25),\n",
    "            'PERCENTILE_75': row.quantile(0.75)})\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_site_stats(df: pd.DataFrame, site_columns: list[str], good_or_bad: str) -> pd.DataFrame:\n",
    "        assert good_or_bad in ['good', 'bad'], \"Label could only be 'good' or 'bad'\"\n",
    "        selected_df = df[['WAFER_ID', 'OPE_NO', 'INLINE_PARAMETER_ID'] + site_columns].reset_index(drop=True)\n",
    "        # Perform statistical calculations for each row\n",
    "        side_features = selected_df.apply(lambda row: ExtractFeaturesBySite.calculate_statistics(row[site_columns]), axis=1)\n",
    "        side_features = side_features.fillna(0)\n",
    "        df_with_features = pd.concat([selected_df, side_features], axis=1)\n",
    "        if good_or_bad == 'good':\n",
    "            df_with_features['label'] = 0\n",
    "        else:\n",
    "            df_with_features['label'] = 1\n",
    "        return df_with_features\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_features_by_site(df: pd.DataFrame,\n",
    "                                 good_site_columns: list[str],\n",
    "                                 bad_site_columns: list[str],\n",
    "                                 missing_value_threshold: Union[int, float] = 0.6,\n",
    "                                 process_miss_site_mode: str = 'drop') -> Union[pd.DataFrame, None]:\n",
    "        \"\"\"\n",
    "        Extracts features from a DataFrame based on good and bad site columns.\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The input DataFrame.\n",
    "        - good_site_columns (list): List of columns representing good sites.\n",
    "        - bad_site_columns (list): List of columns representing bad sites.\n",
    "        - missing_value_threshold (Union[int, float]): Threshold for missing values.\n",
    "        - process_miss_site_mode (str): Mode for handling missing values in site columns, e.g. drop or fill\n",
    "        Returns:\n",
    "        - Union[pd.DataFrame, None]: DataFrame with extracted features or None if no data is available.\n",
    "        \"\"\"\n",
    "        df_pandas_specific_oper = ExtractFeaturesBySite.process_missing_values_for_site(df=df,\n",
    "                                                                                        good_site_columns=good_site_columns,\n",
    "                                                                                        bad_site_columns=bad_site_columns,\n",
    "                                                                                        missing_value_threshold=missing_value_threshold,\n",
    "                                                                                        process_miss_site_mode=process_miss_site_mode)\n",
    "        if df_pandas_specific_oper.shape[0] != 0:\n",
    "            side_with_features1 = ExtractFeaturesBySite.calculate_site_stats(df_pandas_specific_oper, good_site_columns,\n",
    "                                                                             good_or_bad='good')\n",
    "            side_with_features2 = ExtractFeaturesBySite.calculate_site_stats(df_pandas_specific_oper, bad_site_columns,\n",
    "                                                                             good_or_bad='bad')\n",
    "            side_with_features1_select = side_with_features1[\n",
    "                ['WAFER_ID', 'OPE_NO', 'INLINE_PARAMETER_ID', 'MAX_VAL', 'MIN_VAL', 'MEDIAN',\n",
    "                 'AVERAGE', 'STD_DEV', 'PERCENTILE_25', 'PERCENTILE_75', 'label']]\n",
    "            side_with_features2_select = side_with_features2[\n",
    "                ['WAFER_ID', 'OPE_NO', 'INLINE_PARAMETER_ID', 'MAX_VAL', 'MIN_VAL', 'MEDIAN',\n",
    "                 'AVERAGE', 'STD_DEV', 'PERCENTILE_25', 'PERCENTILE_75', 'label']]\n",
    "            side_with_features_all = pd.concat([side_with_features1_select, side_with_features2_select], axis=0)\n",
    "            return side_with_features_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7be5982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitInlineModelBySite:\n",
    "    def __init__(self,\n",
    "                 df: pyspark.sql.dataframe,\n",
    "                 by: list[str],\n",
    "                 good_site_columns: list[str],\n",
    "                 bad_site_columns: list[str],\n",
    "                 process_miss_site_mode: str,\n",
    "                 columns_to_process: list[str],\n",
    "                 missing_value_threshold: Union[int, float],\n",
    "                 model: str = 'pca'):\n",
    "        \"\"\"\n",
    "        Initialize the FitInlineModelBySite object.\n",
    "\n",
    "        Parameters:\n",
    "        - df: pyspark.sql.dataframe, the input data\n",
    "        - by: list[str], the grouping variable, inline data should be [\"OPE_NO\"]\n",
    "        - good_site_columns: List of str, column names for good sites\n",
    "        - bad_site_columns: List of str, column names for bad sites\n",
    "        - process_miss_site_mode: str, mode for handling missing values in site data, e.g. drop or fill\n",
    "        - columns_to_process: List of str, columns to process in missing value functions\n",
    "        - missing_value_threshold: Union[int, float], threshold for missing values\n",
    "        - model: str, default is 'pca', other options include 'rf' for random forest\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.by = by\n",
    "        self.model = model\n",
    "        self.good_site_columns = good_site_columns\n",
    "        self.bad_site_columns = bad_site_columns\n",
    "        self.process_miss_site_mode = process_miss_site_mode\n",
    "        self.columns_to_process = columns_to_process\n",
    "        self.missing_value_threshold = missing_value_threshold\n",
    "\n",
    "    @staticmethod\n",
    "    def process_missing_values(df, columns_to_process, missing_value_threshold):\n",
    "        for column in columns_to_process:\n",
    "            missing_percentage = df[column].isnull().mean()\n",
    "            if missing_percentage > missing_value_threshold:\n",
    "                df = df.drop(columns=[column])\n",
    "            else:\n",
    "                df[column] = df[column].fillna(df[column].mean())\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pivot_table(df, columns_to_process, missing_value_threshold):\n",
    "        df = FitInlineModelBySite.process_missing_values(df, columns_to_process, missing_value_threshold)\n",
    "        index_list = ['WAFER_ID', 'label']\n",
    "        values_list = df.columns.difference(['WAFER_ID', 'OPE_NO', 'INLINE_PARAMETER_ID', 'label'])\n",
    "        pivot_result = df.pivot_table(index=index_list,\n",
    "                                      columns=['OPE_NO', 'INLINE_PARAMETER_ID'],\n",
    "                                      values=values_list)\n",
    "        pivot_result.columns = pivot_result.columns.map('#'.join)\n",
    "        pivot_result = FitInlineModelBySite.process_missing_values(pivot_result, pivot_result.columns, missing_value_threshold)\n",
    "        pivot_result = pivot_result.reset_index(drop=False)\n",
    "        # Remove completely identical columns\n",
    "        for column in pivot_result.columns.difference(index_list):\n",
    "            if pivot_result[column].nunique() == 1:\n",
    "                pivot_result = pivot_result.drop(column, axis=1)\n",
    "        return pivot_result\n",
    "\n",
    "    def fit_pca_model(self):\n",
    "        schema_all = StructType([StructField(\"features\", StringType(), True),\n",
    "                                 StructField(\"importance\", FloatType(), True)])\n",
    "        good_site_columns = self.good_site_columns\n",
    "        bad_site_columns = self.bad_site_columns\n",
    "        missing_value_threshold = self.missing_value_threshold\n",
    "        process_miss_site_mode = self.process_miss_site_mode\n",
    "        columns_to_process = self.columns_to_process\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_model_result(df_run):\n",
    "            side_with_features_all = ExtractFeaturesBySite.extract_features_by_site(df=df_run,\n",
    "                                                                                    good_site_columns=good_site_columns,\n",
    "                                                                                    bad_site_columns=bad_site_columns,\n",
    "                                                                                    missing_value_threshold=missing_value_threshold,\n",
    "                                                                                    process_miss_site_mode=process_miss_site_mode)\n",
    "            if side_with_features_all is None:\n",
    "                return pd.DataFrame({\"features\": \"STATS#OPE#PARAM\", \"importance\": -100}, index=[0])\n",
    "\n",
    "            pivot_result = FitInlineModelBySite.get_pivot_table(df=side_with_features_all,\n",
    "                                                                columns_to_process=columns_to_process,\n",
    "                                                                missing_value_threshold=missing_value_threshold)\n",
    "            x_train = pivot_result[pivot_result.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "\n",
    "            if x_train.shape[1] > 1:\n",
    "                n_components = min(min(x_train.shape) - 2, 20)\n",
    "                model = pca(n_components=n_components, verbose=None)\n",
    "                results = model.fit_transform(x_train)\n",
    "                res_top = results['topfeat']\n",
    "                res_top_select = res_top[res_top['type'] == 'best'][['feature', 'loading']]\n",
    "                res_top_select['importance'] = abs(res_top_select['loading'])\n",
    "                res_top_select = res_top_select.rename(columns={'feature': 'features'}).drop(\"loading\", axis=1).drop_duplicates()\n",
    "                return res_top_select\n",
    "            else:\n",
    "                res_top_select = pd.DataFrame({\"features\": \"STATS#OPE#PARAM\", \"importance\": -101}, index=[0])\n",
    "                return res_top_select\n",
    "        return self.df.groupby(self.by).apply(get_model_result)\n",
    "\n",
    "    def fit_rf_model(self):\n",
    "        schema_all = StructType([StructField(\"features\", StringType(), True),\n",
    "                                 StructField(\"importance\", FloatType(), True)])\n",
    "        good_site_columns = self.good_site_columns\n",
    "        bad_site_columns = self.bad_site_columns\n",
    "        missing_value_threshold = self.missing_value_threshold\n",
    "        process_miss_site_mode = self.process_miss_site_mode\n",
    "        columns_to_process = self.columns_to_process\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_model_result(df_run):\n",
    "            side_with_features_all = ExtractFeaturesBySite.extract_features_by_site(df=df_run,\n",
    "                                                                                    good_site_columns=good_site_columns,\n",
    "                                                                                    bad_site_columns=bad_site_columns,\n",
    "                                                                                    missing_value_threshold=missing_value_threshold,\n",
    "                                                                                    process_miss_site_mode=process_miss_site_mode)\n",
    "            if side_with_features_all is None:\n",
    "                return pd.DataFrame({\"features\": \"STATS#OPE#PARAM\", \"importance\": -100}, index=[0])\n",
    "\n",
    "            pivot_result = FitInlineModelBySite.get_pivot_table(df=side_with_features_all,\n",
    "                                                                columns_to_process=columns_to_process,\n",
    "                                                                missing_value_threshold=missing_value_threshold)\n",
    "            x_train = pivot_result[pivot_result.columns.difference(['WAFER_ID', 'label']).tolist()]\n",
    "            y_train = pivot_result[['label']]\n",
    "            if min(x_train.shape) > 4 and y_train['label'].nunique() > 1:\n",
    "                pipe = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='constant', fill_value=-999)),\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('model', RandomForestClassifier(random_state=2024))])\n",
    "                param_grid = {'model__n_estimators': [*range(10, 60, 10)],\n",
    "                              'model__max_depth': [*range(5, 50, 10)],\n",
    "                              'model__min_samples_split': [2, 5],\n",
    "                              'model__min_samples_leaf': [1, 3]}\n",
    "                grid = GridSearchCV(estimator=pipe, scoring='roc_auc', param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "                grid.fit(x_train.values, y_train.values.ravel())\n",
    "                roc_auc_score_ = grid.best_score_\n",
    "                if roc_auc_score_ >= 0.6:\n",
    "                    small_importance_res = pd.DataFrame({'features': x_train.columns,\n",
    "                                                         'importance': grid.best_estimator_.steps[2][1].feature_importances_})\n",
    "                    return small_importance_res\n",
    "                else:\n",
    "                    small_importance_res = pd.DataFrame({\"features\": \"STATS#OPE#PARAM\", \"importance\": -101}, index=[0])\n",
    "                    return small_importance_res\n",
    "            else:\n",
    "                small_importance_res = pd.DataFrame({\"features\": \"STATS#OPE#PARAM\", \"importance\": -102}, index=[0])\n",
    "                return small_importance_res\n",
    "        return self.df.groupby(self.by).apply(get_model_result)\n",
    "\n",
    "    def run(self):\n",
    "        if self.model == 'pca':\n",
    "            res = self.fit_pca_model()\n",
    "        elif self.model == 'rf':\n",
    "            res = self.fit_rf_model()\n",
    "        else:\n",
    "            res = None\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdff84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitInlineModelResults:\n",
    "    def __init__(self, df: pyspark.sql.dataframe, grpby_list: List[str], request_id: str):\n",
    "        self.df = df\n",
    "        self.grpby_list = grpby_list\n",
    "        self.request_id = request_id\n",
    "\n",
    "    @staticmethod\n",
    "    def split_features(df: pd.DataFrame, index: int) -> str:\n",
    "        return df['features'].apply(lambda x: x.split('#')[index])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_split_features(df: pd.DataFrame, grpby_list: List[str]) -> pd.DataFrame:\n",
    "        n_feats = len(grpby_list)\n",
    "        for i in range(n_feats):\n",
    "            df[grpby_list[i]] = SplitInlineModelResults.split_features(df, i + 1)\n",
    "\n",
    "        df['INLINE_PARAMETER_ID'] = SplitInlineModelResults.split_features(df, n_feats + 1)\n",
    "        df = df.drop(['features'], axis=1).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def split_calculate_features(df: pyspark.sql.dataframe, grpby_list: List[str], by: str) -> pyspark.sql.dataframe:\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"INLINE_PARAMETER_ID\", StringType(), True),\n",
    "                              StructField(\"importance\", FloatType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_model_result(df_run):\n",
    "            split_table = SplitInlineModelResults.get_split_features(df=df_run, grpby_list=grpby_list)\n",
    "            split_table_grpby = split_table.groupby(grpby_list + ['INLINE_PARAMETER_ID'])[\n",
    "                'importance'].sum().reset_index(drop=False)\n",
    "            return split_table_grpby\n",
    "\n",
    "        return df.groupby(by).apply(get_model_result)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_certain_column(df: pyspark.sql.dataframe, grpby_list: List[str], request_id: str,\n",
    "                           by: str) -> pyspark.sql.dataframe:\n",
    "        struct_fields = [StructField(col_, StringType(), True) for col_ in grpby_list]\n",
    "        struct_fields.extend([StructField(\"INLINE_PARAMETER_ID\", StringType(), True),\n",
    "                              StructField(\"AVG_SPEC_CHK_RESULT_COUNT\", FloatType(), True),\n",
    "                              StructField(\"request_id\", StringType(), True),\n",
    "                              StructField(\"weight\", FloatType(), True),\n",
    "                              StructField(\"weight_percent\", FloatType(), True),\n",
    "                              StructField(\"index_no\", IntegerType(), True)])\n",
    "        schema_all = StructType(struct_fields)\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_result(final_res):\n",
    "            # Calculate weights and normalize\n",
    "            final_res['importance'] = final_res['importance'].astype(float)\n",
    "            final_res = final_res.query(\"importance > 0\")\n",
    "            final_res['weight'] = final_res['importance'] / final_res['importance'].sum()\n",
    "            final_res['weight_percent'] = final_res['weight'] * 100\n",
    "            final_res = final_res.sort_values('weight', ascending=False)\n",
    "\n",
    "            final_res['index_no'] = [i + 1 for i in range(len(final_res))]\n",
    "            final_res['AVG_SPEC_CHK_RESULT_COUNT'] = 0.0\n",
    "            final_res['request_id'] = request_id\n",
    "            final_res = final_res.drop(['importance', 'temp'], axis=1)\n",
    "            return final_res\n",
    "        return df.groupby(by).apply(get_result)\n",
    "\n",
    "    def run(self):\n",
    "        df = self.df.withColumn('temp', lit(0))\n",
    "        res = self.split_calculate_features(df=df, by='temp')\n",
    "        res = res.withColumn('temp', lit(1))\n",
    "        final_res = self.add_certain_column(df=res, by='temp')\n",
    "        final_res = final_res.withColumn('request_id', lit(self.request_id))\n",
    "        return final_res\n",
    "\n",
    "\n",
    "# class ExertInlineBySite:\n",
    "#     @staticmethod\n",
    "#     def fit_by_site_model(df: pyspark.sql.dataframe,\n",
    "#                            request_id: str,\n",
    "#                            merge_operno_list: List[Dict[str, List[str]]],\n",
    "#                            good_site_columns: List[str],\n",
    "#                            bad_site_columns: List[str],\n",
    "#                            columns_list=None,\n",
    "#                            key_words=None,\n",
    "#                            convert_to_numeric_list=None,\n",
    "#                            grpby_list=None,\n",
    "#                            certain_column=None) -> Union[str, pyspark.sql.dataframe.DataFrame]:\n",
    "#         # drop duplicates\n",
    "#         good_site_columns = list(set(good_site_columns))\n",
    "#         bad_site_columns = list(set(bad_site_columns))\n",
    "#         site_columns = good_site_columns + bad_site_columns\n",
    "\n",
    "#         if columns_list is None:\n",
    "#             columns_list = ['WAFER_ID', 'OPE_NO', 'INLINE_PARAMETER_ID', 'SITE_COUNT', 'AVERAGE'] + site_columns\n",
    "\n",
    "#         if key_words is None:\n",
    "#             key_words = ['CXS', 'CYS', 'FDS']\n",
    "\n",
    "#         if convert_to_numeric_list is None:\n",
    "#             convert_to_numeric_list = ['SITE_COUNT', 'AVERAGE'] + site_columns\n",
    "\n",
    "#         if grpby_list is None:\n",
    "#             grpby_list = ['OPE_NO']\n",
    "\n",
    "#         if certain_column is None:\n",
    "#             certain_column = 'INLINE_PARAMETER_ID'\n",
    "\n",
    "#         df_preprocess = DataPreprocessorForInline(df=df,\n",
    "#                                                   columns_list=columns_list,\n",
    "#                                                   certain_column=certain_column,\n",
    "#                                                   key_words=key_words,\n",
    "#                                                   convert_to_numeric_list=convert_to_numeric_list,\n",
    "#                                                   merge_operno_list=merge_operno_list).run()\n",
    "#         print(df_preprocess.count())\n",
    "#         if df_preprocess.isEmpty():\n",
    "#             msg = 'No data of this type in the database!'\n",
    "#             raise RCABaseException(msg)\n",
    "\n",
    "#         res = FitInlineModelBySite(df=df_preprocess,\n",
    "#                                     by=grpby_list,\n",
    "#                                     model='pca',\n",
    "#                                     good_site_columns=good_site_columns,\n",
    "#                                     bad_site_columns=bad_site_columns,\n",
    "#                                     process_miss_site_mode='drop',\n",
    "#                                     columns_to_process=['AVERAGE', 'MAX_VAL', 'MEDIAN', 'MIN_VAL', 'STD_DEV',\n",
    "#                                                         'PERCENTILE_25', 'PERCENTILE_75'],\n",
    "#                                     missing_value_threshold=0.6).run()\n",
    "#         # res.show()\n",
    "#         if res.isEmpty():\n",
    "#             msg = 'No difference in this data. The output of the algorithm is 0.'\n",
    "#             raise RCABaseException(msg)\n",
    "\n",
    "#         final_res = SplitInlineModelResults(df=res, request_id=request_id).run()\n",
    "#         if final_res.isEmpty():\n",
    "#             msg = 'Temporary exception in adding columns to algorithm results'\n",
    "#             raise RCABaseException(msg)\n",
    "#         else:\n",
    "#             return final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efacaf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_site_columns = good_site\n",
    "bad_site_columns = bad_site\n",
    "\n",
    "good_site_columns = list(set(good_site_columns))\n",
    "bad_site_columns = list(set(bad_site_columns))\n",
    "site_columns = good_site_columns + bad_site_columns\n",
    "\n",
    "grpby_list = ['OPE_NO']\n",
    "\n",
    "columns_list = grpby_list + ['WAFER_ID', 'INLINE_PARAMETER_ID', 'SITE_COUNT', 'AVERAGE'] + site_columns\n",
    "\n",
    "key_words = ['CXS', 'CYS', 'FDS']\n",
    "\n",
    "convert_to_numeric_list = ['SITE_COUNT', 'AVERAGE'] + site_columns\n",
    "\n",
    "certain_column = 'INLINE_PARAMETER_ID'\n",
    "\n",
    "merge_operno_list = merge_operno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4622deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'715'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ee847b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31791"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocess = DataPreprocessorForInline(df=df1,\n",
    "                                                  columns_list=columns_list,\n",
    "                                                  certain_column=certain_column,\n",
    "                                                  key_words=key_words,\n",
    "                                                  convert_to_numeric_list=convert_to_numeric_list,\n",
    "                                                  merge_operno_list=merge_operno_list).run()\n",
    "df_preprocess.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0abed948",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|importance|\n",
      "+--------------------+----------+\n",
      "|PERCENTILE_75#IKA...|0.24843656|\n",
      "|PERCENTILE_75#IKA...|0.25839117|\n",
      "|MIN_VAL#IKAS.OPEN...|0.30845588|\n",
      "|PERCENTILE_75#IKA...|0.35949597|\n",
      "|MAX_VAL#IKAS.OPEN...| 0.3102229|\n",
      "|MIN_VAL#IKAS.OPEN...|0.41644922|\n",
      "|MAX_VAL#IKAS.OPEN...| 0.3663416|\n",
      "|MAX_VAL#IKAS.OPEN...|0.39013106|\n",
      "|MIN_VAL#IKAS.OPEN...|0.62744516|\n",
      "|MAX_VAL#IKAS.OPEN...|0.34464037|\n",
      "|MIN_VAL#IKAS.OPEN...|0.37356374|\n",
      "|STD_DEV#IKAS.OPEN...|  0.449199|\n",
      "|MAX_VAL#IKAS.OPEN...|0.30251545|\n",
      "|MAX_VAL#IKAS.OPEN...| 0.3224624|\n",
      "|MIN_VAL#IKAS.OPEN...|0.84759843|\n",
      "|MAX_VAL#IKAS.OPEN...|0.19547674|\n",
      "|MAX_VAL#IKAS.OPEN...|0.35736012|\n",
      "|MAX_VAL#IKAS.OPEN...|0.47721264|\n",
      "|MAX_VAL#IKAS.OPEN...|  0.269516|\n",
      "|MAX_VAL#IKAS.OPEN...|0.61858433|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = FitInlineModelBySite(df=df_preprocess,\n",
    "                           by=grpby_list,\n",
    "                           good_site_columns=good_site_columns,\n",
    "                           bad_site_columns=bad_site_columns,\n",
    "                           process_miss_site_mode='drop',\n",
    "                           columns_to_process=['AVERAGE', 'MAX_VAL', 'MEDIAN', 'MIN_VAL', 'STD_DEV',\n",
    "                                               'PERCENTILE_25', 'PERCENTILE_75'],\n",
    "                           missing_value_threshold=0.6,\n",
    "                           model='pca').run()\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a39dea7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PERCENTILE_75#IKAS.OPEN.1#FX01</td>\n",
       "      <td>0.248437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PERCENTILE_75#IKAS.OPEN.1#MX02</td>\n",
       "      <td>0.258391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.1#MY03</td>\n",
       "      <td>0.308456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PERCENTILE_75#IKAS.OPEN.1#MX01</td>\n",
       "      <td>0.359496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.1#MY03</td>\n",
       "      <td>0.310223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.1#MY03</td>\n",
       "      <td>0.416449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.1#MY01</td>\n",
       "      <td>0.366342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.1#MY02</td>\n",
       "      <td>0.390131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.1#MY02</td>\n",
       "      <td>0.627445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.1#MX03</td>\n",
       "      <td>0.344640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.1#MY03</td>\n",
       "      <td>0.373564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>STD_DEV#IKAS.OPEN.1#FX01</td>\n",
       "      <td>0.449199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.2#TGS2</td>\n",
       "      <td>0.302515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.2#TGS2</td>\n",
       "      <td>0.322462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.2#TGS2</td>\n",
       "      <td>0.847598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.2#TGS2</td>\n",
       "      <td>0.195477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.2#TGS2</td>\n",
       "      <td>0.357360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.2#TGS2</td>\n",
       "      <td>0.477213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.2#FX01</td>\n",
       "      <td>0.269516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.2#TGS1</td>\n",
       "      <td>0.618584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AVERAGE#IKAS.OPEN.2#TGS2</td>\n",
       "      <td>0.582754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.2#FX05</td>\n",
       "      <td>0.346521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.2#FX04</td>\n",
       "      <td>0.315790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PERCENTILE_75#IKAS.OPEN.2#TGS2</td>\n",
       "      <td>0.527115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MEDIAN#IKAS.OPEN.2#TGS2</td>\n",
       "      <td>0.615069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PERCENTILE_75#IKAS.OPEN.2#TGS1</td>\n",
       "      <td>0.421328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>STD_DEV#IKAS.OPEN.2#TGS2</td>\n",
       "      <td>0.639748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>PERCENTILE_25#IKAS.OPEN.2#TGS1</td>\n",
       "      <td>0.534938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MEDIAN#IKAS.OPEN.2#TGS1</td>\n",
       "      <td>0.626020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>PERCENTILE_75#IKAS.OPEN.2#FX05</td>\n",
       "      <td>0.349238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PERCENTILE_75#IKAS.OPEN.2#TGS1</td>\n",
       "      <td>0.463445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.2#FGS1</td>\n",
       "      <td>0.329496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>PERCENTILE_75#IKAS.OPEN.3#FX01</td>\n",
       "      <td>0.222640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.3#FX01</td>\n",
       "      <td>0.228790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>PERCENTILE_25#IKAS.OPEN.3#FX01</td>\n",
       "      <td>0.262387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.3#MY04</td>\n",
       "      <td>0.328423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.3#MY01</td>\n",
       "      <td>0.299840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>AVERAGE#IKAS.OPEN.3#MX03</td>\n",
       "      <td>0.226973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.3#MX02</td>\n",
       "      <td>0.302894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.3#MX04</td>\n",
       "      <td>0.399767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.3#MY02</td>\n",
       "      <td>0.331519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.3#MX01</td>\n",
       "      <td>0.332254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.3#MY05</td>\n",
       "      <td>0.390926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.3#MY03</td>\n",
       "      <td>0.544963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.3#MX03</td>\n",
       "      <td>0.414455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.3#MY02</td>\n",
       "      <td>0.306434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.3#MX02</td>\n",
       "      <td>0.246816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.3#MX03</td>\n",
       "      <td>0.267773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.3#MY01</td>\n",
       "      <td>0.279724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>MAX_VAL#IKAS.OPEN.3#MX04</td>\n",
       "      <td>0.374077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>PERCENTILE_25#IKAS.OPEN.3#MY01</td>\n",
       "      <td>0.242969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>MIN_VAL#IKAS.OPEN.3#MX05</td>\n",
       "      <td>0.327856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          features  importance\n",
       "0   PERCENTILE_75#IKAS.OPEN.1#FX01    0.248437\n",
       "1   PERCENTILE_75#IKAS.OPEN.1#MX02    0.258391\n",
       "2         MIN_VAL#IKAS.OPEN.1#MY03    0.308456\n",
       "3   PERCENTILE_75#IKAS.OPEN.1#MX01    0.359496\n",
       "4         MAX_VAL#IKAS.OPEN.1#MY03    0.310223\n",
       "5         MIN_VAL#IKAS.OPEN.1#MY03    0.416449\n",
       "6         MAX_VAL#IKAS.OPEN.1#MY01    0.366342\n",
       "7         MAX_VAL#IKAS.OPEN.1#MY02    0.390131\n",
       "8         MIN_VAL#IKAS.OPEN.1#MY02    0.627445\n",
       "9         MAX_VAL#IKAS.OPEN.1#MX03    0.344640\n",
       "10        MIN_VAL#IKAS.OPEN.1#MY03    0.373564\n",
       "11        STD_DEV#IKAS.OPEN.1#FX01    0.449199\n",
       "12        MAX_VAL#IKAS.OPEN.2#TGS2    0.302515\n",
       "13        MAX_VAL#IKAS.OPEN.2#TGS2    0.322462\n",
       "14        MIN_VAL#IKAS.OPEN.2#TGS2    0.847598\n",
       "15        MAX_VAL#IKAS.OPEN.2#TGS2    0.195477\n",
       "16        MAX_VAL#IKAS.OPEN.2#TGS2    0.357360\n",
       "17        MAX_VAL#IKAS.OPEN.2#TGS2    0.477213\n",
       "18        MAX_VAL#IKAS.OPEN.2#FX01    0.269516\n",
       "19        MAX_VAL#IKAS.OPEN.2#TGS1    0.618584\n",
       "20        AVERAGE#IKAS.OPEN.2#TGS2    0.582754\n",
       "21        MIN_VAL#IKAS.OPEN.2#FX05    0.346521\n",
       "22        MAX_VAL#IKAS.OPEN.2#FX04    0.315790\n",
       "23  PERCENTILE_75#IKAS.OPEN.2#TGS2    0.527115\n",
       "24         MEDIAN#IKAS.OPEN.2#TGS2    0.615069\n",
       "25  PERCENTILE_75#IKAS.OPEN.2#TGS1    0.421328\n",
       "26        STD_DEV#IKAS.OPEN.2#TGS2    0.639748\n",
       "27  PERCENTILE_25#IKAS.OPEN.2#TGS1    0.534938\n",
       "28         MEDIAN#IKAS.OPEN.2#TGS1    0.626020\n",
       "29  PERCENTILE_75#IKAS.OPEN.2#FX05    0.349238\n",
       "30  PERCENTILE_75#IKAS.OPEN.2#TGS1    0.463445\n",
       "31        MIN_VAL#IKAS.OPEN.2#FGS1    0.329496\n",
       "32  PERCENTILE_75#IKAS.OPEN.3#FX01    0.222640\n",
       "33        MAX_VAL#IKAS.OPEN.3#FX01    0.228790\n",
       "34  PERCENTILE_25#IKAS.OPEN.3#FX01    0.262387\n",
       "35        MIN_VAL#IKAS.OPEN.3#MY04    0.328423\n",
       "36        MAX_VAL#IKAS.OPEN.3#MY01    0.299840\n",
       "37        AVERAGE#IKAS.OPEN.3#MX03    0.226973\n",
       "38        MIN_VAL#IKAS.OPEN.3#MX02    0.302894\n",
       "39        MIN_VAL#IKAS.OPEN.3#MX04    0.399767\n",
       "40        MIN_VAL#IKAS.OPEN.3#MY02    0.331519\n",
       "41        MIN_VAL#IKAS.OPEN.3#MX01    0.332254\n",
       "42        MAX_VAL#IKAS.OPEN.3#MY05    0.390926\n",
       "43        MIN_VAL#IKAS.OPEN.3#MY03    0.544963\n",
       "44        MAX_VAL#IKAS.OPEN.3#MX03    0.414455\n",
       "45        MIN_VAL#IKAS.OPEN.3#MY02    0.306434\n",
       "46        MIN_VAL#IKAS.OPEN.3#MX02    0.246816\n",
       "47        MIN_VAL#IKAS.OPEN.3#MX03    0.267773\n",
       "48        MIN_VAL#IKAS.OPEN.3#MY01    0.279724\n",
       "49        MAX_VAL#IKAS.OPEN.3#MX04    0.374077\n",
       "50  PERCENTILE_25#IKAS.OPEN.3#MY01    0.242969\n",
       "51        MIN_VAL#IKAS.OPEN.3#MX05    0.327856"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_pandas = res.toPandas()\n",
    "res_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec02f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bdfb0463",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----+\n",
      "|            features|importance|temp|\n",
      "+--------------------+----------+----+\n",
      "|PERCENTILE_75#IKA...|0.24843656|   0|\n",
      "|PERCENTILE_75#IKA...|0.25839117|   0|\n",
      "|MIN_VAL#IKAS.OPEN...|0.30845588|   0|\n",
      "|PERCENTILE_75#IKA...|0.35949597|   0|\n",
      "|MAX_VAL#IKAS.OPEN...| 0.3102229|   0|\n",
      "|MIN_VAL#IKAS.OPEN...|0.41644922|   0|\n",
      "|MAX_VAL#IKAS.OPEN...| 0.3663416|   0|\n",
      "|MAX_VAL#IKAS.OPEN...|0.39013106|   0|\n",
      "|MIN_VAL#IKAS.OPEN...|0.62744516|   0|\n",
      "|MAX_VAL#IKAS.OPEN...|0.34464037|   0|\n",
      "|MIN_VAL#IKAS.OPEN...|0.37356374|   0|\n",
      "|STD_DEV#IKAS.OPEN...|  0.449199|   0|\n",
      "|MAX_VAL#IKAS.OPEN...|0.30251545|   0|\n",
      "|MAX_VAL#IKAS.OPEN...| 0.3224624|   0|\n",
      "|MIN_VAL#IKAS.OPEN...|0.84759843|   0|\n",
      "|MAX_VAL#IKAS.OPEN...|0.19547674|   0|\n",
      "|MAX_VAL#IKAS.OPEN...|0.35736012|   0|\n",
      "|MAX_VAL#IKAS.OPEN...|0.47721264|   0|\n",
      "|MAX_VAL#IKAS.OPEN...|  0.269516|   0|\n",
      "|MAX_VAL#IKAS.OPEN...|0.61858433|   0|\n",
      "+--------------------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = res.withColumn('temp', lit(0))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebfd2d82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----------+\n",
      "|     OPE_NO|INLINE_PARAMETER_ID|importance|\n",
      "+-----------+-------------------+----------+\n",
      "|IKAS.OPEN.1|               FX01|0.69763553|\n",
      "|IKAS.OPEN.1|               MX01|0.35949597|\n",
      "|IKAS.OPEN.1|               MX02|0.25839117|\n",
      "|IKAS.OPEN.1|               MX03|0.34464037|\n",
      "|IKAS.OPEN.1|               MY01| 0.3663416|\n",
      "|IKAS.OPEN.1|               MY02| 1.0175762|\n",
      "|IKAS.OPEN.1|               MY03| 1.4086918|\n",
      "|IKAS.OPEN.2|               FGS1|0.32949623|\n",
      "|IKAS.OPEN.2|               FX01|  0.269516|\n",
      "|IKAS.OPEN.2|               FX04|0.31579003|\n",
      "|IKAS.OPEN.2|               FX05| 0.6957592|\n",
      "|IKAS.OPEN.2|               TGS1| 2.6643157|\n",
      "|IKAS.OPEN.2|               TGS2|  4.867313|\n",
      "|IKAS.OPEN.3|               FX01|0.71381706|\n",
      "|IKAS.OPEN.3|               MX01|0.33225435|\n",
      "|IKAS.OPEN.3|               MX02|0.54970974|\n",
      "|IKAS.OPEN.3|               MX03|0.90920126|\n",
      "|IKAS.OPEN.3|               MX04| 0.7738442|\n",
      "|IKAS.OPEN.3|               MX05|0.32785642|\n",
      "|IKAS.OPEN.3|               MY01| 0.8225322|\n",
      "+-----------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_split = SplitInlineModelResults.split_calculate_features(df=df, grpby_list=grpby_list, by='temp')\n",
    "res_split.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47d97abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----------+----+\n",
      "|     OPE_NO|INLINE_PARAMETER_ID|importance|temp|\n",
      "+-----------+-------------------+----------+----+\n",
      "|IKAS.OPEN.1|               FX01|0.69763553|   1|\n",
      "|IKAS.OPEN.1|               MX01|0.35949597|   1|\n",
      "|IKAS.OPEN.1|               MX02|0.25839117|   1|\n",
      "|IKAS.OPEN.1|               MX03|0.34464037|   1|\n",
      "|IKAS.OPEN.1|               MY01| 0.3663416|   1|\n",
      "|IKAS.OPEN.1|               MY02| 1.0175762|   1|\n",
      "|IKAS.OPEN.1|               MY03| 1.4086918|   1|\n",
      "|IKAS.OPEN.2|               FGS1|0.32949623|   1|\n",
      "|IKAS.OPEN.2|               FX01|  0.269516|   1|\n",
      "|IKAS.OPEN.2|               FX04|0.31579003|   1|\n",
      "|IKAS.OPEN.2|               FX05| 0.6957592|   1|\n",
      "|IKAS.OPEN.2|               TGS1| 2.6643157|   1|\n",
      "|IKAS.OPEN.2|               TGS2|  4.867313|   1|\n",
      "|IKAS.OPEN.3|               FX01|0.71381706|   1|\n",
      "|IKAS.OPEN.3|               MX01|0.33225435|   1|\n",
      "|IKAS.OPEN.3|               MX02|0.54970974|   1|\n",
      "|IKAS.OPEN.3|               MX03|0.90920126|   1|\n",
      "|IKAS.OPEN.3|               MX04| 0.7738442|   1|\n",
      "|IKAS.OPEN.3|               MX05|0.32785642|   1|\n",
      "|IKAS.OPEN.3|               MY01| 0.8225322|   1|\n",
      "+-----------+-------------------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_split = res_split.withColumn('temp', lit(1))\n",
    "res_split.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1934841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------------+----------+-----------+--------------+--------+\n",
      "|    OPER_NO|INLINE_PARAMETER_ID|AVG_SPEC_CHK_RESULT_COUNT|request_id|     weight|weight_percent|index_no|\n",
      "+-----------+-------------------+-------------------------+----------+-----------+--------------+--------+\n",
      "|IKAS.OPEN.2|               TGS2|                      0.0|       715|   0.244264|     24.426401|       1|\n",
      "|IKAS.OPEN.2|               TGS1|                      0.0|       715| 0.13370754|     13.370753|       2|\n",
      "|IKAS.OPEN.1|               MY03|                      0.0|       715| 0.07069459|      7.069459|       3|\n",
      "|IKAS.OPEN.1|               MY02|                      0.0|       715|0.051066626|     5.1066623|       4|\n",
      "|IKAS.OPEN.3|               MX03|                      0.0|       715|0.045627873|     4.5627875|       5|\n",
      "|IKAS.OPEN.3|               MY01|                      0.0|       715|0.041278422|     4.1278424|       6|\n",
      "|IKAS.OPEN.3|               MX04|                      0.0|       715|0.038835037|     3.8835037|       7|\n",
      "|IKAS.OPEN.3|               FX01|                      0.0|       715|  0.0358226|     3.5822601|       8|\n",
      "|IKAS.OPEN.1|               FX01|                      0.0|       715| 0.03501054|     3.5010538|       9|\n",
      "|IKAS.OPEN.2|               FX05|                      0.0|       715|0.034916375|     3.4916375|      10|\n",
      "|IKAS.OPEN.3|               MY02|                      0.0|       715|0.032015417|     3.2015417|      11|\n",
      "|IKAS.OPEN.3|               MX02|                      0.0|       715|0.027586946|     2.7586946|      12|\n",
      "|IKAS.OPEN.3|               MY03|                      0.0|       715|0.027348757|     2.7348757|      13|\n",
      "|IKAS.OPEN.3|               MY05|                      0.0|       715|0.019618455|     1.9618456|      14|\n",
      "|IKAS.OPEN.1|               MY01|                      0.0|       715|0.018384695|     1.8384695|      15|\n",
      "|IKAS.OPEN.1|               MX01|                      0.0|       715| 0.01804115|      1.804115|      16|\n",
      "|IKAS.OPEN.1|               MX03|                      0.0|       715|0.017295629|     1.7295629|      17|\n",
      "|IKAS.OPEN.3|               MX01|                      0.0|       715|0.016674042|     1.6674042|      18|\n",
      "|IKAS.OPEN.2|               FGS1|                      0.0|       715|0.016535627|     1.6535627|      19|\n",
      "|IKAS.OPEN.3|               MY04|                      0.0|       715|0.016481766|     1.6481767|      20|\n",
      "+-----------+-------------------+-------------------------+----------+-----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_res = SplitInlineModelResults.add_certain_column(df=res_split, grpby_list=grpby_list, by='temp', request_id=request_id)\n",
    "final_res = final_res.withColumn('request_id', lit(request_id))\n",
    "final_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a611bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd880e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19d98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c39dd288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "+-----------+-------------------+-------------------------+-----------+--------------+--------+----------+\n",
      "|    OPER_NO|INLINE_PARAMETER_ID|AVG_SPEC_CHK_RESULT_COUNT|     weight|weight_percent|index_no|request_id|\n",
      "+-----------+-------------------+-------------------------+-----------+--------------+--------+----------+\n",
      "|IKAS.OPEN.2|               TGS2|                      0.0|   0.244264|     24.426401|       1|       715|\n",
      "|IKAS.OPEN.2|               TGS1|                      0.0| 0.13370754|     13.370753|       2|       715|\n",
      "|IKAS.OPEN.1|               MY03|                      0.0| 0.07069459|      7.069459|       3|       715|\n",
      "|IKAS.OPEN.1|               MY02|                      0.0|0.051066626|     5.1066623|       4|       715|\n",
      "|IKAS.OPEN.3|               MX03|                      0.0|0.045627873|     4.5627875|       5|       715|\n",
      "|IKAS.OPEN.3|               MY01|                      0.0|0.041278422|     4.1278424|       6|       715|\n",
      "|IKAS.OPEN.3|               MX04|                      0.0|0.038835037|     3.8835037|       7|       715|\n",
      "|IKAS.OPEN.3|               FX01|                      0.0|  0.0358226|     3.5822601|       8|       715|\n",
      "|IKAS.OPEN.1|               FX01|                      0.0| 0.03501054|     3.5010538|       9|       715|\n",
      "|IKAS.OPEN.2|               FX05|                      0.0|0.034916375|     3.4916375|      10|       715|\n",
      "|IKAS.OPEN.3|               MY02|                      0.0|0.032015417|     3.2015417|      11|       715|\n",
      "|IKAS.OPEN.3|               MX02|                      0.0|0.027586946|     2.7586946|      12|       715|\n",
      "|IKAS.OPEN.3|               MY03|                      0.0|0.027348757|     2.7348757|      13|       715|\n",
      "|IKAS.OPEN.3|               MY05|                      0.0|0.019618455|     1.9618456|      14|       715|\n",
      "|IKAS.OPEN.1|               MY01|                      0.0|0.018384695|     1.8384695|      15|       715|\n",
      "|IKAS.OPEN.1|               MX01|                      0.0| 0.01804115|      1.804115|      16|       715|\n",
      "|IKAS.OPEN.1|               MX03|                      0.0|0.017295629|     1.7295629|      17|       715|\n",
      "|IKAS.OPEN.3|               MX01|                      0.0|0.016674042|     1.6674042|      18|       715|\n",
      "|IKAS.OPEN.2|               FGS1|                      0.0|0.016535627|     1.6535627|      19|       715|\n",
      "|IKAS.OPEN.3|               MY04|                      0.0|0.016481766|     1.6481767|      20|       715|\n",
      "+-----------+-------------------+-------------------------+-----------+--------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_res = SplitInlineModelResults.add_certain_column(df=res_split, by='temp')\n",
    "final_res = final_res.withColumn('request_id', lit(request_id))\n",
    "print(final_res.count())\n",
    "final_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e0f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ca8432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58f69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
