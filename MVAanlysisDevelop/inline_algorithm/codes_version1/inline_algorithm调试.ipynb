{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2575c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f578f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pymysql \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pyspark.pandas as ps\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from scipy import stats\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Optional\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import URL \n",
    "# from backend_spark.doris_common.doris_client import DorisClient\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, monotonically_increasing_id, lit, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "343dba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "  .master(\"local[*]\")\n",
    "  .config(\"spark.jars.packages\", \"ai.catboost:catboost-spark_3.3_2.12:1.2\")\n",
    "  .appName(\"RF\")\n",
    "  .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e41c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "client81 = pymysql.connect(user='root', password='Nexchip@123', port=9030, host='10.52.199.81')\n",
    "data_count = 0\n",
    "\n",
    "# doris 数据库连接\n",
    "# client91 = DorisClient(\"10.52.199.91\", 18030, 9030, user=\"ikas_user\", password=\"Ikas_user@123\", data_base=\"ODS_EDA\",\n",
    "#                      mem_limit=\"68719476736\")\n",
    "\n",
    "# client81 = DorisClient(\"10.52.199.81\", 18030, 9030, user=\"root\", password=\"Nexchip@123\", data_base=\"ODS_EDA\",\n",
    "#                      mem_limit=\"68719476736\")\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "###################################解析sql 的辅助函数###################################\n",
    "####################################################################################\n",
    "def read_sql1(sql_stat, read_client=client81, session=spark):\n",
    "    # df1 = read_client.doris_read(session, sql_stat)\n",
    "    df1 = spark.createDataFrame(pd.read_sql(sql=sql_stat, con=client81))\n",
    "    return df1\n",
    "\n",
    "\n",
    "def read_sql(sql_stat, read_client=client81, session=spark):\n",
    "    # df1 = read_client.doris_read(session, sql_stat)\n",
    "    ds = pd.read_sql(sql=sql_stat, con=client81)\n",
    "    if ds is None or len(ds) == 0:\n",
    "        empty_schema = StructType([\n",
    "            StructField(\"WAFER_ID\", StringType(), True),\n",
    "            StructField(\"OPE_NO\", StringType(), True),\n",
    "            StructField(\"INLINE_PARAMETER_ID\", StringType(), True),\n",
    "            StructField(\"PRODUCT_ID\", StringType(), True),\n",
    "            StructField(\"AVERAGE\", StringType(), True),\n",
    "            StructField(\"STD_DEV\", StringType(), True),\n",
    "            StructField(\"MEASURE_TIME\", TimestampType(), True),\n",
    "            StructField(\"AVG_SPEC_CHK_RESULT\", StringType(), True),\n",
    "            StructField(\"label\", StringType(), True)\n",
    "        ])\n",
    "        df1 = spark.createDataFrame([], empty_schema)\n",
    "        #print(df1)\n",
    "        #null_dataframe = pd.DataFrame(\n",
    "        #{\n",
    "        #\"WAFER_ID\" : [\"\"],\n",
    "        #\"OPE_NO\" : [\"\"],\n",
    "        #\"INLINE_PARAMETER_ID\" : [\"\"],\n",
    "        #\"PRODUCT_ID\" : [\"\"],\n",
    "        #\"AVERAGE\" : [0.0],\n",
    "        #\"STD_DEV\" : [0.0],\n",
    "        #\"MEASURE_TIME\" : [\"\"],\n",
    "        #\"AVG_SPEC_CHK_RESULT\" : [\"\"],\n",
    "        #\"label\" : [0],\n",
    "        #}\n",
    "        #)\n",
    "        #df1 = spark.createDataFrame(null_dataframe)\n",
    "    else:\n",
    "        df1 = spark.createDataFrame(ds)\n",
    "        data_count = 1\n",
    "    return df1\n",
    "\n",
    "def process_like(key: str, value: list[str]) -> str:\n",
    "    # 处理模糊条件的匹配: (key like 'aa%' or key like \"bb%\")\n",
    "    key = keyword_map_from_json_to_table.get(key)\n",
    "    v_join = ' or '.join([f\"{key} like  '{v.replace('*', '%')}' \" for v in value])\n",
    "    return \"({})\".format(v_join)\n",
    "\n",
    "\n",
    "def process_not_like(key: str, value: list[str]) -> str:\n",
    "    # 处理非模糊条件的匹配:key in ('aa', 'bb')\n",
    "    key = keyword_map_from_json_to_table.get(key)\n",
    "    v_join = \",\".join([f\"'{v}'\" for v in value])\n",
    "    return \"{} in ({})\".format(key, v_join)\n",
    "\n",
    "\n",
    "def test_not_like():\n",
    "    result = (process_not_like(\"tool_name\", [\"aa\", \"bb\", \"cc\"]))\n",
    "    assert \"tool_name in ('aa','bb','cc')\" == result, \"not like 验证失败\"\n",
    "\n",
    "\n",
    "def test_like():\n",
    "    result = process_like(\"tool_name\", [\"aa*\", \"bb*\", \"cc*\"])\n",
    "\n",
    "    assert \"(tool_name like  'aa%'  or tool_name like  'bb%'  or tool_name like  'cc%' )\" == result, \"like 验证失败\"\n",
    "\n",
    "\n",
    "def process_one_keyword(key, value: list[str]) -> Optional[str]:\n",
    "    if len(value) == 0:\n",
    "        return None\n",
    "\n",
    "    not_like_list = [v for v in value if \"*\" not in v]\n",
    "    like_list = [v for v in value if \"*\" in v]\n",
    "\n",
    "    # 处理模糊条件\n",
    "    if len(not_like_list) != 0:\n",
    "        not_like_sql_str = process_not_like(key, not_like_list)\n",
    "    else:\n",
    "        not_like_sql_str = \"\"\n",
    "\n",
    "    # 处理非模糊条件\n",
    "\n",
    "    if len(like_list) != 0:\n",
    "        like_sql_str = process_like(key, like_list)\n",
    "    else:\n",
    "        like_sql_str = \"\"\n",
    "\n",
    "    # 去除为一个元素为空字符串的情况的情况的情况\n",
    "    concat_sql_str_list = [sql_str for sql_str in [like_sql_str, not_like_sql_str] if len(sql_str) != 0]\n",
    "    # 使用or 操作 单字段过滤 的like 和 not like 语句\n",
    "    return \"(\" + \" or \".join(concat_sql_str_list) + \")\"\n",
    "\n",
    "\n",
    "def check_time_start_end(min_time, max_time):\n",
    "    if min_time is not None and max_time is not None:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"起始时间和结束时间必须全填\")\n",
    "\n",
    "def get_time_selection_sql(time_keyword, max_time=None, min_time=None):\n",
    "    \"\"\"\n",
    "    获取时间区间的筛选的sql, 起始时间和结束时间都是可选的\n",
    "    :param time_keyword:\n",
    "    :param max_time:\n",
    "    :param min_time:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 根据取值，生成单个时间过滤条件\n",
    "    if min_time:\n",
    "        time_part_min = f\"{time_keyword} > '{min_time}'\"\n",
    "    else:\n",
    "        time_part_min = \" \"\n",
    "\n",
    "    if max_time:\n",
    "        time_part_max = f\"{time_keyword} <= '{max_time}'\"\n",
    "    else:\n",
    "        time_part_max = \" \"\n",
    "\n",
    "    # 如果存在，拼接多个查询条件，或者只保留一个过滤条件\n",
    "    if (max_time is not None) and (min_time is not None):\n",
    "        time_sql = f' {time_part_min} and {time_part_max}'\n",
    "    elif (max_time is None) and (min_time is None):\n",
    "        time_sql = \" \"\n",
    "    else:\n",
    "        time_sql = time_part_max if max_time else time_part_min\n",
    "\n",
    "    return time_sql\n",
    "\n",
    "def concat_time_filter_sql_with_other_keyword_sql(time_filter_sql: str, other_keyword_sql:str) -> str:\n",
    "    \"\"\"\n",
    "    拼接时间过滤条件与非时间过滤条件\n",
    "    :param time_filter_sql:\n",
    "    :param other_keyword_sql:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    time_strip = time_filter_sql.strip()\n",
    "    other_strip = other_keyword_sql.strip()\n",
    "    if len(time_strip) == 0 and len(other_strip) == 0:\n",
    "        return \"\"\n",
    "    elif len(time_strip) != 0 and len(other_strip) == 0:\n",
    "        return  time_filter_sql\n",
    "    elif len(time_strip) == 0 and len(other_strip) != 0:\n",
    "        return other_keyword_sql\n",
    "    else:\n",
    "        return f'{time_filter_sql} and {other_keyword_sql}'\n",
    "\n",
    "def trans_select_condition_to_sql_with_label(select_condition_dict: dict, table_name: str) -> str:\n",
    "    # 查询条件转sql,并打标签，label '0': good wafer, '1': bad wafer\n",
    "    filter_sql_list = []\n",
    "    for keyword, value in select_condition_dict.items():\n",
    "        if keyword not in [\"dateRange\", \"waferId\", \"uploadId\", \"mergeProdg1\"]:\n",
    "            sql_filter_one_keyword = process_one_keyword(keyword, value)\n",
    "            if sql_filter_one_keyword is not None:\n",
    "                filter_sql_list.append(sql_filter_one_keyword)\n",
    "\n",
    "    # 处理时间区间\n",
    "    time_bin = select_condition_dict.get(\"dateRange\")\n",
    "\n",
    "    if len(time_bin) >0:\n",
    "        time_bin_dict = time_bin[0]\n",
    "        min_time = time_bin_dict.get(\"start\")\n",
    "        max_time = time_bin_dict.get(\"end\")\n",
    "    else:\n",
    "        min_time = None \n",
    "        max_time = None \n",
    "\n",
    "    # 去除时间检查，时间范围为可选输入\n",
    "    # 检查起始时间和结束时间全部非空\n",
    "    # check_time_start_end(min_time, max_time)\n",
    "\n",
    "    # 处理waferId\n",
    "    waferId = select_condition_dict.get(\"waferId\")\n",
    "    good_wafer_list = waferId.get(\"good\")\n",
    "    bad_wafer_list = waferId.get(\"bad\")\n",
    "\n",
    "    # 根据time 过滤条件,生成sql\n",
    "    time_filter_sql = get_time_selection_sql(time_keyword=keyword_map_from_json_to_table.get('dateRange'), max_time=max_time, min_time=min_time)\n",
    "\n",
    "\n",
    "    if len(good_wafer_list) + len(bad_wafer_list) == 0:\n",
    "        raise ValueError(\"good, bad wafer 至少选择一个\")\n",
    "    elif len(good_wafer_list) > 0 and len(bad_wafer_list) > 0:\n",
    "        # good wafer, bad wafe 均有指定，需要从层层字段的过滤的条件下选择\n",
    "        good_wafer_filter_sql = process_one_keyword(\"waferId\", good_wafer_list)\n",
    "        bad_wafer_filter_sql = process_one_keyword(\"waferId\", bad_wafer_list)\n",
    "        # or 拼接\n",
    "        wafer_filter_sql = \" or \".join([good_wafer_filter_sql, bad_wafer_filter_sql])\n",
    "        wafer_filter_sql = f\"({wafer_filter_sql})\"\n",
    "        # 加入wafer 过滤条件\n",
    "        filter_sql_list.append(wafer_filter_sql)\n",
    "        other_keyword_filter = \" and \".join(filter_sql_list)\n",
    "\n",
    "        case_when_statment = f\"\"\"(\n",
    "        case \n",
    "        when {good_wafer_filter_sql} then 0\n",
    "        else 1 \n",
    "        end \n",
    "        ) label\n",
    "        \"\"\"\n",
    "\n",
    "        filter_sql_concat = concat_time_filter_sql_with_other_keyword_sql(time_filter_sql, other_keyword_filter)\n",
    "        if filter_sql_concat != '':\n",
    "            select_sql = f\"select *, {case_when_statment} from {table_name} where {filter_sql_concat}\"\n",
    "        else:\n",
    "             select_sql = f\"select *, {case_when_statment} from {table_name}\"\n",
    "\n",
    "        \n",
    "       \n",
    "    elif len(good_wafer_list) > 0 and len(bad_wafer_list) == 0:\n",
    "        # 选good, 剩余为 bad\n",
    "        other_keyword_filter = \" and \".join(filter_sql_list)\n",
    "        good_wafer_filter_sql = process_one_keyword(\"waferId\", good_wafer_list)\n",
    "\n",
    "        case_when_statment = f\"(case when {good_wafer_filter_sql} then 0 else 1  end ) label\"\n",
    "        filter_sql_concat = concat_time_filter_sql_with_other_keyword_sql(time_filter_sql, other_keyword_filter)\n",
    "        select_sql = f\"select *, {case_when_statment} from {table_name} where {filter_sql_concat}\"\n",
    "\n",
    "    elif len(good_wafer_list) == 0 and len(bad_wafer_list) > 0:\n",
    "        # 选bad, 剩余为good\n",
    "        other_keyword_filter = \" and \".join(filter_sql_list)\n",
    "        bad_wafer_filter_sql = process_one_keyword(\"waferId\", bad_wafer_list)\n",
    "        case_when_statment = f\"\"\"(case when {bad_wafer_filter_sql} then 1 else 0 end ) label\"\"\"\n",
    "        filter_sql_concat = concat_time_filter_sql_with_other_keyword_sql(time_filter_sql, other_keyword_filter)\n",
    "        select_sql = f\"select *, {case_when_statment} from {table_name} where {filter_sql_concat}\"\n",
    "        # case1 stat results 表没有case_info 时间列，暂时去掉\n",
    "        # select_sql = f\"select *, {case_when_statment} from {table_name} where {other_keyword_filter}\"\n",
    "    select_keywords = \"\"\"WAFER_ID,\n",
    "    OPE_NO,\n",
    "    INLINE_PARAMETER_ID,\n",
    "    PRODUCT_ID,\n",
    "    LOT_ID,\n",
    "    `AVERAGE`,\n",
    "    STD_DEV,\n",
    "    MEASURE_TIME,\n",
    "    AVG_SPEC_CHK_RESULT\"\"\"\n",
    "    select_sql = select_sql.replace( \"*\",  select_keywords )\n",
    "    print(select_sql)\n",
    "    return select_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d24f290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAFER_ID</th>\n",
       "      <th>OPE_NO</th>\n",
       "      <th>INLINE_PARAMETER_ID</th>\n",
       "      <th>MEASURE_TIME</th>\n",
       "      <th>RANGE_INDEX</th>\n",
       "      <th>FAB_ID</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>LOT_ID</th>\n",
       "      <th>AVERAGE</th>\n",
       "      <th>MAX_VAL</th>\n",
       "      <th>...</th>\n",
       "      <th>RANGE</th>\n",
       "      <th>ACT_CODE</th>\n",
       "      <th>ETL_INSERT_TIME</th>\n",
       "      <th>ETL_ARC_FLAG</th>\n",
       "      <th>ETL_BATCH_SYNC_TS</th>\n",
       "      <th>ETL_DEL_FLAG</th>\n",
       "      <th>ETL_DS_JOB_NM</th>\n",
       "      <th>ETL_SRC_DB</th>\n",
       "      <th>ETL_SRC_TBL</th>\n",
       "      <th>ETL_TBL_OPER_TS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NAZ415-06</td>\n",
       "      <td>1C.CDG10</td>\n",
       "      <td>CIW0</td>\n",
       "      <td>2022-12-03 01:45:41</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNM301N.0A01</td>\n",
       "      <td>NAZ415000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-29 04:34:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EDA</td>\n",
       "      <td>INLINE_WAFER_SUMMARY</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NAZ415-06</td>\n",
       "      <td>1C.CDG10</td>\n",
       "      <td>EEW0</td>\n",
       "      <td>2022-12-03 01:45:41</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNM301N.0A01</td>\n",
       "      <td>NAZ415000</td>\n",
       "      <td>3999.845454</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-29 04:34:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EDA</td>\n",
       "      <td>INLINE_WAFER_SUMMARY</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NAZ415-06</td>\n",
       "      <td>1C.CDG10</td>\n",
       "      <td>FEW0</td>\n",
       "      <td>2022-12-03 01:45:41</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNM301N.0A01</td>\n",
       "      <td>NAZ415000</td>\n",
       "      <td>123.402563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-29 04:34:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EDA</td>\n",
       "      <td>INLINE_WAFER_SUMMARY</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NAZ415-06</td>\n",
       "      <td>1C.CDG10</td>\n",
       "      <td>HFT0</td>\n",
       "      <td>2022-12-03 01:45:41</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNM301N.0A01</td>\n",
       "      <td>NAZ415000</td>\n",
       "      <td>44.099000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-29 04:34:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EDA</td>\n",
       "      <td>INLINE_WAFER_SUMMARY</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NAZ415-06</td>\n",
       "      <td>1C.CDG10</td>\n",
       "      <td>OEW0</td>\n",
       "      <td>2022-12-03 01:45:41</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNM301N.0A01</td>\n",
       "      <td>NAZ415000</td>\n",
       "      <td>6000.016364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-29 04:34:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EDA</td>\n",
       "      <td>INLINE_WAFER_SUMMARY</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32273</th>\n",
       "      <td>NBX219-17</td>\n",
       "      <td>1U.CDG20</td>\n",
       "      <td>PTW0</td>\n",
       "      <td>2023-08-31 23:37:37</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNR901N.0B0J</td>\n",
       "      <td>NBX219000</td>\n",
       "      <td>5500.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-31 23:47:47</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32274</th>\n",
       "      <td>NBX219-17</td>\n",
       "      <td>1U.CDG20</td>\n",
       "      <td>REW0</td>\n",
       "      <td>2023-08-31 23:37:37</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNR901N.0B0J</td>\n",
       "      <td>NBX219000</td>\n",
       "      <td>198.846013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-31 23:47:47</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32275</th>\n",
       "      <td>NBX219-17</td>\n",
       "      <td>1U.CDG20</td>\n",
       "      <td>SEW0</td>\n",
       "      <td>2023-08-31 23:37:37</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNR901N.0B0J</td>\n",
       "      <td>NBX219000</td>\n",
       "      <td>1.292111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-31 23:47:47</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32276</th>\n",
       "      <td>NBX219-17</td>\n",
       "      <td>1U.CDG20</td>\n",
       "      <td>SFW0</td>\n",
       "      <td>2023-08-31 23:37:37</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNR901N.0B0J</td>\n",
       "      <td>NBX219000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-31 23:47:47</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32277</th>\n",
       "      <td>NBX219-17</td>\n",
       "      <td>1U.CDG20</td>\n",
       "      <td>TSW0</td>\n",
       "      <td>2023-08-31 23:37:37</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNR901N.0B0J</td>\n",
       "      <td>NBX219000</td>\n",
       "      <td>350.116364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-31 23:47:47</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32278 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        WAFER_ID    OPE_NO INLINE_PARAMETER_ID         MEASURE_TIME  \\\n",
       "0      NAZ415-06  1C.CDG10                CIW0  2022-12-03 01:45:41   \n",
       "1      NAZ415-06  1C.CDG10                EEW0  2022-12-03 01:45:41   \n",
       "2      NAZ415-06  1C.CDG10                FEW0  2022-12-03 01:45:41   \n",
       "3      NAZ415-06  1C.CDG10                HFT0  2022-12-03 01:45:41   \n",
       "4      NAZ415-06  1C.CDG10                OEW0  2022-12-03 01:45:41   \n",
       "...          ...       ...                 ...                  ...   \n",
       "32273  NBX219-17  1U.CDG20                PTW0  2023-08-31 23:37:37   \n",
       "32274  NBX219-17  1U.CDG20                REW0  2023-08-31 23:37:37   \n",
       "32275  NBX219-17  1U.CDG20                SEW0  2023-08-31 23:37:37   \n",
       "32276  NBX219-17  1U.CDG20                SFW0  2023-08-31 23:37:37   \n",
       "32277  NBX219-17  1U.CDG20                TSW0  2023-08-31 23:37:37   \n",
       "\n",
       "       RANGE_INDEX FAB_ID      PRODUCT_ID     LOT_ID      AVERAGE  MAX_VAL  \\\n",
       "0                0     N1  AFPNM301N.0A01  NAZ415000          NaN      NaN   \n",
       "1                0     N1  AFPNM301N.0A01  NAZ415000  3999.845454      NaN   \n",
       "2                0     N1  AFPNM301N.0A01  NAZ415000   123.402563      NaN   \n",
       "3                0     N1  AFPNM301N.0A01  NAZ415000    44.099000      NaN   \n",
       "4                0     N1  AFPNM301N.0A01  NAZ415000  6000.016364      NaN   \n",
       "...            ...    ...             ...        ...          ...      ...   \n",
       "32273            0     N1  AFPNR901N.0B0J  NBX219000  5500.000000      NaN   \n",
       "32274            0     N1  AFPNR901N.0B0J  NBX219000   198.846013      NaN   \n",
       "32275            0     N1  AFPNR901N.0B0J  NBX219000     1.292111      NaN   \n",
       "32276            0     N1  AFPNR901N.0B0J  NBX219000     3.000000      NaN   \n",
       "32277            0     N1  AFPNR901N.0B0J  NBX219000   350.116364      NaN   \n",
       "\n",
       "       ...  RANGE  ACT_CODE      ETL_INSERT_TIME  ETL_ARC_FLAG  \\\n",
       "0      ...    NaN       NaN  2023-05-29 04:34:30             0   \n",
       "1      ...    NaN       NaN  2023-05-29 04:34:30             0   \n",
       "2      ...    NaN       NaN  2023-05-29 04:34:30             0   \n",
       "3      ...    NaN       NaN  2023-05-29 04:34:30             0   \n",
       "4      ...    NaN       NaN  2023-05-29 04:34:30             0   \n",
       "...    ...    ...       ...                  ...           ...   \n",
       "32273  ...    NaN       NaN  2023-08-31 23:47:47             0   \n",
       "32274  ...    NaN       NaN  2023-08-31 23:47:47             0   \n",
       "32275  ...    NaN       NaN  2023-08-31 23:47:47             0   \n",
       "32276  ...    NaN       NaN  2023-08-31 23:47:47             0   \n",
       "32277  ...    NaN       NaN  2023-08-31 23:47:47             0   \n",
       "\n",
       "         ETL_BATCH_SYNC_TS  ETL_DEL_FLAG  ETL_DS_JOB_NM  ETL_SRC_DB  \\\n",
       "0      1970-01-01 00:00:00             0            NaN         EDA   \n",
       "1      1970-01-01 00:00:00             0            NaN         EDA   \n",
       "2      1970-01-01 00:00:00             0            NaN         EDA   \n",
       "3      1970-01-01 00:00:00             0            NaN         EDA   \n",
       "4      1970-01-01 00:00:00             0            NaN         EDA   \n",
       "...                    ...           ...            ...         ...   \n",
       "32273  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32274  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32275  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32276  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32277  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "\n",
       "                ETL_SRC_TBL      ETL_TBL_OPER_TS  \n",
       "0      INLINE_WAFER_SUMMARY  1970-01-01 00:00:00  \n",
       "1      INLINE_WAFER_SUMMARY  1970-01-01 00:00:00  \n",
       "2      INLINE_WAFER_SUMMARY  1970-01-01 00:00:00  \n",
       "3      INLINE_WAFER_SUMMARY  1970-01-01 00:00:00  \n",
       "4      INLINE_WAFER_SUMMARY  1970-01-01 00:00:00  \n",
       "...                     ...                  ...  \n",
       "32273                   NaN  1970-01-01 00:00:00  \n",
       "32274                   NaN  1970-01-01 00:00:00  \n",
       "32275                   NaN  1970-01-01 00:00:00  \n",
       "32276                   NaN  1970-01-01 00:00:00  \n",
       "32277                   NaN  1970-01-01 00:00:00  \n",
       "\n",
       "[32278 rows x 143 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_pandas = pd.read_csv(\"inline1.csv\")\n",
    "df1_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e6edf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = ps.from_pandas(df1_pandas).to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abeb7cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32278"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03ed2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, max, col, countDistinct, when, rank, lit\n",
    "\n",
    "############################################\n",
    "######## 1. 客户只定义了bad_wafer = []是什么  ########\n",
    "############################################\n",
    "# 将传进来的BAD_WAFER, 用 | 连接起来，\n",
    "# F.col('WAFER_ID').like('NDJ065%') | F.col('WAFER_ID').like('NDJ067%') 作为条件传入增加label\n",
    "# 同时将isin模式也作为条件传入增加label\n",
    "\n",
    "def get_label_single(df, bad_wafer):\n",
    "    like_conditions = [f\"col('WAFER_ID').like('{bad}')\" for bad in bad_wafer]\n",
    "    all_like_conditions = \" | \".join(like_conditions)\n",
    "    isin_conditions = \"col('WAFER_ID').isin(bad_wafer)\"\n",
    "    df = df.withColumn('label', \n",
    "                when( eval(all_like_conditions) | eval(isin_conditions), int(1)).otherwise(int(0)))\n",
    "    return df\n",
    "\n",
    "\n",
    "############################################\n",
    "## 2. 客户定义了bad_wafer = [] 和 good_wafer = []######\n",
    "############################################\n",
    "# 将传进来的BAD_WAFER, 用 | 连接起来，\n",
    "# 将传进来的GOOD_WAFER, 也用 | 连接起来，\n",
    "# 同时将isin模式也作为条件传入增加label\n",
    "\n",
    "def get_label_double(df, bad_wafer, good_wafer):\n",
    "    good_like_conditions = [f\"col('WAFER_ID').like('{good}')\" for good in good_wafer]\n",
    "    all_good_like_conditions = \" | \".join(good_like_conditions)\n",
    "    good_isin_conditions = \"col('WAFER_ID').isin(good_wafer)\"\n",
    "\n",
    "    bad_like_conditions = [f\"col('WAFER_ID').like('{bad}')\" for bad in bad_wafer]\n",
    "    all_bad_like_conditions = \" | \".join(bad_like_conditions)\n",
    "    bad_isin_conditions = \"col('WAFER_ID').isin(bad_wafer)\"\n",
    "\n",
    "    df = df.withColumn('label',  when(eval(all_good_like_conditions) | eval(good_isin_conditions), int(0)).when(eval(all_bad_like_conditions) | eval(bad_isin_conditions), int(1)).otherwise(222333))\n",
    "    df = df.filter(df['label'] != int(222333))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08ba2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "good = [\"NAZ415-06\",\"NAZ415-08\",\"NAZ415-12\",\"NAZ415-13\",\"NAZ439-07\",\"NAZ703-08\",\"NAZ703-09\"]\n",
    "bad  = [\"NAZ439-03\",\"NAZ439-06\",\"NAZ703-01\",\"NBX082-05\",\"NBX082-12\",\"NBX082-15\",\"NBX082-16\",\"NBX219-17\"]\n",
    "\n",
    "df1 = get_label_double(df1, bad, good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22860f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aa0431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "########################################T检验分析#####################################\n",
    "####################################################################################\n",
    "\n",
    "# 创建pandas_udf 映射信息\n",
    "null_dataframe = pd.DataFrame(\n",
    "    {\n",
    "    \"OPE_NO\" : [\"test\"],\n",
    "    'INLINE_PARAMETER_ID':['bbbss'],\n",
    "        \"weight\":[0.0],\n",
    "        'AVG_SPEC_CHK_RESULT_COUNT':[0.0]}\n",
    "    )\n",
    "\n",
    "schema = spark.createDataFrame(null_dataframe).schema\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def process_df_ttest(df):\n",
    "    \"\"\"t检验\"\"\"\n",
    "    def  dis_gravity(df,cols):\n",
    "        \"\"\"分类重心距离\"\"\"\n",
    "        df_good = df.query('label == 0')\n",
    "        df_bad = df.query('label == 1')\n",
    "        feature_cols = []\n",
    "        for col in cols:\n",
    "            sample_good_count = len(df_good[col].dropna())\n",
    "            sample_bad_count = len(df_bad[col].dropna())\n",
    "\n",
    "            if df[col].nunique() >1 and (sample_bad_count) > 0 and sample_good_count > 0:\n",
    "                sum_value = df_good[col].dropna().mean() + df_bad[col].dropna().mean()\n",
    "                feature_cols.append(col)\n",
    "\n",
    "        for col in feature_cols:\n",
    "            df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].max())\n",
    "\n",
    "        if len(feature_cols) == 0:\n",
    "            return -1\n",
    "        # print(f\"x_good: {x_good}  x_bad: {x_bad}\")\n",
    "        # print(x_good.mean(axis=0), x_bad.mean(axis=0))\n",
    "\n",
    "        # for col in feature_cols:\n",
    "        #     sdf_g= df.query('label ==0')[col].mean()\n",
    "        #     sdf_b = df.query('label ==1')[col].mean()\n",
    "        x_good = []\n",
    "        x_bad = []\n",
    "\n",
    "        for col in feature_cols:\n",
    "            x_good.append(df_good[col].dropna().mean())\n",
    "            x_bad.append(df_bad[col].dropna().mean())\n",
    "        \n",
    "        x_good = np.array(x_good).reshape(-1)\n",
    "        x_bad = np.array(x_bad).reshape(-1)\n",
    "        diff = x_good - x_bad \n",
    "        score =  np.linalg.norm(diff)\n",
    "        return score \n",
    "\n",
    "\n",
    "    def ttest(sample1, sample2):\n",
    "        \"\"\"t检验\"\"\"\n",
    "        tr = stats.ttest_ind(sample1, sample2)\n",
    "        return tr.__getattribute__(\"pvalue\"),  tr.__getattribute__(\"statistic\")\n",
    "\n",
    "\n",
    "    new_df = df.dropna(subset=['OPE_NO', 'INLINE_PARAMETER_ID']).copy()\n",
    "    new_df['AVG_SPEC_CHK_RESULT'] =  pd.to_numeric( new_df['AVG_SPEC_CHK_RESULT'])\n",
    "    mean_spec_chk_res = new_df['AVG_SPEC_CHK_RESULT'].sum()\n",
    "\n",
    "    if pd.isna(mean_spec_chk_res):\n",
    "        mean_spec_chk_res = 0.0\n",
    "\n",
    "    agg_list = [\"AVERAGE\",'STD_DEV', 'label']\n",
    "\n",
    "    # Make sure that col must be numeric type , if not ,convert it to numeric type\n",
    "    for col in agg_list:\n",
    "        # new_df[col] = new_df[col].astype(float)\n",
    "        new_df[col] = pd.to_numeric(new_df[col])\n",
    "    agg_list.remove(\"label\")\n",
    "\n",
    "    df_good = new_df.query('label == 0')\n",
    "    df_bad = new_df.query('label == 1')\n",
    "    score = dis_gravity(new_df, agg_list)\n",
    "\n",
    "    new_df = new_df[['OPE_NO', 'INLINE_PARAMETER_ID']].head(1)\n",
    "    new_df['weight'] = score\n",
    "    new_df['AVG_SPEC_CHK_RESULT_COUNT'] = mean_spec_chk_res\n",
    "    return new_df[['OPE_NO', 'INLINE_PARAMETER_ID', 'weight', 'AVG_SPEC_CHK_RESULT_COUNT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5485faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_id = 'sd'\n",
    "df1 = df1.select(\"OPE_NO\", \"INLINE_PARAMETER_ID\", \"AVERAGE\",'STD_DEV', 'AVG_SPEC_CHK_RESULT', 'label')\n",
    "mean_value_AVERAGE = df1.agg({\"AVERAGE\": \"mean\"}).collect()[0][0]\n",
    "mean_value_STD_DEV = df1.agg({\"STD_DEV\": \"mean\"}).collect()[0][0]\n",
    "# df1 = df1.na.fill({\"AVERAGE\": mean_value_AVERAGE, \"STD_DEV\":mean_value_STD_DEV})\n",
    "df1= (df1.repartition(10, 'OPE_NO', 'INLINE_PARAMETER_ID').groupby('OPE_NO', 'INLINE_PARAMETER_ID').apply(process_df_ttest).where(\"weight > 0\")\n",
    "            .withColumnRenamed(existing=\"OPE_NO\", new=\"OPER_NO\")\n",
    "            .withColumn('request_id',  lit(request_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a38d5a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------------------+-------------------------+----------+\n",
      "| OPER_NO|INLINE_PARAMETER_ID|              weight|AVG_SPEC_CHK_RESULT_COUNT|request_id|\n",
      "+--------+-------------------+--------------------+-------------------------+----------+\n",
      "|1C.CDG10|               FEW0| 0.11711917285714435|                      0.0|        sd|\n",
      "|1C.CDG10|               SFW0|  1.4464285714285716|                      0.0|        sd|\n",
      "|1C.PQW10|               02W0|6.666666666666765E-4|                      0.0|        sd|\n",
      "|1C.PQW10|               28W0|0.001566666666666...|                      0.0|        sd|\n",
      "|1C.PQW10|               44W0|1.333333333333325...|                      0.0|        sd|\n",
      "|1C.PQX10|               AJW0|1.118000000000002...|                      0.0|        sd|\n",
      "|1C.PQX10|               AYW0|7.192000000000001E-4|                      0.0|        sd|\n",
      "|1C.PQX10|               B5W0|3.848000000000000...|                      0.0|        sd|\n",
      "|1C.PQX10|               MX01|0.001159803244831...|                      0.0|        sd|\n",
      "|1C.PQX10|               XIW0|9.377999999999997E-4|                      0.0|        sd|\n",
      "|1C.PQX10|               XMW0|             5.96E-5|                      0.0|        sd|\n",
      "|1C.PQX10|               YTW0|             1.62E-5|                      0.0|        sd|\n",
      "|1U.CDG20|               EEW0| 0.18832313035727566|                      0.0|        sd|\n",
      "|1U.ECU10|               FGS1|0.006991090675832...|                      0.0|        sd|\n",
      "|1U.EQW10|               27W0|0.006749999999999999|                      0.0|        sd|\n",
      "|1U.EQW10|               32W0| 0.00607499999999999|                      0.0|        sd|\n",
      "|1U.EQW10|               33W0|0.006524999999999989|                      0.0|        sd|\n",
      "|1U.EQW20|               20W0|0.055871428571428565|                      0.0|        sd|\n",
      "|1U.EQW20|               26W0|0.056057142857142854|                      0.0|        sd|\n",
      "|1U.EQW20|               29W0|0.003742857142857145|                      0.0|        sd|\n",
      "+--------+-------------------+--------------------+-------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2b686f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3615a4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc4f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e6fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#####################################正式调用以上函数##################################\n",
    "####################################################################################\n",
    "# 1. 解析json 为字典， df1 为kafka 输入的结果数据\n",
    "df2 = df1.toPandas() \n",
    "# 默认只取第一行\n",
    "if len(df2) > 0:\n",
    "    df2 = df2.head(1)\n",
    "request_id = df2[\"requestId\"].values[0]\n",
    "request_params = df2[\"requestParam\"].values[0]\n",
    "request_params = request_params.replace('\\'', \"\\\"\")  # 避免存在单引号，因为json 引号只有双引号\n",
    "parse_dict = json.loads(request_params)\n",
    "\n",
    "# 2. 从kafka 关键字映射都具体数据源中的字段,没有的可以删除\n",
    "keyword_map_from_json_to_table: dict = {\n",
    "    \"waferId\": \"WAFER_ID\",\n",
    "    \"dateRange\": \"MEASURE_TIME\",\n",
    "    \"productId\": \"PRODUCT_ID\",\n",
    "    \"operNo\": \"OPE_NO\",\n",
    "    # \"inlineParameterId\": 'INLINE_PARAMETER_ID', \n",
    "    # \"eqp\": \"TOOL_NAME\",\n",
    "    \"lot\": \"LOT_ID\",\n",
    "    # \"recipeName\": \"RECIPE_NAME\"\n",
    "}\n",
    "\n",
    "select_condition_list = parse_dict\n",
    "\n",
    "#  3. 查询表名, 需要修改\n",
    "table_name = \"ODS_EDA.ODS_INLINE_WAFER_SUMMARY\"\n",
    "\n",
    "\n",
    "\n",
    "# 4. 查询条件转sql,并读取数据\n",
    "flag = True\n",
    "try:\n",
    "    select_df_list = [read_sql(trans_select_condition_to_sql_with_label(select_condition_dict, table_name)) for select_condition_dict in select_condition_list]\n",
    "    # 多个进行union\n",
    "    df1 = reduce(DataFrame.unionAll, select_df_list)\n",
    "except Exception as e:\n",
    "    flag = False\n",
    "    df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": '系统内部异常! ',  'requestId': request_id}, index=[0])\n",
    "    df_kafka_ = spark.createDataFrame(df_kafka)\n",
    "    df1 = df_kafka_\n",
    "\n",
    "\n",
    "\n",
    "if df1.isEmpty():\n",
    "    df_kafka = pd.DataFrame({\"code\": 1,  \"msg\": '该条件下无数据，请检查! ',  'requestId': request_id}, index=[0])\n",
    "    df_kafka_ = spark.createDataFrame(df_kafka)\n",
    "    df1 = df_kafka_\n",
    "else:\n",
    "    if flag == False:\n",
    "        df1 = df1\n",
    "    else:\n",
    "\n",
    "        df1 = df1.select(\"OPE_NO\", \"INLINE_PARAMETER_ID\", \"AVERAGE\",'STD_DEV', 'AVG_SPEC_CHK_RESULT', 'label')\n",
    "        mean_value_AVERAGE = df1.agg({\"AVERAGE\": \"mean\"}).collect()[0][0]\n",
    "        mean_value_STD_DEV = df1.agg({\"STD_DEV\": \"mean\"}).collect()[0][0]\n",
    "        # df1 = df1.na.fill({\"AVERAGE\": mean_value_AVERAGE, \"STD_DEV\":mean_value_STD_DEV})\n",
    "        df1= (df1.repartition(10, 'OPE_NO', 'INLINE_PARAMETER_ID').groupby('OPE_NO', 'INLINE_PARAMETER_ID').apply(process_df_ttest).where(\"weight > 0\")\n",
    "                    .withColumnRenamed(existing=\"OPE_NO\", new=\"OPER_NO\")\n",
    "                    .withColumn('request_id',  lit(request_id)))\n",
    "        if df1.isEmpty():\n",
    "            df_kafka = pd.DataFrame({'code': 1,  'msg': '该场景下数据库中暂无充足的数据(真实BAD个数可能为0或真实GOOD个数可能为0)', 'requestId': request_id}, index=[1])\n",
    "            df1 = spark.createDataFrame(df_kafka)\n",
    "\n",
    "        else:\n",
    "            df1 = df1.toPandas()\n",
    "            df1 = df1.sort_values(by=['weight'], ascending=False).head(30)\n",
    "            df1['index_no'] = range(1, 31)\n",
    "            df1['weight'] = df1['weight'] / df1['weight'].sum()\n",
    "            df1['weight_percent'] = df1['weight'] * 100\n",
    "            df1['AVG_SPEC_CHK_RESULT_COUNT'] = df1['AVG_SPEC_CHK_RESULT_COUNT'].fillna(0)\n",
    "            df1['AVG_SPEC_CHK_RESULT_COUNT'] = df1['AVG_SPEC_CHK_RESULT_COUNT'].astype(int)\n",
    "\n",
    "            user =\"root\"\n",
    "            host = \"10.52.199.81\"\n",
    "            password = \"Nexchip%40123\"\n",
    "            db = \"etl\"\n",
    "            port = 9030\n",
    "            engine = create_engine(\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\".format(user = user,\n",
    "                                                                                                password = password,\n",
    "                                                                                                host = host,\n",
    "                                                                                                port = port,\n",
    "                                                                                                db = db))\n",
    "            doris_stream_load_from_df(df1, engine, \"inline_results\")\n",
    "\n",
    "            df_kafka = pd.DataFrame({'code': 0,  'msg': '运行成功', 'requestId': request_id}, index=[1])\n",
    "            df1 = spark.createDataFrame(df_kafka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad125ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf3e4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44d538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8dd19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6bbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#####################################结果写回数据库####################################\n",
    "####################################################################################\n",
    "def doris_stream_load_from_df(df, engine, table, is_json=True, chunksize=100000, partitions=None):\n",
    "    engine_url = engine.url\n",
    "    url = 'http://%s:18030/api/%s/%s/_stream_load' % (engine_url.host, engine_url.database, table)\n",
    "\n",
    "    format_str = 'csv' if not is_json else 'json'\n",
    "    headers = {\n",
    "        'Content-Type': 'text/plain; charset=UTF-8',\n",
    "        'format': format_str,\n",
    "        'Expect': '100-continue'\n",
    "    }\n",
    "    if is_json:\n",
    "        headers['strip_outer_array'] = 'true'\n",
    "        headers['read_json_by_line'] = 'true'\n",
    "    else:\n",
    "        headers['column_separator'] = '@'\n",
    "    \n",
    "    if partitions:\n",
    "        headers['partitions'] = partitions\n",
    "    \n",
    "    auth = requests.auth.HTTPBasicAuth(engine_url.username, engine_url.password)\n",
    "    session = requests.sessions.Session()\n",
    "    session.should_strip_auth = lambda old_url, new_url: False\n",
    "    \n",
    "    l = len(df)\n",
    "    if l > 0:\n",
    "        if chunksize and chunksize < l:\n",
    "            batches = l // chunksize\n",
    "            if l % chunksize > 0:\n",
    "                batches += 1\n",
    "            for i in range(batches):\n",
    "                si = i * chunksize\n",
    "                ei = min(si + chunksize, l)\n",
    "                sub = df[si:ei]\n",
    "                do_doris_stream_load_from_df(sub, session, url, headers, auth, is_json)\n",
    "        else:\n",
    "            do_doris_stream_load_from_df(df, session, url, headers, auth, is_json)\n",
    "\n",
    "\n",
    "def do_doris_stream_load_from_df(df, session, url, headers, auth, is_json=False):\n",
    "    data = df.to_csv(header=False, index=False, sep='@') if not is_json else df.to_json(orient='records', date_format='iso')\n",
    "    #print(data)\n",
    "    \n",
    "    resp = session.request(\n",
    "        'PUT',\n",
    "        url = url,\n",
    "        data=data.encode('utf-8'),\n",
    "        headers=headers,\n",
    "        auth=auth\n",
    "    )\n",
    "    print(resp.reason, resp.text)\n",
    "    check_stream_load_response(resp.text)\n",
    "\n",
    "\n",
    "\n",
    "def check_stream_load_response(resp_text):\n",
    "    resp = json.loads(resp_text)\n",
    "    if resp['Status'] not in [\"Success\", \"Publish Timeout\"]:\n",
    "        raise Exception(resp['Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9e3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e5a340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70fc6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51bfc28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045edd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a7c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae82c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ea6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3c239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a272017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b638a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
