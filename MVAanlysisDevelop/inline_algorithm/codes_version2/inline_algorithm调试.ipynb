{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2575c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f578f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import pymysql \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pyspark.pandas as ps\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from scipy import stats\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Optional\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import URL \n",
    "# from backend_spark.doris_common.doris_client import DorisClient\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, monotonically_increasing_id, lit, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "343dba65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "  .master(\"local[*]\")\n",
    "  .config(\"spark.jars.packages\", \"ai.catboost:catboost-spark_3.3_2.12:1.2\")\n",
    "  .appName(\"RF\")\n",
    "  .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "d9e41c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "client81 = pymysql.connect(user='root', password='Nexchip@123', port=9030, host='10.52.199.81')\n",
    "data_count = 0\n",
    "\n",
    "# doris 数据库连接\n",
    "# client91 = DorisClient(\"10.52.199.91\", 18030, 9030, user=\"ikas_user\", password=\"Ikas_user@123\", data_base=\"ODS_EDA\",\n",
    "#                      mem_limit=\"68719476736\")\n",
    "\n",
    "# client81 = DorisClient(\"10.52.199.81\", 18030, 9030, user=\"root\", password=\"Nexchip@123\", data_base=\"ODS_EDA\",\n",
    "#                      mem_limit=\"68719476736\")\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "###################################解析sql 的辅助函数###################################\n",
    "####################################################################################\n",
    "def read_sql1(sql_stat, read_client=client81, session=spark):\n",
    "    # df1 = read_client.doris_read(session, sql_stat)\n",
    "    df1 = spark.createDataFrame(pd.read_sql(sql=sql_stat, con=client81))\n",
    "    return df1\n",
    "\n",
    "\n",
    "def read_sql(sql_stat, read_client=client81, session=spark):\n",
    "    # df1 = read_client.doris_read(session, sql_stat)\n",
    "    ds = pd.read_sql(sql=sql_stat, con=client81)\n",
    "    if ds is None or len(ds) == 0:\n",
    "        empty_schema = StructType([\n",
    "            StructField(\"WAFER_ID\", StringType(), True),\n",
    "            StructField(\"OPE_NO\", StringType(), True),\n",
    "            StructField(\"INLINE_PARAMETER_ID\", StringType(), True),\n",
    "            StructField(\"PRODUCT_ID\", StringType(), True),\n",
    "            StructField(\"AVERAGE\", StringType(), True),\n",
    "            StructField(\"STD_DEV\", StringType(), True),\n",
    "            StructField(\"MEASURE_TIME\", TimestampType(), True),\n",
    "            StructField(\"AVG_SPEC_CHK_RESULT\", StringType(), True),\n",
    "            StructField(\"label\", StringType(), True)\n",
    "        ])\n",
    "        df1 = spark.createDataFrame([], empty_schema)\n",
    "        #print(df1)\n",
    "        #null_dataframe = pd.DataFrame(\n",
    "        #{\n",
    "        #\"WAFER_ID\" : [\"\"],\n",
    "        #\"OPE_NO\" : [\"\"],\n",
    "        #\"INLINE_PARAMETER_ID\" : [\"\"],\n",
    "        #\"PRODUCT_ID\" : [\"\"],\n",
    "        #\"AVERAGE\" : [0.0],\n",
    "        #\"STD_DEV\" : [0.0],\n",
    "        #\"MEASURE_TIME\" : [\"\"],\n",
    "        #\"AVG_SPEC_CHK_RESULT\" : [\"\"],\n",
    "        #\"label\" : [0],\n",
    "        #}\n",
    "        #)\n",
    "        #df1 = spark.createDataFrame(null_dataframe)\n",
    "    else:\n",
    "        df1 = spark.createDataFrame(ds)\n",
    "        data_count = 1\n",
    "    return df1\n",
    "\n",
    "def process_like(key: str, value: list[str]) -> str:\n",
    "    # 处理模糊条件的匹配: (key like 'aa%' or key like \"bb%\")\n",
    "    key = keyword_map_from_json_to_table.get(key)\n",
    "    v_join = ' or '.join([f\"{key} like  '{v.replace('*', '%')}' \" for v in value])\n",
    "    return \"({})\".format(v_join)\n",
    "\n",
    "\n",
    "def process_not_like(key: str, value: list[str]) -> str:\n",
    "    # 处理非模糊条件的匹配:key in ('aa', 'bb')\n",
    "    key = keyword_map_from_json_to_table.get(key)\n",
    "    v_join = \",\".join([f\"'{v}'\" for v in value])\n",
    "    return \"{} in ({})\".format(key, v_join)\n",
    "\n",
    "\n",
    "def test_not_like():\n",
    "    result = (process_not_like(\"tool_name\", [\"aa\", \"bb\", \"cc\"]))\n",
    "    assert \"tool_name in ('aa','bb','cc')\" == result, \"not like 验证失败\"\n",
    "\n",
    "\n",
    "def test_like():\n",
    "    result = process_like(\"tool_name\", [\"aa*\", \"bb*\", \"cc*\"])\n",
    "\n",
    "    assert \"(tool_name like  'aa%'  or tool_name like  'bb%'  or tool_name like  'cc%' )\" == result, \"like 验证失败\"\n",
    "\n",
    "\n",
    "def process_one_keyword(key, value: list[str]) -> Optional[str]:\n",
    "    if len(value) == 0:\n",
    "        return None\n",
    "\n",
    "    not_like_list = [v for v in value if \"*\" not in v]\n",
    "    like_list = [v for v in value if \"*\" in v]\n",
    "\n",
    "    # 处理模糊条件\n",
    "    if len(not_like_list) != 0:\n",
    "        not_like_sql_str = process_not_like(key, not_like_list)\n",
    "    else:\n",
    "        not_like_sql_str = \"\"\n",
    "\n",
    "    # 处理非模糊条件\n",
    "\n",
    "    if len(like_list) != 0:\n",
    "        like_sql_str = process_like(key, like_list)\n",
    "    else:\n",
    "        like_sql_str = \"\"\n",
    "\n",
    "    # 去除为一个元素为空字符串的情况的情况的情况\n",
    "    concat_sql_str_list = [sql_str for sql_str in [like_sql_str, not_like_sql_str] if len(sql_str) != 0]\n",
    "    # 使用or 操作 单字段过滤 的like 和 not like 语句\n",
    "    return \"(\" + \" or \".join(concat_sql_str_list) + \")\"\n",
    "\n",
    "\n",
    "def check_time_start_end(min_time, max_time):\n",
    "    if min_time is not None and max_time is not None:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"起始时间和结束时间必须全填\")\n",
    "\n",
    "def get_time_selection_sql(time_keyword, max_time=None, min_time=None):\n",
    "    \"\"\"\n",
    "    获取时间区间的筛选的sql, 起始时间和结束时间都是可选的\n",
    "    :param time_keyword:\n",
    "    :param max_time:\n",
    "    :param min_time:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 根据取值，生成单个时间过滤条件\n",
    "    if min_time:\n",
    "        time_part_min = f\"{time_keyword} > '{min_time}'\"\n",
    "    else:\n",
    "        time_part_min = \" \"\n",
    "\n",
    "    if max_time:\n",
    "        time_part_max = f\"{time_keyword} <= '{max_time}'\"\n",
    "    else:\n",
    "        time_part_max = \" \"\n",
    "\n",
    "    # 如果存在，拼接多个查询条件，或者只保留一个过滤条件\n",
    "    if (max_time is not None) and (min_time is not None):\n",
    "        time_sql = f' {time_part_min} and {time_part_max}'\n",
    "    elif (max_time is None) and (min_time is None):\n",
    "        time_sql = \" \"\n",
    "    else:\n",
    "        time_sql = time_part_max if max_time else time_part_min\n",
    "\n",
    "    return time_sql\n",
    "\n",
    "def concat_time_filter_sql_with_other_keyword_sql(time_filter_sql: str, other_keyword_sql:str) -> str:\n",
    "    \"\"\"\n",
    "    拼接时间过滤条件与非时间过滤条件\n",
    "    :param time_filter_sql:\n",
    "    :param other_keyword_sql:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    time_strip = time_filter_sql.strip()\n",
    "    other_strip = other_keyword_sql.strip()\n",
    "    if len(time_strip) == 0 and len(other_strip) == 0:\n",
    "        return \"\"\n",
    "    elif len(time_strip) != 0 and len(other_strip) == 0:\n",
    "        return  time_filter_sql\n",
    "    elif len(time_strip) == 0 and len(other_strip) != 0:\n",
    "        return other_keyword_sql\n",
    "    else:\n",
    "        return f'{time_filter_sql} and {other_keyword_sql}'\n",
    "\n",
    "def trans_select_condition_to_sql_with_label(select_condition_dict: dict, table_name: str) -> str:\n",
    "    # 查询条件转sql,并打标签，label '0': good wafer, '1': bad wafer\n",
    "    filter_sql_list = []\n",
    "    for keyword, value in select_condition_dict.items():\n",
    "        if keyword not in [\"dateRange\", \"waferId\", \"uploadId\", \"mergeProdg1\"]:\n",
    "            sql_filter_one_keyword = process_one_keyword(keyword, value)\n",
    "            if sql_filter_one_keyword is not None:\n",
    "                filter_sql_list.append(sql_filter_one_keyword)\n",
    "\n",
    "    # 处理时间区间\n",
    "    time_bin = select_condition_dict.get(\"dateRange\")\n",
    "\n",
    "    if len(time_bin) >0:\n",
    "        time_bin_dict = time_bin[0]\n",
    "        min_time = time_bin_dict.get(\"start\")\n",
    "        max_time = time_bin_dict.get(\"end\")\n",
    "    else:\n",
    "        min_time = None \n",
    "        max_time = None \n",
    "\n",
    "    # 去除时间检查，时间范围为可选输入\n",
    "    # 检查起始时间和结束时间全部非空\n",
    "    # check_time_start_end(min_time, max_time)\n",
    "\n",
    "    # 处理waferId\n",
    "    waferId = select_condition_dict.get(\"waferId\")\n",
    "    good_wafer_list = waferId.get(\"good\")\n",
    "    bad_wafer_list = waferId.get(\"bad\")\n",
    "\n",
    "    # 根据time 过滤条件,生成sql\n",
    "    time_filter_sql = get_time_selection_sql(time_keyword=keyword_map_from_json_to_table.get('dateRange'), max_time=max_time, min_time=min_time)\n",
    "\n",
    "\n",
    "    if len(good_wafer_list) + len(bad_wafer_list) == 0:\n",
    "        raise ValueError(\"good, bad wafer 至少选择一个\")\n",
    "    elif len(good_wafer_list) > 0 and len(bad_wafer_list) > 0:\n",
    "        # good wafer, bad wafe 均有指定，需要从层层字段的过滤的条件下选择\n",
    "        good_wafer_filter_sql = process_one_keyword(\"waferId\", good_wafer_list)\n",
    "        bad_wafer_filter_sql = process_one_keyword(\"waferId\", bad_wafer_list)\n",
    "        # or 拼接\n",
    "        wafer_filter_sql = \" or \".join([good_wafer_filter_sql, bad_wafer_filter_sql])\n",
    "        wafer_filter_sql = f\"({wafer_filter_sql})\"\n",
    "        # 加入wafer 过滤条件\n",
    "        filter_sql_list.append(wafer_filter_sql)\n",
    "        other_keyword_filter = \" and \".join(filter_sql_list)\n",
    "\n",
    "        case_when_statment = f\"\"\"(\n",
    "        case \n",
    "        when {good_wafer_filter_sql} then 0\n",
    "        else 1 \n",
    "        end \n",
    "        ) label\n",
    "        \"\"\"\n",
    "\n",
    "        filter_sql_concat = concat_time_filter_sql_with_other_keyword_sql(time_filter_sql, other_keyword_filter)\n",
    "        if filter_sql_concat != '':\n",
    "            select_sql = f\"select *, {case_when_statment} from {table_name} where {filter_sql_concat}\"\n",
    "        else:\n",
    "             select_sql = f\"select *, {case_when_statment} from {table_name}\"\n",
    "\n",
    "        \n",
    "       \n",
    "    elif len(good_wafer_list) > 0 and len(bad_wafer_list) == 0:\n",
    "        # 选good, 剩余为 bad\n",
    "        other_keyword_filter = \" and \".join(filter_sql_list)\n",
    "        good_wafer_filter_sql = process_one_keyword(\"waferId\", good_wafer_list)\n",
    "\n",
    "        case_when_statment = f\"(case when {good_wafer_filter_sql} then 0 else 1  end ) label\"\n",
    "        filter_sql_concat = concat_time_filter_sql_with_other_keyword_sql(time_filter_sql, other_keyword_filter)\n",
    "        select_sql = f\"select *, {case_when_statment} from {table_name} where {filter_sql_concat}\"\n",
    "\n",
    "    elif len(good_wafer_list) == 0 and len(bad_wafer_list) > 0:\n",
    "        # 选bad, 剩余为good\n",
    "        other_keyword_filter = \" and \".join(filter_sql_list)\n",
    "        bad_wafer_filter_sql = process_one_keyword(\"waferId\", bad_wafer_list)\n",
    "        case_when_statment = f\"\"\"(case when {bad_wafer_filter_sql} then 1 else 0 end ) label\"\"\"\n",
    "        filter_sql_concat = concat_time_filter_sql_with_other_keyword_sql(time_filter_sql, other_keyword_filter)\n",
    "        select_sql = f\"select *, {case_when_statment} from {table_name} where {filter_sql_concat}\"\n",
    "        # case1 stat results 表没有case_info 时间列，暂时去掉\n",
    "        # select_sql = f\"select *, {case_when_statment} from {table_name} where {other_keyword_filter}\"\n",
    "    select_keywords = \"\"\"WAFER_ID,\n",
    "    OPE_NO,\n",
    "    INLINE_PARAMETER_ID,\n",
    "    PRODUCT_ID,\n",
    "    LOT_ID,\n",
    "    `AVERAGE`,\n",
    "    STD_DEV,\n",
    "    MEASURE_TIME,\n",
    "    AVG_SPEC_CHK_RESULT\"\"\"\n",
    "    select_sql = select_sql.replace( \"*\",  select_keywords )\n",
    "    print(select_sql)\n",
    "    return select_sql\n",
    "\n",
    "\n",
    "\n",
    "def get_data_from_doris(select_condition_list, table_name):\n",
    "    try:\n",
    "        select_df_list = [read_sql(trans_select_condition_to_sql_with_label(select_condition_dict, table_name)) for select_condition_dict in select_condition_list]\n",
    "        # 多个进行union\n",
    "        df1 = reduce(DataFrame.unionAll, select_df_list)\n",
    "        return df1\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "############################################################################\n",
    "##############################从kafka消息读取需要的资料#########################\n",
    "############################################################################\n",
    "def get_some_info(df:pd.DataFrame):\n",
    "    try:\n",
    "        if len(df) > 0:\n",
    "            df = df.head(1)\n",
    "\n",
    "        request_id = df[\"requestId\"].values[0]\n",
    "        request_params = df[\"requestParam\"].values[0]\n",
    "        # 避免存在单引号，因为json 引号只有双引号\n",
    "        request_params = request_params.replace('\\'', \"\\\"\")   \n",
    "        parse_dict = json.loads(request_params)\n",
    "        return parse_dict, request_id\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6edeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d24f290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WAFER_ID</th>\n",
       "      <th>OPE_NO</th>\n",
       "      <th>INLINE_PARAMETER_ID</th>\n",
       "      <th>MEASURE_TIME</th>\n",
       "      <th>RANGE_INDEX</th>\n",
       "      <th>FAB_ID</th>\n",
       "      <th>PRODUCT_ID</th>\n",
       "      <th>LOT_ID</th>\n",
       "      <th>AVERAGE</th>\n",
       "      <th>MAX_VAL</th>\n",
       "      <th>...</th>\n",
       "      <th>RANGE</th>\n",
       "      <th>ACT_CODE</th>\n",
       "      <th>ETL_INSERT_TIME</th>\n",
       "      <th>ETL_ARC_FLAG</th>\n",
       "      <th>ETL_BATCH_SYNC_TS</th>\n",
       "      <th>ETL_DEL_FLAG</th>\n",
       "      <th>ETL_DS_JOB_NM</th>\n",
       "      <th>ETL_SRC_DB</th>\n",
       "      <th>ETL_SRC_TBL</th>\n",
       "      <th>ETL_TBL_OPER_TS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NAZ415-06</td>\n",
       "      <td>1C.CDG10</td>\n",
       "      <td>CIW0</td>\n",
       "      <td>2022-12-03 01:45:41</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNM301N.0A01</td>\n",
       "      <td>NAZ415000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-29 04:34:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EDA</td>\n",
       "      <td>INLINE_WAFER_SUMMARY</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NAZ415-06</td>\n",
       "      <td>1C.CDG10</td>\n",
       "      <td>EEW0</td>\n",
       "      <td>2022-12-03 01:45:41</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNM301N.0A01</td>\n",
       "      <td>NAZ415000</td>\n",
       "      <td>3999.845454</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-29 04:34:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EDA</td>\n",
       "      <td>INLINE_WAFER_SUMMARY</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NAZ415-06</td>\n",
       "      <td>1C.CDG10</td>\n",
       "      <td>FEW0</td>\n",
       "      <td>2022-12-03 01:45:41</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNM301N.0A01</td>\n",
       "      <td>NAZ415000</td>\n",
       "      <td>123.402563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-29 04:34:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EDA</td>\n",
       "      <td>INLINE_WAFER_SUMMARY</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NAZ415-06</td>\n",
       "      <td>1C.CDG10</td>\n",
       "      <td>HFT0</td>\n",
       "      <td>2022-12-03 01:45:41</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNM301N.0A01</td>\n",
       "      <td>NAZ415000</td>\n",
       "      <td>44.099000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-29 04:34:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EDA</td>\n",
       "      <td>INLINE_WAFER_SUMMARY</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NAZ415-06</td>\n",
       "      <td>1C.CDG10</td>\n",
       "      <td>OEW0</td>\n",
       "      <td>2022-12-03 01:45:41</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNM301N.0A01</td>\n",
       "      <td>NAZ415000</td>\n",
       "      <td>6000.016364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-29 04:34:30</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EDA</td>\n",
       "      <td>INLINE_WAFER_SUMMARY</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32273</th>\n",
       "      <td>NBX219-17</td>\n",
       "      <td>1U.CDG20</td>\n",
       "      <td>PTW0</td>\n",
       "      <td>2023-08-31 23:37:37</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNR901N.0B0J</td>\n",
       "      <td>NBX219000</td>\n",
       "      <td>5500.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-31 23:47:47</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32274</th>\n",
       "      <td>NBX219-17</td>\n",
       "      <td>1U.CDG20</td>\n",
       "      <td>REW0</td>\n",
       "      <td>2023-08-31 23:37:37</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNR901N.0B0J</td>\n",
       "      <td>NBX219000</td>\n",
       "      <td>198.846013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-31 23:47:47</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32275</th>\n",
       "      <td>NBX219-17</td>\n",
       "      <td>1U.CDG20</td>\n",
       "      <td>SEW0</td>\n",
       "      <td>2023-08-31 23:37:37</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNR901N.0B0J</td>\n",
       "      <td>NBX219000</td>\n",
       "      <td>1.292111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-31 23:47:47</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32276</th>\n",
       "      <td>NBX219-17</td>\n",
       "      <td>1U.CDG20</td>\n",
       "      <td>SFW0</td>\n",
       "      <td>2023-08-31 23:37:37</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNR901N.0B0J</td>\n",
       "      <td>NBX219000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-31 23:47:47</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32277</th>\n",
       "      <td>NBX219-17</td>\n",
       "      <td>1U.CDG20</td>\n",
       "      <td>TSW0</td>\n",
       "      <td>2023-08-31 23:37:37</td>\n",
       "      <td>0</td>\n",
       "      <td>N1</td>\n",
       "      <td>AFPNR901N.0B0J</td>\n",
       "      <td>NBX219000</td>\n",
       "      <td>350.116364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-31 23:47:47</td>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32278 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        WAFER_ID    OPE_NO INLINE_PARAMETER_ID         MEASURE_TIME  \\\n",
       "0      NAZ415-06  1C.CDG10                CIW0  2022-12-03 01:45:41   \n",
       "1      NAZ415-06  1C.CDG10                EEW0  2022-12-03 01:45:41   \n",
       "2      NAZ415-06  1C.CDG10                FEW0  2022-12-03 01:45:41   \n",
       "3      NAZ415-06  1C.CDG10                HFT0  2022-12-03 01:45:41   \n",
       "4      NAZ415-06  1C.CDG10                OEW0  2022-12-03 01:45:41   \n",
       "...          ...       ...                 ...                  ...   \n",
       "32273  NBX219-17  1U.CDG20                PTW0  2023-08-31 23:37:37   \n",
       "32274  NBX219-17  1U.CDG20                REW0  2023-08-31 23:37:37   \n",
       "32275  NBX219-17  1U.CDG20                SEW0  2023-08-31 23:37:37   \n",
       "32276  NBX219-17  1U.CDG20                SFW0  2023-08-31 23:37:37   \n",
       "32277  NBX219-17  1U.CDG20                TSW0  2023-08-31 23:37:37   \n",
       "\n",
       "       RANGE_INDEX FAB_ID      PRODUCT_ID     LOT_ID      AVERAGE  MAX_VAL  \\\n",
       "0                0     N1  AFPNM301N.0A01  NAZ415000          NaN      NaN   \n",
       "1                0     N1  AFPNM301N.0A01  NAZ415000  3999.845454      NaN   \n",
       "2                0     N1  AFPNM301N.0A01  NAZ415000   123.402563      NaN   \n",
       "3                0     N1  AFPNM301N.0A01  NAZ415000    44.099000      NaN   \n",
       "4                0     N1  AFPNM301N.0A01  NAZ415000  6000.016364      NaN   \n",
       "...            ...    ...             ...        ...          ...      ...   \n",
       "32273            0     N1  AFPNR901N.0B0J  NBX219000  5500.000000      NaN   \n",
       "32274            0     N1  AFPNR901N.0B0J  NBX219000   198.846013      NaN   \n",
       "32275            0     N1  AFPNR901N.0B0J  NBX219000     1.292111      NaN   \n",
       "32276            0     N1  AFPNR901N.0B0J  NBX219000     3.000000      NaN   \n",
       "32277            0     N1  AFPNR901N.0B0J  NBX219000   350.116364      NaN   \n",
       "\n",
       "       ...  RANGE  ACT_CODE      ETL_INSERT_TIME  ETL_ARC_FLAG  \\\n",
       "0      ...    NaN       NaN  2023-05-29 04:34:30             0   \n",
       "1      ...    NaN       NaN  2023-05-29 04:34:30             0   \n",
       "2      ...    NaN       NaN  2023-05-29 04:34:30             0   \n",
       "3      ...    NaN       NaN  2023-05-29 04:34:30             0   \n",
       "4      ...    NaN       NaN  2023-05-29 04:34:30             0   \n",
       "...    ...    ...       ...                  ...           ...   \n",
       "32273  ...    NaN       NaN  2023-08-31 23:47:47             0   \n",
       "32274  ...    NaN       NaN  2023-08-31 23:47:47             0   \n",
       "32275  ...    NaN       NaN  2023-08-31 23:47:47             0   \n",
       "32276  ...    NaN       NaN  2023-08-31 23:47:47             0   \n",
       "32277  ...    NaN       NaN  2023-08-31 23:47:47             0   \n",
       "\n",
       "         ETL_BATCH_SYNC_TS  ETL_DEL_FLAG  ETL_DS_JOB_NM  ETL_SRC_DB  \\\n",
       "0      1970-01-01 00:00:00             0            NaN         EDA   \n",
       "1      1970-01-01 00:00:00             0            NaN         EDA   \n",
       "2      1970-01-01 00:00:00             0            NaN         EDA   \n",
       "3      1970-01-01 00:00:00             0            NaN         EDA   \n",
       "4      1970-01-01 00:00:00             0            NaN         EDA   \n",
       "...                    ...           ...            ...         ...   \n",
       "32273  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32274  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32275  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32276  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "32277  1970-01-01 00:00:00             0            NaN         NaN   \n",
       "\n",
       "                ETL_SRC_TBL      ETL_TBL_OPER_TS  \n",
       "0      INLINE_WAFER_SUMMARY  1970-01-01 00:00:00  \n",
       "1      INLINE_WAFER_SUMMARY  1970-01-01 00:00:00  \n",
       "2      INLINE_WAFER_SUMMARY  1970-01-01 00:00:00  \n",
       "3      INLINE_WAFER_SUMMARY  1970-01-01 00:00:00  \n",
       "4      INLINE_WAFER_SUMMARY  1970-01-01 00:00:00  \n",
       "...                     ...                  ...  \n",
       "32273                   NaN  1970-01-01 00:00:00  \n",
       "32274                   NaN  1970-01-01 00:00:00  \n",
       "32275                   NaN  1970-01-01 00:00:00  \n",
       "32276                   NaN  1970-01-01 00:00:00  \n",
       "32277                   NaN  1970-01-01 00:00:00  \n",
       "\n",
       "[32278 rows x 143 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_pandas = pd.read_csv(\"inline1.csv\")\n",
    "df1_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6edf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32278"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = ps.from_pandas(df1_pandas).to_spark()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03ed2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, max, col, countDistinct, when, rank, lit\n",
    "\n",
    "############################################\n",
    "######## 1. 客户只定义了bad_wafer = []是什么  ########\n",
    "############################################\n",
    "# 将传进来的BAD_WAFER, 用 | 连接起来，\n",
    "# F.col('WAFER_ID').like('NDJ065%') | F.col('WAFER_ID').like('NDJ067%') 作为条件传入增加label\n",
    "# 同时将isin模式也作为条件传入增加label\n",
    "\n",
    "def get_label_single(df, bad_wafer):\n",
    "    like_conditions = [f\"col('WAFER_ID').like('{bad}')\" for bad in bad_wafer]\n",
    "    all_like_conditions = \" | \".join(like_conditions)\n",
    "    isin_conditions = \"col('WAFER_ID').isin(bad_wafer)\"\n",
    "    df = df.withColumn('label', \n",
    "                when( eval(all_like_conditions) | eval(isin_conditions), int(1)).otherwise(int(0)))\n",
    "    return df\n",
    "\n",
    "\n",
    "############################################\n",
    "## 2. 客户定义了bad_wafer = [] 和 good_wafer = []######\n",
    "############################################\n",
    "# 将传进来的BAD_WAFER, 用 | 连接起来，\n",
    "# 将传进来的GOOD_WAFER, 也用 | 连接起来，\n",
    "# 同时将isin模式也作为条件传入增加label\n",
    "\n",
    "def get_label_double(df, bad_wafer, good_wafer):\n",
    "    good_like_conditions = [f\"col('WAFER_ID').like('{good}')\" for good in good_wafer]\n",
    "    all_good_like_conditions = \" | \".join(good_like_conditions)\n",
    "    good_isin_conditions = \"col('WAFER_ID').isin(good_wafer)\"\n",
    "\n",
    "    bad_like_conditions = [f\"col('WAFER_ID').like('{bad}')\" for bad in bad_wafer]\n",
    "    all_bad_like_conditions = \" | \".join(bad_like_conditions)\n",
    "    bad_isin_conditions = \"col('WAFER_ID').isin(bad_wafer)\"\n",
    "\n",
    "    df = df.withColumn('label',  when(eval(all_good_like_conditions) | eval(good_isin_conditions), int(0)).when(eval(all_bad_like_conditions) | eval(bad_isin_conditions), int(1)).otherwise(222333))\n",
    "    df = df.filter(df['label'] != int(222333))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ba2d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32278"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = [\"NAZ415-06\",\"NAZ415-08\",\"NAZ415-12\",\"NAZ415-13\",\"NAZ439-07\",\"NAZ703-08\",\"NAZ703-09\"]\n",
    "bad  = [\"NAZ439-03\",\"NAZ439-06\",\"NAZ703-01\",\"NBX082-05\",\"NBX082-12\",\"NBX082-15\",\"NBX082-16\",\"NBX219-17\"]\n",
    "\n",
    "df1 = get_label_double(df1, bad, good)\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759a6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cb3e225",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22860f7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1_pandas = df1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629e1317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1C.CDG10', '1U.CDG10', '1U.CDG20', '1V.PQA10', '2U.CDG10',\n",
       "       '2U.CDG20', '2U.PQA10', '2V.PQA10', '2V.PQW10', '3U.CDG10',\n",
       "       '3U.CDG20', '3U.PQW10', '6V.CDG10', '6V.CDG20', '6V.PQA10',\n",
       "       '7U.EQA20', '7U.PQA10', '7U.PQA20', '7U.PQX10', 'PV.CDG10',\n",
       "       'TM.EQA10', 'TM.PQA10', 'TM.PQX10', 'TV.CDG10', 'TV.EQA20',\n",
       "       'TV.PQA10', 'TV.PQX10', 'XX.EQW01', 'XX.PQW01', 'XX.PQX01',\n",
       "       'XX.PQX02', '1U.ECU10', '1V.ECU10', '1V.PQX10', '1V.PQX20',\n",
       "       '2U.EQW10', '2U.PQX10', '2V.ECU10', '2V.PQX10', '2V.PQX20',\n",
       "       '3U.PQA10', '3U.PQX10', '6V.PQW10', '6V.PQX10', '7U.ECU10',\n",
       "       '7U.EQA10', '7U.PQW10', 'PV.EQA10', 'PV.PQA10', 'PV.PQX10',\n",
       "       'TM.PQW10', 'TV.PQW10', '1C.EQW10', '1C.PQA10', '1C.PQW10',\n",
       "       '1C.PQX10', '1U.CQC10', '1U.CQC50', '1U.EQW20', '1V.EQC30',\n",
       "       '1V.EQW10', '1V.PQW10', '1V.PQX30', '2U.CQC10', '2U.CQC50',\n",
       "       '2U.PQW10', '3U.CQC10', '3U.CQC50', '6V.CQC10', '6V.CQC20',\n",
       "       '6V.CQC30', '6V.CQC40', 'XX.PQW03', '1F.FQE10', '1U.EQW10',\n",
       "       '1U.PQW10', '1U.PQX10', '2V.EQW10', '3U.EQW10', '6V.EQW10',\n",
       "       '7U.EQW10', 'TM.EQW10', 'TV.EQW10', '1V.EQW20', '6V.EQA10',\n",
       "       'XX.CCX01', 'XX.CCZ01', 'XX.CCY01', '1U.EQC10', '1U.PQA10',\n",
       "       '1V.EQC10', 'XX.IQW01', '5V.PQA10', '6U.CDG10', '6U.CDG20',\n",
       "       '6U.PQA10', '7U.CDG10', 'PV.PQW10', 'TV.CQC10', 'XX.PQX03'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_pandas['OPE_NO'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7601ae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CIW0', 'EEW0', 'FEW0', 'HFT0', 'OEW0', 'PEW0', 'PTW0', 'REW0',\n",
       "       'SEW0', 'SFW0', 'TSW0', 'MCW0', 'MEW0', 'MFW0', 'MSW0', 'SNW0',\n",
       "       '01W0', '02W0', '03W0', '04W0', '05W0', '06W0', '07W0', '08W0',\n",
       "       '09W0', '10W0', '11W0', '12W0', '13W0', '14W0', '15W0', '16W0',\n",
       "       '17W0', '18W0', '19W0', '20W0', '21W0', '22W0', '23W0', '24W0',\n",
       "       '25W0', '26W0', '27W0', '28W0', '29W0', '30W0', '31W0', '32W0',\n",
       "       '33W0', '34W0', '35W0', '36W0', '37W0', '38W0', '39W0', '40W0',\n",
       "       '41W0', '42W0', '43W0', '44W0', '45W0', 'DNW01', 'DNW02', 'DNW03',\n",
       "       'DNW04', 'DNW05', 'DNW06', 'DNW07', 'DNW08', 'DNW09', 'DNW10',\n",
       "       'DNW11', 'DNW12', 'XSIG', 'YSIG', 'A1W0', 'A2W0', 'A3W0', 'A4W0',\n",
       "       'A5W0', 'A6W0', 'A7W0', 'A8W0', 'A9W0', 'AAW0', 'ABW0', 'ACW0',\n",
       "       'ADW0', 'AEW0', 'AFW0', 'AGW0', 'AHW0', 'AIW0', 'AJW0', 'AMW0',\n",
       "       'AXW0', 'AYW0', 'B5W0', 'FX01', 'FX02', 'FX03', 'FX04', 'FX05',\n",
       "       'FY01', 'FY02', 'FY03', 'FY04', 'FY05', 'IYW0', 'JYW0', 'MPW0',\n",
       "       'MX01', 'MX02', 'MX03', 'MX04', 'MX05', 'MY01', 'MY02', 'MY03',\n",
       "       'MY04', 'MY05', 'QX01', 'QX02', 'QX03', 'QX04', 'QX05', 'QY01',\n",
       "       'QY02', 'QY03', 'QY04', 'QY05', 'SAW0', 'SBW0', 'WAW0', 'WBW0',\n",
       "       'X7W0', 'X8W0', 'XDW0', 'XIW0', 'XMW0', 'XSW0', 'XTW0', 'Y7W0',\n",
       "       'Y8W0', 'YDW0', 'YIW0', 'YMW0', 'YSW0', 'YTW0', 'FGS1', 'TGS1',\n",
       "       'TGS2', 'WIW0', '100W0', '101W0', '102W0', '103W0', '104W0',\n",
       "       '105W0', '106W0', '107W0', '108W0', '109W0', '110W0', '111W0',\n",
       "       '112W0', '113W0', '114W0', '115W0', '116W0', '117W0', '118W0',\n",
       "       '119W0', '120W0', '121W0', '122W0', '123W0', '124W0', '125W0',\n",
       "       '126W0', '127W0', '128W0', '129W0', '130W0', '131W0', '132W0',\n",
       "       '133W0', '134W0', '135W0', '136W0', '137W0', '138W0', '139W0',\n",
       "       '140W0', '141W0', '142W0', '143W0', '144W0', '145W0', '146W0',\n",
       "       '147W0', '148W0', '149W0', '150W0', '151W0', '152W0', '153W0',\n",
       "       '154W0', '155W0', '156W0', '46W0', '47W0', '48W0', '49W0', '50W0',\n",
       "       '51W0', '52W0', '53W0', '54W0', '55W0', '56W0', '57W0', '58W0',\n",
       "       '59W0', '60W0', '61W0', '62W0', '63W0', '64W0', '65W0', '66W0',\n",
       "       '67W0', '68W0', '69W0', '70W0', '71W0', '72W0', '73W0', '74W0',\n",
       "       '75W0', '76W0', '77W0', '78W0', '79W0', '80W0', '81W0', '82W0',\n",
       "       '83W0', '84W0', '85W0', '86W0', '87W0', '88W0', '89W0', '90W0',\n",
       "       '91W0', '92W0', '93W0', '94W0', '95W0', '96W0', '97W0', '98W0',\n",
       "       '99W0', 'DPW0', 'CXS1', 'CYS1', 'FDS1', 'TAW1', 'TDS1', 'THW1',\n",
       "       'TLW1', 'TUW1', 'T1S1', 'T2S1', 'FDS2', 'HAW1', 'HDS1', 'HHW1',\n",
       "       'HLW1', 'HRW1', 'HUW1', 'TAW2', 'TDS2', 'THW2', 'TLW2', 'TRW1',\n",
       "       'TRW2', 'TUW2', 'DNW13', 'DNW14', 'DNW15', 'DNW16', 'WX02W0',\n",
       "       'WX03W0', 'WX11W0', 'WX12W0', 'WX20W0', 'WX21W0', 'WX30W0',\n",
       "       'WY02W0', 'WY03W0', 'WY11W0', 'WY12W0', 'WY20W0', 'WY21W0',\n",
       "       'WY30W0', 'FX010', 'FX06', 'FX07', 'FX08', 'FX09', 'FY010', 'FY06',\n",
       "       'FY07', 'FY08', 'FY09', 'MX010', 'MX06', 'MX07', 'MX08', 'MX09',\n",
       "       'MY010', 'MY06', 'MY07', 'MY08', 'MY09', 'QX010', 'QX06', 'QX07',\n",
       "       'QX08', 'QX09', 'QY010', 'QY06', 'QY07', 'QY08', 'QY09', 'T1S2',\n",
       "       'TAW3', 'TDS3', 'TAW4', 'TAW5', 'TAWB', 'TDS0', 'TUW9', 'TUWB',\n",
       "       'DPW1', 'TX01', 'TY01', 'TUWC', 'TUWT', '157W0', '158W0', '159W0',\n",
       "       '160W0', '161W0', '162W0', '163W0', '164W0', '165W0', '166W0',\n",
       "       '167W0', '168W0', '169W0', '170W0', '171W0', '172W0', '173W0',\n",
       "       '174W0', '175W0', '176W0', '177W0', '178W0', '179W0', '180W0',\n",
       "       '181W0', '182W0', '183W0', '184W0', '185W0', '186W0', '187W0',\n",
       "       '188W0', '189W0', '190W0', '191W0', '192W0', '193W0', '194W0',\n",
       "       '195W0', '196W0', '197W0', '198W0', '199W0', '200W0', 'TX06',\n",
       "       'TY06', 'TX02', 'TX03', 'TX04', 'TX05', 'TY02', 'TY03', 'TY04',\n",
       "       'TY05', '201W0', '202W0', '203W0', '204W0', '205W0', '206W0',\n",
       "       '207W0', '208W0', '209W0', '210W0', '211W0', '212W0', '213W0',\n",
       "       '214W0', '215W0', '216W0', '217W0', '218W0', '219W0', '220W0',\n",
       "       '221W0', '222W0', '223W0', '224W0', '225W0', '226W0', '227W0',\n",
       "       '228W0', '229W0', '230W0', '231W0', '232W0', '233W0', '234W0',\n",
       "       '235W0', '236W0', '237W0', '238W0', '239W0', '240W0', '241W0',\n",
       "       '242W0', '243W0', '244W0', '245W0', '246W0', '247W0', '248W0',\n",
       "       '249W0', '250W0', 'TAWT'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_pandas['INLINE_PARAMETER_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "097c443e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OPE_NO</th>\n",
       "      <th>INLINE_PARAMETER_ID</th>\n",
       "      <th>AVERAGE</th>\n",
       "      <th>STD_DEV</th>\n",
       "      <th>AVG_SPEC_CHK_RESULT</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>1C.PQX10</td>\n",
       "      <td>XMW0</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12380</th>\n",
       "      <td>1C.PQX10</td>\n",
       "      <td>XMW0</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18393</th>\n",
       "      <td>1C.PQX10</td>\n",
       "      <td>XMW0</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24314</th>\n",
       "      <td>1C.PQX10</td>\n",
       "      <td>XMW0</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31598</th>\n",
       "      <td>1C.PQX10</td>\n",
       "      <td>XMW0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31954</th>\n",
       "      <td>1C.PQX10</td>\n",
       "      <td>XMW0</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         OPE_NO INLINE_PARAMETER_ID   AVERAGE  STD_DEV  AVG_SPEC_CHK_RESULT  \\\n",
       "2767   1C.PQX10                XMW0  0.000129      NaN                  NaN   \n",
       "12380  1C.PQX10                XMW0  0.000093      NaN                  NaN   \n",
       "18393  1C.PQX10                XMW0  0.000042      NaN                  NaN   \n",
       "24314  1C.PQX10                XMW0  0.000072      NaN                  NaN   \n",
       "31598  1C.PQX10                XMW0  0.000011      NaN                  NaN   \n",
       "31954  1C.PQX10                XMW0  0.000017      NaN                  NaN   \n",
       "\n",
       "       label  \n",
       "2767       1  \n",
       "12380      1  \n",
       "18393      1  \n",
       "24314      1  \n",
       "31598      0  \n",
       "31954      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_pandas1 = df1_pandas.query(\"OPE_NO == '1C.PQX10' & INLINE_PARAMETER_ID == 'XMW0'\")[['OPE_NO', 'INLINE_PARAMETER_ID', 'AVERAGE', 'STD_DEV', 'AVG_SPEC_CHK_RESULT', 'label']]\n",
    "df1_pandas1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e49737",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40e446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "124d939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df, agg_list):\n",
    "    new_df = df.dropna(subset=['OPE_NO', 'INLINE_PARAMETER_ID']).copy()\n",
    "    \n",
    "    # 数据类型转换\n",
    "    convert_to_numeric_list = agg_list + ['label', 'AVG_SPEC_CHK_RESULT']\n",
    "    new_df[convert_to_numeric_list] = new_df[convert_to_numeric_list].apply(pd.to_numeric)\n",
    "    \n",
    "    # 缺失值填充\n",
    "    new_df[agg_list] = new_df[agg_list].apply(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "    sum_spec_chk_res = new_df['AVG_SPEC_CHK_RESULT'].sum()\n",
    "    sum_spec_chk_res = 0.0 if pd.isna(sum_spec_chk_res) else sum_spec_chk_res\n",
    "    return sum_spec_chk_res, new_df\n",
    "    \n",
    "    \n",
    "def dis_gravity(df, cols):\n",
    "    \"\"\"分类重心距离\"\"\"\n",
    "    df_good = df.query('label == 0')\n",
    "    df_bad = df.query('label == 1')\n",
    "\n",
    "    feature_cols = []\n",
    "    for col in cols:\n",
    "        sample_good_count = len(df_good[col].dropna())\n",
    "        sample_bad_count = len(df_bad[col].dropna())\n",
    "        if df[col].nunique() > 1 and sample_bad_count > 0 and sample_good_count > 0:\n",
    "            feature_cols.append(col)\n",
    "\n",
    "    if len(feature_cols) == 0:\n",
    "        return -1\n",
    "\n",
    "    x_good = []\n",
    "    x_bad = []\n",
    "    for col in feature_cols:\n",
    "        x_good.append(df_good[col].dropna().mean())\n",
    "        x_bad.append(df_bad[col].dropna().mean())\n",
    "\n",
    "    x_good = np.array(x_good).reshape(-1)\n",
    "    x_bad = np.array(x_bad).reshape(-1)\n",
    "    score =  np.linalg.norm(x_good - x_bad)\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "def fit_inline_algorithm(df, by, agg_list):\n",
    "    \n",
    "    schema_all = StructType([StructField(\"OPE_NO\", StringType(), True),\n",
    "                              StructField(\"weight\", FloatType(), True),\n",
    "                              StructField(\"INLINE_PARAMETER_ID\", StringType(), True),\n",
    "                              StructField(\"AVG_SPEC_CHK_RESULT_COUNT\", FloatType(), True)])\n",
    "\n",
    "    @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def get_model_result(df):\n",
    "        sum_spec_chk_res, new_df = pre_process(df, agg_list)\n",
    "        score = dis_gravity(new_df, agg_list)\n",
    "\n",
    "        # 增加一些字段信息\n",
    "        new_df = new_df[['OPE_NO', 'INLINE_PARAMETER_ID']].head(1)\n",
    "        new_df['weight'] = score\n",
    "        new_df['AVG_SPEC_CHK_RESULT_COUNT'] = sum_spec_chk_res\n",
    "        new_df = new_df[['OPE_NO', 'INLINE_PARAMETER_ID', 'weight', 'AVG_SPEC_CHK_RESULT_COUNT']]\n",
    "        return new_df\n",
    "    return df.groupby(by).apply(get_model_result)\n",
    "\n",
    "\n",
    "\n",
    "def add_certain_column(df, by, request_id):\n",
    "    \"\"\"\n",
    "    param df: 最后的建模结果\n",
    "    param by: 分组字段, 手动增加一列add\n",
    "    param request_id: 传入的request_id\n",
    "    return: 最后的建模结果增加特定的列\n",
    "    \"\"\"\n",
    "    try:\n",
    "        schema_all = StructType([\n",
    "\n",
    "            StructField(\"OPER_NO\", StringType(), True),\n",
    "            StructField(\"INLINE_PARAMETER_ID\", StringType(), True),\n",
    "            StructField(\"AVG_SPEC_CHK_RESULT_COUNT\", FloatType(), True),\n",
    "            StructField(\"weight\", FloatType(), True),\n",
    "            StructField(\"request_id\", StringType(), True),\n",
    "            StructField(\"weight_percent\", FloatType(), True),\n",
    "            StructField(\"index_no\", IntegerType(), True)])\n",
    "\n",
    "        @pandas_udf(returnType=schema_all, functionType=PandasUDFType.GROUPED_MAP)\n",
    "        def get_result(final_res):\n",
    "            final_res['weight'] = final_res['weight'].astype(float)\n",
    "            final_res = final_res.query(\"weight > 0\")\n",
    "            final_res['weight'] = final_res['weight'] / final_res['weight'].sum()\n",
    "            final_res['weight_percent'] = final_res['weight'] * 100\n",
    "            \n",
    "            final_res['request_id'] = request_id\n",
    "            final_res = final_res.sort_values('weight', ascending=False)\n",
    "            final_res['index_no'] = [i + 1 for i in range(len(final_res))]\n",
    "            \n",
    "            final_res['AVG_SPEC_CHK_RESULT_COUNT'] = final_res['AVG_SPEC_CHK_RESULT_COUNT'].fillna(0)\n",
    "            final_res = final_res.drop('add', axis=1)\n",
    "            return final_res\n",
    "        return df.groupby(by).apply(get_result)\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f51b593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------------+------------+----------+--------------+--------+\n",
      "| OPER_NO|INLINE_PARAMETER_ID|AVG_SPEC_CHK_RESULT_COUNT|      weight|request_id|weight_percent|index_no|\n",
      "+--------+-------------------+-------------------------+------------+----------+--------------+--------+\n",
      "|2V.EQW10|               DPW1|                      0.0|  0.41912445|        sd|     41.912445|       1|\n",
      "|2V.EQW10|               DPW0|                      0.0|   0.3725551|        sd|      37.25551|       2|\n",
      "|6V.EQW10|               DPW1|                      0.0|  0.13970816|        sd|     13.970816|       3|\n",
      "|1V.PQW10|               08W0|                     12.0|0.0027941633|        sd|    0.27941632|       4|\n",
      "|1V.PQW10|               01W0|                     12.0|0.0027941633|        sd|    0.27941632|       5|\n",
      "|1V.PQW10|               07W0|                     12.0|0.0027941633|        sd|    0.27941632|       6|\n",
      "|1V.PQW10|               05W0|                     12.0|0.0027941633|        sd|    0.27941632|       7|\n",
      "|1V.PQW10|               11W0|                     12.0|0.0027941633|        sd|    0.27941632|       8|\n",
      "|1V.PQW10|               04W0|                     12.0|0.0027941633|        sd|    0.27941632|       9|\n",
      "|1V.PQW10|               09W0|                     12.0|0.0027941633|        sd|    0.27941632|      10|\n",
      "|1V.PQW10|               03W0|                     12.0|0.0027941633|        sd|    0.27941632|      11|\n",
      "|1V.PQW10|               13W0|                     12.0|0.0027941633|        sd|    0.27941632|      12|\n",
      "|1V.PQW10|               06W0|                     12.0|0.0027941633|        sd|    0.27941632|      13|\n",
      "|1V.PQW10|               02W0|                     12.0|0.0027941633|        sd|    0.27941632|      14|\n",
      "|1V.PQW10|               10W0|                     12.0|0.0027941633|        sd|    0.27941632|      15|\n",
      "|1V.PQW10|               12W0|                     12.0|0.0027941633|        sd|    0.27941632|      16|\n",
      "|2U.PQW10|               07W0|                     12.0|0.0024837006|        sd|    0.24837007|      17|\n",
      "|2U.PQW10|               05W0|                     12.0|0.0024837006|        sd|    0.24837007|      18|\n",
      "|2U.PQW10|               01W0|                     12.0|0.0024837006|        sd|    0.24837007|      19|\n",
      "|2U.PQW10|               12W0|                     12.0|0.0024837006|        sd|    0.24837007|      20|\n",
      "+--------+-------------------+-------------------------+------------+----------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 39090)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "request_id = 'sd'\n",
    "\n",
    "res = (fit_inline_algorithm(df = df1.repartition(10, 'OPE_NO', 'INLINE_PARAMETER_ID'), \n",
    "                            by=['OPE_NO', 'INLINE_PARAMETER_ID'], \n",
    "                            agg_list=['AVERAGE', 'STD_DEV'])\n",
    "        .withColumnRenamed(existing=\"OPE_NO\", new=\"OPER_NO\")\n",
    "        .withColumn('add', lit(0)))\n",
    "\n",
    "final_res = add_certain_column(df=res, by='add', request_id=request_id)\n",
    "final_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665a057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e275b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e01ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#####################################正式调用以上函数##################################\n",
    "####################################################################################\n",
    "# 1. 解析json 为字典， df1 为kafka 输入的结果数据\n",
    "df2 = df1.toPandas() \n",
    "parse_dict, request_id = get_some_info(df2)\n",
    "\n",
    "\n",
    "# 2. 从kafka 关键字映射都具体数据源中的字段,没有的可以删除\n",
    "keyword_map_from_json_to_table: dict = {\n",
    "    \"waferId\": \"WAFER_ID\",\n",
    "    \"dateRange\": \"MEASURE_TIME\",\n",
    "    \"productId\": \"PRODUCT_ID\",\n",
    "    \"operNo\": \"OPE_NO\",\n",
    "    # \"inlineParameterId\": 'INLINE_PARAMETER_ID', \n",
    "    # \"eqp\": \"TOOL_NAME\",\n",
    "    \"lot\": \"LOT_ID\",\n",
    "    # \"recipeName\": \"RECIPE_NAME\"\n",
    "}\n",
    "\n",
    "select_condition_list = parse_dict\n",
    "\n",
    "# 3. 查询表名, 需按实际情况修改\n",
    "table_name = \"ODS_EDA.ODS_INLINE_WAFER_SUMMARY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "b3e6fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主程序\n",
    "try:\n",
    "    # 1. 从数据库中获取数据\n",
    "    df1 = get_data_from_doris(select_condition_list=select_condition_list, table_name=table_name)\n",
    "    df1 = df1.select(\"OPE_NO\", \"INLINE_PARAMETER_ID\", \"AVERAGE\",'STD_DEV', 'AVG_SPEC_CHK_RESULT', 'label')\n",
    "    if df1 is None or df1.count() == 0:\n",
    "        msg = '解析SQL获取数据异常: 数据库中可能没有数据!'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "        \n",
    "    # 2. fit Inline算法\n",
    "    res = (fit_inline_algorithm(df = df1.repartition(10, 'OPE_NO', 'INLINE_PARAMETER_ID'), \n",
    "                                by=['OPE_NO', 'INLINE_PARAMETER_ID'], \n",
    "                                agg_list=['AVERAGE', 'STD_DEV'])\n",
    "            .withColumnRenamed(existing=\"OPE_NO\", new=\"OPER_NO\")\n",
    "            .withColumn('add', lit(0)))\n",
    "    if res is None or res.count() == 0:\n",
    "        msg = '该场景下数据库中暂无充足的数据(真实BAD个数可能为0或真实GOOD个数可能为0)'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "        \n",
    "    # 3. 增加特定的列，算法结果处理\n",
    "    final_res = add_certain_column(df=res, by='add', request_id=request_id)\n",
    "    if final_res is None or final_res.count() == 0:\n",
    "        msg = '算法结果增加列暂时异常'\n",
    "        df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f'{msg}', \"requestId\": request_id}, index=[0])\n",
    "        df1 = spark.createDataFrame(df_kafka)\n",
    "        raise ValueError\n",
    "    \n",
    "    \n",
    "    # 4.final_res 是最后的结果，要写回数据库\n",
    "    ddd = final_res.toPandas()\n",
    "    user =\"root\"\n",
    "    host = \"10.52.199.81\"\n",
    "    password = \"Nexchip%40123\"\n",
    "    db = \"etl\"\n",
    "    port = 9030\n",
    "    engine = create_engine(\"mysql+pymysql://{user}:{password}@{host}:{port}/{db}\".format(user = user,\n",
    "                                                                                        password = password,\n",
    "                                                                                        host = host,\n",
    "                                                                                        port = port,\n",
    "                                                                                        db = db))\n",
    "    doris_stream_load_from_df(ddd, engine, \"inline_results\")\n",
    "\n",
    "    # 最后运行成功，输出以下kafka信息\n",
    "    df_kafka = pd.DataFrame({'code': 0,  'msg': '运行成功', 'requestId': request_id}, index=[1])\n",
    "    df1 = spark.createDataFrame(df_kafka)\n",
    "    \n",
    "except ValueError as ve:\n",
    "    pass\n",
    "\n",
    "except Exception as e:\n",
    "    df_kafka = pd.DataFrame({\"code\": 1, \"msg\": f\"主程序发生异常: {str(e)}\", \"requestId\": request_id}, index=[0])\n",
    "    df1 = spark.createDataFrame(df_kafka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "4ad125ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(code=1, msg=\"主程序发生异常: name 'select_condition_list' is not defined\", requestId='sd')]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 50994)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf3e4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#####################################结果写回数据库####################################\n",
    "####################################################################################\n",
    "def doris_stream_load_from_df(df, engine, table, is_json=True, chunksize=100000, partitions=None):\n",
    "    engine_url = engine.url\n",
    "    url = 'http://%s:18030/api/%s/%s/_stream_load' % (engine_url.host, engine_url.database, table)\n",
    "\n",
    "    format_str = 'csv' if not is_json else 'json'\n",
    "    headers = {\n",
    "        'Content-Type': 'text/plain; charset=UTF-8',\n",
    "        'format': format_str,\n",
    "        'Expect': '100-continue'\n",
    "    }\n",
    "    if is_json:\n",
    "        headers['strip_outer_array'] = 'true'\n",
    "        headers['read_json_by_line'] = 'true'\n",
    "    else:\n",
    "        headers['column_separator'] = '@'\n",
    "    \n",
    "    if partitions:\n",
    "        headers['partitions'] = partitions\n",
    "    \n",
    "    auth = requests.auth.HTTPBasicAuth(engine_url.username, engine_url.password)\n",
    "    session = requests.sessions.Session()\n",
    "    session.should_strip_auth = lambda old_url, new_url: False\n",
    "    \n",
    "    l = len(df)\n",
    "    if l > 0:\n",
    "        if chunksize and chunksize < l:\n",
    "            batches = l // chunksize\n",
    "            if l % chunksize > 0:\n",
    "                batches += 1\n",
    "            for i in range(batches):\n",
    "                si = i * chunksize\n",
    "                ei = min(si + chunksize, l)\n",
    "                sub = df[si:ei]\n",
    "                do_doris_stream_load_from_df(sub, session, url, headers, auth, is_json)\n",
    "        else:\n",
    "            do_doris_stream_load_from_df(df, session, url, headers, auth, is_json)\n",
    "\n",
    "\n",
    "def do_doris_stream_load_from_df(df, session, url, headers, auth, is_json=False):\n",
    "    data = df.to_csv(header=False, index=False, sep='@') if not is_json else df.to_json(orient='records', date_format='iso')\n",
    "    #print(data)\n",
    "    \n",
    "    resp = session.request(\n",
    "        'PUT',\n",
    "        url = url,\n",
    "        data=data.encode('utf-8'),\n",
    "        headers=headers,\n",
    "        auth=auth\n",
    "    )\n",
    "    print(resp.reason, resp.text)\n",
    "    check_stream_load_response(resp.text)\n",
    "\n",
    "\n",
    "\n",
    "def check_stream_load_response(resp_text):\n",
    "    resp = json.loads(resp_text)\n",
    "    if resp['Status'] not in [\"Success\", \"Publish Timeout\"]:\n",
    "        raise Exception(resp['Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8aa0431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "########################################T检验分析#####################################\n",
    "####################################################################################\n",
    "\n",
    "# 创建pandas_udf 映射信息\n",
    "null_dataframe = pd.DataFrame(\n",
    "    {'OPE_NO' : [\"test\"],\n",
    "    'INLINE_PARAMETER_ID':['bbbss'],\n",
    "    'weight':[0.0],\n",
    "    'AVG_SPEC_CHK_RESULT_COUNT':[0.0]})\n",
    "\n",
    "schema = spark.createDataFrame(null_dataframe).schema\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def process_df_ttest(df):\n",
    "    \"\"\"t检验\"\"\"\n",
    "    def dis_gravity(df,cols):\n",
    "        \"\"\"分类重心距离\"\"\"\n",
    "        df_good = df.query('label == 0')\n",
    "        df_bad = df.query('label == 1')\n",
    "        feature_cols = []\n",
    "        for col in cols:\n",
    "            sample_good_count = len(df_good[col].dropna())\n",
    "            sample_bad_count = len(df_bad[col].dropna())\n",
    "\n",
    "            if df[col].nunique() >1 and (sample_bad_count) > 0 and sample_good_count > 0:\n",
    "                sum_value = df_good[col].dropna().mean() + df_bad[col].dropna().mean()\n",
    "                feature_cols.append(col)\n",
    "\n",
    "        for col in feature_cols:\n",
    "            df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].max())\n",
    "\n",
    "        if len(feature_cols) == 0:\n",
    "            return -1\n",
    "        # print(f\"x_good: {x_good}  x_bad: {x_bad}\")\n",
    "        # print(x_good.mean(axis=0), x_bad.mean(axis=0))\n",
    "\n",
    "        # for col in feature_cols:\n",
    "        #     sdf_g= df.query('label ==0')[col].mean()\n",
    "        #     sdf_b = df.query('label ==1')[col].mean()\n",
    "        x_good = []\n",
    "        x_bad = []\n",
    "\n",
    "        for col in feature_cols:\n",
    "            x_good.append(df_good[col].dropna().mean())\n",
    "            x_bad.append(df_bad[col].dropna().mean())\n",
    "        \n",
    "        x_good = np.array(x_good).reshape(-1)\n",
    "        x_bad = np.array(x_bad).reshape(-1)\n",
    "        diff = x_good - x_bad \n",
    "        score =  np.linalg.norm(diff)\n",
    "        return score \n",
    "\n",
    "\n",
    "    def ttest(sample1, sample2):\n",
    "        \"\"\"t检验\"\"\"\n",
    "        tr = stats.ttest_ind(sample1, sample2)\n",
    "        return tr.__getattribute__(\"pvalue\"),  tr.__getattribute__(\"statistic\")\n",
    "\n",
    "\n",
    "    new_df = df.dropna(subset=['OPE_NO', 'INLINE_PARAMETER_ID']).copy()\n",
    "    new_df['AVG_SPEC_CHK_RESULT'] =  pd.to_numeric( new_df['AVG_SPEC_CHK_RESULT'])\n",
    "    mean_spec_chk_res = new_df['AVG_SPEC_CHK_RESULT'].sum()\n",
    "\n",
    "    if pd.isna(mean_spec_chk_res):\n",
    "        mean_spec_chk_res = 0.0\n",
    "\n",
    "    agg_list = [\"AVERAGE\",'STD_DEV', 'label']\n",
    "\n",
    "    # Make sure that col must be numeric type , if not ,convert it to numeric type\n",
    "    for col in agg_list:\n",
    "        # new_df[col] = new_df[col].astype(float)\n",
    "        new_df[col] = pd.to_numeric(new_df[col])\n",
    "    agg_list.remove(\"label\")\n",
    "\n",
    "    df_good = new_df.query('label == 0')\n",
    "    df_bad = new_df.query('label == 1')\n",
    "    score = dis_gravity(new_df, agg_list)\n",
    "\n",
    "    new_df = new_df[['OPE_NO', 'INLINE_PARAMETER_ID']].head(1)\n",
    "    new_df['weight'] = score\n",
    "    new_df['AVG_SPEC_CHK_RESULT_COUNT'] = mean_spec_chk_res\n",
    "    return new_df[['OPE_NO', 'INLINE_PARAMETER_ID', 'weight', 'AVG_SPEC_CHK_RESULT_COUNT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045edd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a7c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae82c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ea6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3c239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a272017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b638a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
